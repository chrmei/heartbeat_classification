# Heartbeat Classification Project

## Overview

This project implements machine learning and deep learning architectures to classify cardiac signals from ECG (electrocardiogram) data. The project combines two datasets: **MIT-BIH Arrhythmia Dataset** (5 classes) and **PTB Diagnostic ECG Database** (2 classes - normal/abnormal).

**Project Difficulty:** 8/10

**Current Status:** âœ… **PROJECT COMPLETED** - All modeling phases completed with results exceeding benchmark performance.

## Key Results

Based on the final project report, our models achieved the following performance:

| Dataset | Model | Accuracy | Precision | Recall | F1 Score |
|---------|-------|----------|-----------|--------|----------|
| **MIT-BIH** | CNN8 (Optimized) | **98.51%** | 90.62% | 94.24% | 92.36% |
| **PTB** | CNN8 + Transfer Learning | **98.42%** | 97.51% | 98.64% | 98.05% |

**Benchmark Comparison:**
- MIT-BIH: Exceeded benchmark [2] by ~5% (benchmark: 93.40%)
- PTB: Exceeded benchmark [2] by ~2.5% (benchmark: 95.90%)

For detailed results and methodology, see the [Final Report](reports/renderings/03_Final%20Report.pdf).

## Datasets

- `mitbih_train.csv` (87,554 samples, 188 features including label)
- `mitbih_test.csv` (21,892 samples)
- `ptbdb_normal.csv` (4,046 samples)
- `ptbdb_abnormal.csv` (10,506 samples)

Each row represents one heartbeat segment with 187 time points + 1 label column.

## Data Quality Summary

Based on comprehensive data audit analysis:

| Dataset | Samples | Features | Missing Values | Duplicates | Memory Usage |
|---------|---------|----------|----------------|------------|--------------|
| **MIT-BIH Train** | 87,554 | 188 | 0 | 0 | 131.7 MB |
| **MIT-BIH Test** | 21,892 | 188 | 0 | 0 | 32.9 MB |
| **PTB Normal** | 4,046 | 188 | 0 | 1 | 6.1 MB |
| **PTB Abnormal** | 10,506 | 188 | 0 | 6 | 15.8 MB |

**Key Findings:**
- âœ… **No missing values** in any dataset
- âœ… **Clean data structure** with consistent 188 features (187 ECG samples + 1 label)
- âœ… **Minimal duplicates** (removed during preprocessing)
- âœ… **Memory efficient** data storage
- âš ï¸ **Class imbalance** present in both datasets (addressed with SMOTE sampling)

## Project Structure

```
heartbeat_classification/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ original/          # Raw Kaggle data (âœ… Complete)
â”‚   â”‚   â”œâ”€â”€ mitbih_train.csv
â”‚   â”‚   â”œâ”€â”€ mitbih_test.csv
â”‚   â”‚   â”œâ”€â”€ ptbdb_normal.csv
â”‚   â”‚   â””â”€â”€ ptbdb_abnormal.csv
â”‚   â”œâ”€â”€ processed/         # Cleaned & preprocessed data (âœ… Complete)
â”‚   â”‚   â”œâ”€â”€ mitbih/
â”‚   â”‚   â””â”€â”€ ptb/
â”‚   â””â”€â”€ interim/           # Feature-engineered datasets (âœ… Complete)
â”‚       â”œâ”€â”€ mitbih_train_features.csv
â”‚       â”œâ”€â”€ mitbih_test_features.csv
â”‚       â”œâ”€â”€ ptbdb_normal_features.csv
â”‚       â””â”€â”€ ptbdb_abnormal_features.csv
â”œâ”€â”€ notebooks/             # Analysis notebooks (âœ… Complete)
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb
â”‚   â”œâ”€â”€ 03_A_*.ipynb       # MIT-BIH baseline models
â”‚   â”œâ”€â”€ 03_B_*.ipynb       # PTB baseline models
â”‚   â”œâ”€â”€ 04_A_*.ipynb       # MIT-BIH deep learning models
â”‚   â”œâ”€â”€ 04_B_*.ipynb       # PTB deep learning models
â”‚   â”œâ”€â”€ 05_A_DL_SHAP.ipynb # MIT-BIH interpretability
â”‚   â”œâ”€â”€ 05_B_DL_SHAP.ipynb # PTB interpretability
â”‚   â””â”€â”€ archive/           # Archived development notebooks
â”‚       â”œâ”€â”€ christian/
â”‚       â””â”€â”€ julia/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils/             # Utility functions (âœ… Complete)
â”‚   â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”‚   â”œâ”€â”€ model_saver.py
â”‚   â”‚   â”œâ”€â”€ audit_report.py
â”‚   â”‚   â””â”€â”€ dl_architectures.py
â”‚   â””â”€â”€ visualization/     # Visualization tools (âœ… Complete)
â”‚       â”œâ”€â”€ visualization.py
â”‚       â””â”€â”€ confusion_matrix.py
â”œâ”€â”€ models/                 # Saved trained models (âœ… Complete)
â”‚   â”œâ”€â”€ MIT_02_01_baseline_models_randomized_search_no_sampling/
â”‚   â”œâ”€â”€ MIT_02_02_baseline_models_randomized_search_sampling/
â”‚   â”œâ”€â”€ MIT_02_03_dl_models/
â”‚   â””â”€â”€ PTB_04_02_dl_models/
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ data_audit/         # Data quality reports (âœ… Complete)
â”‚   â”œâ”€â”€ baseline_models/    # Baseline model results (âœ… Complete)
â”‚   â”‚   â”œâ”€â”€ MIT_02_01_RANDOMIZED_SEARCH/
â”‚   â”‚   â””â”€â”€ MIT_02_02_RS_SAMPLING/
â”‚   â”œâ”€â”€ deep_learning/      # Deep learning model results (âœ… Complete)
â”‚   â”‚   â”œâ”€â”€ cnn8_transfer/
â”‚   â”‚   â”œâ”€â”€ models_optimization/
â”‚   â”‚   â””â”€â”€ model_comparison.csv
â”‚   â”œâ”€â”€ interpretability/   # SHAP analysis results (âœ… Complete)
â”‚   â”‚   â”œâ”€â”€ SHAP_MIT/
â”‚   â”‚   â””â”€â”€ SHAP_PTB/
â”‚   â””â”€â”€ renderings/         # Project reports (âœ… Complete)
â”‚       â”œâ”€â”€ 01_Rendering 1.pdf
â”‚       â”œâ”€â”€ 02_Rendering2-Report.pdf
â”‚       â””â”€â”€ 03_Final Report.pdf
â”œâ”€â”€ docs/                  # Project documentation (âœ… Complete)
â”‚   â”œâ”€â”€ knowledge/
â”‚   â””â”€â”€ ProjectRequirements/
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ requirements.txt       # Dependencies (âœ… Complete)
â”œâ”€â”€ pyproject.toml         # Project configuration (âœ… Complete)
â”œâ”€â”€ README.md              # This file (âœ… Complete)
â””â”€â”€ CONTRIBUTING.md        # Contribution guidelines (âœ… Complete)
```

**Legend:** âœ… Complete

## Notebook Organization

The project notebooks follow a systematic numbering scheme:

- **01-02**: Data exploration and preprocessing
- **03_A**: MIT-BIH baseline models (RandomizedSearch, GridSearch, evaluation)
- **03_B**: PTB baseline models (LazyClassifier, GridSearch, evaluation)
- **04_A**: MIT-BIH deep learning models (CNN, DNN, LSTM, optimization)
- **04_B**: PTB deep learning models (Transfer learning)
- **05_A/B**: Model interpretability (SHAP analysis)

For detailed notebook documentation, see [notebooks/README.md](notebooks/README.md).

**Archived Notebooks:**
Development notebooks from earlier iterations are preserved in `notebooks/archive/` for reference:
- `archive/christian/`: Early development notebooks
- `archive/julia/`: Alternative approaches and experiments

## Features & Capabilities

### âœ… **Implemented Features**

#### 1. **Data Quality Analysis**
- Comprehensive data audit system (`src/utils/audit_report.py`)
- Automated generation of data quality reports for all datasets
- Statistical analysis of missing values, duplicates, and data types
- Memory usage and performance metrics

#### 2. **Data Preprocessing & Feature Engineering**
- Complete preprocessing pipeline (`src/utils/preprocessing.py`)
- Feature engineering with statistical and frequency domain features
- Class imbalance handling with SMOTE (selected as optimal method)
- Data validation and quality checks
- Duplicate removal for PTB dataset

#### 3. **Baseline Model Development**
- Comprehensive model comparison framework
- Multiple algorithms: XGBoost, Random Forest, SVM, Logistic Regression, KNN, Decision Tree, LDA, ANN
- RandomizedSearch and GridSearch hyperparameter optimization
- Performance metrics: Accuracy, Precision, Recall, F1-score
- Model persistence and evaluation utilities

#### 4. **Deep Learning Models**
- CNN architectures (inspired by Kachuee et al. 2018)
- DNN and LSTM models
- Transfer learning from MIT-BIH to PTB dataset
- Model optimization with dropout and batch normalization
- Training on Google Colab with GPU acceleration

#### 5. **Model Interpretability**
- SHAP (SHapley Additive exPlanations) analysis
- Feature importance visualization
- Decision pattern analysis for both datasets

#### 6. **Visualization Tools**
- Advanced ECG plotting utilities (`src/visualization/visualization.py`)
- Confusion matrix visualization (`src/visualization/confusion_matrix.py`)
- Support for single and multiple heartbeat visualization
- Peak detection and signal analysis capabilities
- Customizable plotting parameters and export options

## Setup Instructions

### 1. Clone the Repository
```bash
git clone <repository-url>
cd heartbeat_classification
```

### 2. Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Download Data
Place the original data files in `data/original/` directory:
- mitbih_train.csv
- mitbih_test.csv
- ptbdb_normal.csv
- ptbdb_abnormal.csv

**Note:** The datasets are available from Kaggle. Please ensure you have proper access and licensing.

## Usage Examples

### ğŸ“Š **Data Analysis**
Generate comprehensive data audit reports:

```bash
# Run data audit analysis
python -c "from src.utils.audit_report import generate_data_audit_report; generate_data_audit_report()"
```

### ğŸ““ **Jupyter Notebooks**
Explore the data and models interactively:

```bash
# Launch Jupyter notebook
jupyter notebook notebooks/01_data_exploration.ipynb
jupyter notebook notebooks/02_preprocessing.ipynb

# Baseline models
jupyter notebook notebooks/03_A_02_01_baseline_models_randomized_search.ipynb
jupyter notebook notebooks/03_B_02_baseline_models_lazy_classifier.ipynb

# Deep learning models
jupyter notebook notebooks/04_A_02_CNN_models_smote.ipynb
jupyter notebook notebooks/04_B_01_CNN_Transfer.ipynb

# Interpretability
jupyter notebook notebooks/05_A_DL_SHAP.ipynb
jupyter notebook notebooks/05_B_DL_SHAP.ipynb
```

### ğŸ”§ **Development**
For development and testing:

```bash
# Run data audit
python src/utils/audit_report.py

# Test visualization utilities
python -c "from src.visualization.visualization import plot_heartbeat; import numpy as np; plot_heartbeat(np.random.randn(187))"

# Run model evaluation
python src/utils/evaluation.py
```

## Project Timeline

### Step 1: Data Mining & Visualization âœ… **COMPLETED**
- [x] Create project structure
- [x] Set up requirements.txt
- [x] Write README
- [x] Load and explore datasets
- [x] Generate data audit reports with comprehensive analysis
- [x] Document data quality issues and class imbalance
- [x] Create visualization utilities for ECG plotting

### Step 2: Pre-Processing & Feature Engineering âœ… **COMPLETED**
- [x] Handle class imbalance (SMOTE selected as optimal method)
- [x] Normalize/standardize signals
- [x] Split train/validation/test sets properly
- [x] Feature extraction: statistical and frequency domain features
- [x] Complete preprocessing pipeline implementation
- [x] Duplicate removal for PTB dataset

### Step 3: Baseline Modeling âœ… **COMPLETED**
- [x] **Baseline models**: Multiple algorithms tested
- [x] RandomizedSearch for initial model comparison
- [x] GridSearch for hyperparameter optimization
- [x] Comprehensive model comparison with SMOTE sampling
- [x] Performance evaluation and results documentation
- [x] Model persistence and evaluation utilities

### Step 4: Advanced Optimization âœ… **COMPLETED**
- [x] **GridSearchCV** for best models
- [x] **Extreme values analysis** (RR-Distance analysis)
- [x] Advanced hyperparameter tuning
- [x] Model selection and final evaluation

### Step 5: Deep Learning Implementation âœ… **COMPLETED**
- [x] **Deep Learning models**: CNN, DNN, LSTM architectures
- [x] Transfer learning from MIT-BIH to PTB dataset
- [x] Model optimization with dropout and batch normalization
- [x] Model interpretability: SHAP values analysis
- [x] Advanced neural network architectures

### Step 6: Final Report & Documentation âœ… **COMPLETED**
- [x] Compile all reports
- [x] Clean, document, and organize code
- [x] Final report generation
- [x] Results documentation and comparison with benchmark

## Key Considerations

**Metrics:** Focus on F1-score, precision, recall (due to class imbalance). Also track accuracy, confusion matrix.

**Challenges Addressed:**
- âœ… Severe class imbalance in both datasets (solved with SMOTE)
- âœ… High dimensionality (187 features) - handled by deep learning architectures
- âœ… Two separate datasets with different objectives (5-class vs 2-class)
- âœ… Time series nature - handled with appropriate architectures
- âœ… Model interpretability - addressed with SHAP analysis

**Success Criteria:**
- âœ… Beat benchmark performance (exceeded by 2.5-5%)
- âœ… Robust model with good generalization
- âœ… Clear, professional reports with business insights
- âœ… Model interpretability for clinical validation

## Results Summary

### ğŸ† **Final Model Performance**

**MIT-BIH Arrhythmia Classification (5 classes):**
- **Best Model**: CNN8 (Optimized with dropout and batch normalization)
- **Accuracy**: 98.51%
- **Precision**: 90.62%
- **Recall**: 94.24%
- **F1 Score**: 92.36%

**PTB Myocardial Infarction Detection (2 classes):**
- **Best Model**: CNN8 with Transfer Learning (last residual block unfrozen)
- **Accuracy**: 98.42%
- **Precision**: 97.51%
- **Recall**: 98.64%
- **F1 Score**: 98.05%

### ğŸ“Š **Key Achievements**

1. **Exceeded Benchmark Performance**
   - MIT-BIH: +5% improvement over benchmark [2]
   - PTB: +2.5% improvement over benchmark [2]

2. **Robust Model Development**
   - Comprehensive baseline model comparison
   - Advanced deep learning architectures
   - Transfer learning implementation

3. **Model Interpretability**
   - SHAP analysis for both datasets
   - Feature importance visualization
   - Clinically relevant pattern identification

4. **Complete Documentation**
   - Comprehensive data audit
   - Detailed model evaluation reports
   - Final project report with methodology and results

## Bibliography

Key research articles referenced in this project:

1. **Kachuee, M., Fazeli, S., & Sarrafzadeh, M. (2018)**. ECG Heartbeat Classification: A Deep Transferable Representation. CoRR. doi: 10.48550/arXiv.1805.00794

2. **Murat, F., Yildirim, O., Talo, M., Baloglu, U. B., Demir, Y., & Acharya, U. R. (2020)**. Application of deep learning techniques for heartbeats detection using ECG signals-analysis and review. Computers in Biology and Medicine. doi:10.1016/j.compbiomed.2020.103726

3. **Ansari, Y., Mourad, O., Qaraqe, K., & Serpedin, E. (2023)**. Deep learning for ECG Arrhythmia detection and classification: an overview of progress for period 2017â€“2023. doi: 10.3389/fphys.2023.1246746

For complete bibliography, see the [Final Report](reports/renderings/03_Final%20Report.pdf).

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

## License

This project is part of the DataScientest Data Scientist training program.

## Contact

For questions about this project, please refer to the DataScientest training materials and instructor support.

---

**Project Team:** Christian Meister, Julia Schmidt, Tzu-Jung Huang  
**Completion Date:** November 2025
