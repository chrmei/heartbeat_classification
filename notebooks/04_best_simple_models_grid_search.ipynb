{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14eab3c2",
   "metadata": {},
   "source": [
    "# 4 Find best \"simple\" model for given dataset\n",
    "\n",
    "This part provides a pipeline for heartbeat classification based on the requirements from `notebooks/03_model_testing_example_mit.ipynb`.\n",
    "\n",
    "Steps:\n",
    "1. Use train/validation datasets created beforehand and shared in group\n",
    "2. GridSearchCV with optimized parameter spaces, based on the previous Notebook\n",
    "3. Target models: XGBoost, ANN, SVM\n",
    "4. GridSearch with and without outlier removal\n",
    "5. RepeatedStratifiedKFold cross-validation\n",
    "6. Leak-free scaling using Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64239f57",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('..')\n",
    "os.chdir('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Sampling\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Custom utilities\n",
    "from src.utils.preprocessing import (\n",
    "    load_interim_dataset,\n",
    "    DatasetSplit,\n",
    "    build_full_suffix as pp_build_full_suffix,\n",
    "    generate_all_interim_datasets,\n",
    ")\n",
    "from src.utils.evaluation import eval_model\n",
    "from src.utils.model_saver import create_model_saver\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3a6a8",
   "metadata": {},
   "source": [
    "## 2. Constants & Param Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "SCORING = {'f1_macro': 'f1_macro', 'bal_acc': 'balanced_accuracy', 'f1_weighted': 'f1_weighted'}\n",
    "results_csv = \"reports/03_model_testing_results/model_comparison_best_models.csv\"\n",
    "\n",
    "PARAM_SPACES = {\n",
    "    \"XGBoost\": {\n",
    "        \"estimator\": xgb.XGBClassifier(\n",
    "            objective=\"multi:softmax\",\n",
    "            num_class=5,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=\"mlogloss\",\n",
    "        ),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [150, 200, 250],\n",
    "            \"max_depth\": [8, 9],\n",
    "            \"learning_rate\": [0.2],\n",
    "            \"subsample\": [0.7, 0.8],\n",
    "            \"colsample_bytree\": [0.9],\n",
    "            \"reg_alpha\": [0.1, 0.2],\n",
    "            \"reg_lambda\": [0.0, 0.05],\n",
    "            \"min_child_weight\": [5],\n",
    "            \"gamma\": [0.0, 0.05],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=RANDOM_STATE),\n",
    "        \"needs_scaling\": False,\n",
    "    },\n",
    "    \"ANN\": {\n",
    "        \"estimator\": MLPClassifier(\n",
    "            max_iter=300,\n",
    "            early_stopping=True,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_iter_no_change=10,\n",
    "            solver=\"adam\",\n",
    "        ),\n",
    "        \"params\": {\n",
    "            \"hidden_layer_sizes\": [(128, 64)],\n",
    "            \"activation\": [\"relu\"],\n",
    "            \"alpha\": [3e-4],\n",
    "            \"learning_rate_init\": [0.001, 0.0015],\n",
    "            \"batch_size\": [96, 128],\n",
    "            \"beta_1\": [0.9, 0.91],\n",
    "            \"beta_2\": [0.97, 0.974],\n",
    "            \"validation_fraction\": [0.1],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
    "        \"needs_scaling\": True,\n",
    "    },\n",
    "    # best: {'clf__kernel': 'rbf', 'clf__gamma': 0,5, 'clf__C': 10}\n",
    "    \"SVM\": {\n",
    "        \"estimator\": SVC(),\n",
    "        \"params\": {\n",
    "            \"kernel\": [\"rbf\"],\n",
    "            \"C\": [10],\n",
    "            \"gamma\": [0.4, 0.5, 0.6],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
    "        \"needs_scaling\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "DATA_DIR = \"data/interim/mitbih\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13354270",
   "metadata": {},
   "source": [
    "## 3. Methods used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leak_free_pipeline(model_name: str, estimator, needs_scaling: bool = True) -> Pipeline:\n",
    "    \"\"\"Create a leak-free pipeline with scaling if needed.\"\"\"\n",
    "    if needs_scaling:\n",
    "        return Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', estimator)\n",
    "        ])\n",
    "    else:\n",
    "        return Pipeline([\n",
    "            ('classifier', estimator)\n",
    "        ])\n",
    "\n",
    "\n",
    "def prepare_dataset_with_sampling(\n",
    "    data_dir: str = DATA_DIR,\n",
    "    sampling_method: str = \"No_Sampling\",\n",
    "    remove_outliers: bool = False\n",
    ") -> Tuple[np.ndarray, Optional[np.ndarray], np.ndarray, Optional[np.ndarray]]:\n",
    "    \"\"\"Load an existing interim dataset for the given configuration.\n",
    "\n",
    "    Datasets are assumed to be pre-generated by preprocessing utilities. This\n",
    "    function never overwrites or generates new data; it only loads.\n",
    "    \"\"\"\n",
    "    # Ensure all datasets are generated once (no-op if already done)\n",
    "    generate_all_interim_datasets(data_dir=data_dir, only_once=True)\n",
    "\n",
    "    full_suffix = pp_build_full_suffix(sampling_method, remove_outliers)\n",
    "    split = load_interim_dataset(data_dir=data_dir, sampling_suffix=full_suffix)\n",
    "\n",
    "    X_train_res = split.X_train.values\n",
    "    y_train_res = split.y_train.values\n",
    "    X_val = split.X_val.values if split.X_val is not None else None\n",
    "    y_val = split.y_val.values if split.y_val is not None else None\n",
    "\n",
    "    return X_train_res, X_val, y_train_res, y_val\n",
    "\n",
    "\n",
    "def run_grid_search(\n",
    "    model_name: str,\n",
    "    sampling_method: str = \"No_Sampling\",\n",
    "    remove_outliers: bool = False,\n",
    "    model_saver=None,\n",
    "    results_dir: str = \"reports/comprehensive_model_testing\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run GridSearchCV for a specific model and sampling method.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to train\n",
    "        sampling_method: Sampling method to use\n",
    "        remove_outliers: Whether to remove outliers\n",
    "        model_saver: Model saver instance\n",
    "        results_dir: Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running GridSearchCV for {model_name} with {sampling_method}\")\n",
    "    print(f\"Outlier removal: {remove_outliers}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get model configuration\n",
    "    model_config = PARAM_SPACES[model_name]\n",
    "    estimator = model_config[\"estimator\"]\n",
    "    params = model_config[\"params\"]\n",
    "    cv = model_config[\"cv\"]\n",
    "    needs_scaling = model_config[\"needs_scaling\"]\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_val, y_train, y_val = prepare_dataset_with_sampling(\n",
    "        sampling_method=sampling_method,\n",
    "        remove_outliers=remove_outliers\n",
    "    )\n",
    "    \n",
    "    # Create leak-free pipeline\n",
    "    pipeline = create_leak_free_pipeline(model_name, estimator, needs_scaling)\n",
    "    \n",
    "    # Adjust parameter names for pipeline\n",
    "    pipeline_params = {}\n",
    "    for param_name, param_values in params.items():\n",
    "        pipeline_params[f'classifier__{param_name}'] = param_values\n",
    "    \n",
    "    # Create experiment name\n",
    "    experiment_name = f\"{sampling_method.lower()}_outliers_{remove_outliers}\"\n",
    "    \n",
    "    # Check if model already exists\n",
    "    if model_saver and model_saver.model_exists(model_name, experiment_name):\n",
    "        print(f\"Model {model_name} already exists for experiment {experiment_name}. Skipping training and CSV append.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Training new model for {model_name}...\")\n",
    "        \n",
    "        # Run GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=pipeline_params,\n",
    "            scoring=SCORING,\n",
    "            refit='f1_macro',\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=3\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Save model if saver is provided\n",
    "        if model_saver:\n",
    "            metadata = {\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'best_score': grid_search.best_score_,\n",
    "                'cv_results': grid_search.cv_results_,\n",
    "                'experiment': experiment_name,\n",
    "                'classifier': model_name,\n",
    "                'sampling_method': sampling_method,\n",
    "                'remove_outliers': remove_outliers,\n",
    "            }\n",
    "            model_saver.save_model(model_name, grid_search, experiment_name, metadata)\n",
    "            print(f\"Model {model_name} saved successfully!\")\n",
    "    \n",
    "    # Evaluate on validation set if available\n",
    "    if X_val is not None and y_val is not None:\n",
    "        print(f\"Evaluating {model_name} on validation set...\")\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # For evaluation, we need to fit the model again since pipeline might not be fitted\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = best_model.predict(X_val)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "            y_val, y_pred, average='macro', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Per-class metrics\n",
    "        labels = np.unique(np.concatenate([y_train, y_val]))\n",
    "        precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
    "            y_val, y_pred, average=None, labels=labels, zero_division=0\n",
    "        )\n",
    "        \n",
    "        confusion_mat = confusion_matrix(y_val, y_pred, labels=labels)\n",
    "        \n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'sampling_method': sampling_method,\n",
    "            'remove_outliers': remove_outliers,\n",
    "            'best_cv_score': grid_search.best_score_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'validation_accuracy': accuracy,\n",
    "            'validation_f1_macro': f1_macro,\n",
    "            'validation_precision_macro': precision_macro,\n",
    "            'validation_recall_macro': recall_macro,\n",
    "            'validation_f1_per_class': f1_per_class,\n",
    "            'validation_precision_per_class': precision_per_class,\n",
    "            'validation_recall_per_class': recall_per_class,\n",
    "            'validation_support_per_class': support_per_class,\n",
    "            'confusion_matrix': confusion_mat,\n",
    "            'labels': labels,\n",
    "        }\n",
    "        \n",
    "        print(f\"Validation F1-Macro: {f1_macro:.4f}\")\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Append to canonical results CSV (only for newly trained models)\n",
    "        row = {\n",
    "            'model': model_name,\n",
    "            'sampling_method': sampling_method,\n",
    "            'remove_outliers': remove_outliers,\n",
    "            'val_accuracy': round(float(accuracy), 4),\n",
    "            'val_f1_macro': round(float(f1_macro), 4),\n",
    "            'best_cv_score': round(float(grid_search.best_score_), 4),\n",
    "            'best_parameters': json.dumps(grid_search.best_params_),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "        header = not os.path.exists(results_csv)\n",
    "        pd.DataFrame([row]).to_csv(results_csv, mode='a', index=False, header=header)\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        print(\"No validation set available for evaluation\")\n",
    "        # Append limited info to CSV\n",
    "        \n",
    "        row = {\n",
    "            'model': model_name,\n",
    "            'sampling_method': sampling_method,\n",
    "            'remove_outliers': remove_outliers,\n",
    "            'val_accuracy': None,\n",
    "            'val_f1_macro': None,\n",
    "            'best_cv_score': round(float(grid_search.best_score_), 4),\n",
    "            'best_parameters': json.dumps(grid_search.best_params_),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "        header = not os.path.exists(results_csv)\n",
    "        pd.DataFrame([row]).to_csv(results_csv, mode='a', index=False, header=header)\n",
    "\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'sampling_method': sampling_method,\n",
    "            'remove_outliers': remove_outliers,\n",
    "            'best_cv_score': grid_search.best_score_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b47a3",
   "metadata": {},
   "source": [
    "## 4. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Model Testing\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize model saver\n",
    "# Change to project root directory\n",
    "\n",
    "model_saver = create_model_saver(\"src/models/best_simple_models_testing\")\n",
    "\n",
    "# Define experiments to run\n",
    "experiments = [\n",
    "    # Without outlier removal\n",
    "    \n",
    "    (\"XGBoost\", \"No_Sampling\", False),\n",
    "    (\"ANN\", \"No_Sampling\", False),\n",
    "    (\"SVM\", \"No_Sampling\", False),\n",
    "    \n",
    "    # With outlier removal\n",
    "    (\"XGBoost\", \"No_Sampling\", True),\n",
    "    (\"ANN\", \"No_Sampling\", True),\n",
    "    (\"SVM\", \"No_Sampling\", True),\n",
    "    \n",
    "    # With sampling (no outlier removal)\n",
    "    (\"XGBoost\", \"SMOTE\", False),\n",
    "    (\"ANN\", \"SMOTE\", False),\n",
    "    (\"SVM\", \"SMOTE\", False),\n",
    "    \n",
    "    # With sampling (with outlier removal)\n",
    "    (\"SVM\", \"SMOTE\", True),\n",
    "    (\"XGBoost\", \"SMOTE\", True),\n",
    "    (\"ANN\", \"SMOTE\", True),\n",
    "]\n",
    "\n",
    "# Run experiments\n",
    "all_results = []\n",
    "\n",
    "for model_name, sampling_method, remove_outliers in experiments:\n",
    "    try:\n",
    "        result = run_grid_search(\n",
    "            model_name=model_name,\n",
    "            sampling_method=sampling_method,\n",
    "            remove_outliers=remove_outliers,\n",
    "            model_saver=model_saver\n",
    "        )\n",
    "        if result is not None:  # Only append if result is not None\n",
    "            all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error running {model_name} with {sampling_method}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f9c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*100}\")\n",
    "print(\"BEST OVERALL RESULT (FROM ALL RUNS)\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# Load existing results from CSV to find truly best overall result\n",
    "existing_csv = \"reports/03_model_testing_results/model_comparison_best_models.csv\"\n",
    "if os.path.exists(existing_csv):\n",
    "    df_all_results = pd.read_csv(existing_csv)\n",
    "    # Remove rows with missing validation scores\n",
    "\n",
    "    if len(df_all_results) > 0:\n",
    "        best_idx = df_all_results['val_f1_macro'].idxmax()\n",
    "        best_result = df_all_results.loc[best_idx]\n",
    "        print(f\"Best overall model: {best_result['model']}\")\n",
    "        print(f\"Sampling Method: {best_result['sampling_method']}\")\n",
    "        print(f\"Outlier Removal: {best_result['remove_outliers']}\")\n",
    "        print(f\"Validation F1-Macro: {best_result['val_f1_macro']:.4f}\")\n",
    "        print(f\"Validation Accuracy: {best_result['val_accuracy']:.4f}\")\n",
    "        print(f\"Best CV Score: {best_result['best_cv_score']:.4f}\")\n",
    "        print(f\"Best Parameters: {best_result['best_parameters']}\")\n",
    "    else:\n",
    "        print(\"No valid results found in existing CSV.\")\n",
    "else:\n",
    "    print(\"No existing results CSV found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f432d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f9100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
