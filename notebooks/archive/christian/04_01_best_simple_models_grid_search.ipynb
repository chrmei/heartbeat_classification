{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14eab3c2",
   "metadata": {},
   "source": [
    "# 4 Find best \"simple\" model for given dataset\n",
    "\n",
    "This part provides a pipeline for heartbeat classification based on the requirements from `notebooks/03_model_testing_example_mit.ipynb`.\n",
    "\n",
    "### What the notebook does\n",
    "\n",
    "- The notebook builds a leak-free pipeline per experiment:\n",
    "  - [Optional] `StandardScaler` → [Optional] `Sampler` (e.g., SMOTE) → `Classifier`\n",
    "  - This pipeline is passed into `GridSearchCV` with `RepeatedStratifiedKFold`.\n",
    "\n",
    "- Data loading:\n",
    "  - Loads preprocessed MIT-BIH training split (`X_train`, `y_train`) and a held-out validation split (`X_val`, `y_val`) from `data/processed/mitbih/`.\n",
    "  - When `remove_outliers=True`, outlier removal is applied to the training split only; the validation split is never altered.\n",
    "\n",
    "- Inside each CV fold (leak-free):\n",
    "  1. Split the training data into `train_fold` and `val_fold` (internal to CV).\n",
    "  2. Fit `StandardScaler` on `train_fold` only (If Scaling applies)\n",
    "  3. Fit the `Sampler` (e.g., SMOTE) on `train_fold` only.\n",
    "  4. Train the classifier on the resampled `train_fold`.\n",
    "  5. Evaluate on the untouched `val_fold`.\n",
    "  6. Repeat across folds; aggregate metrics and select best hyperparameters.\n",
    "\n",
    "- After CV:\n",
    "  - Refit the best pipeline on the full training data (`X_train`, `y_train`).\n",
    "  - Evaluate on the held-out validation set (`X_val`, `y_val`) which was never sampled or transformed using training information.\n",
    "  - Append results to `reports/03_model_testing_results/04_model_comparison_best_models.csv` and save the fitted model artifact.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- Sampling (oversampling/undersampling) is performed only after each fold’s split and only on the training fold within the CV loop.\n",
    "- The validation fold in CV and the final held-out validation set are kept untouched, preventing information leakage.\n",
    "- This follows best practices consistently recommended in the literature for imbalanced learning and model evaluation.\n",
    "- Accordingly, reported performance (e.g., accuracy/F1) reflects a trustworthy estimate; high scores are plausible on MIT-BIH when methodology is correct.\n",
    "\n",
    "### Minimal data-flow\n",
    "\n",
    "1. Load processed MIT-BIH data:\n",
    "   - `X_train`, `y_train` (base or with outlier removal applied to training only)\n",
    "   - `X_val`, `y_val` (always untouched)\n",
    "2. For each model/sampling setting:\n",
    "   - Construct pipeline: `[Scaler?] -> [Sampler?] -> Classifier`\n",
    "   - Run `GridSearchCV` with `RepeatedStratifiedKFold`\n",
    "     - Per fold: fit scaler/sampler on `train_fold` only; score on `val_fold`\n",
    "3. Select best params; refit on full `X_train`, `y_train`\n",
    "4. Evaluate on `X_val`, `y_val`; log metrics and save model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64239f57",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a1cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/christianm/Projects/Repos/heartbeat_classification\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# Add src to path\n",
    "print(os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    ")\n",
    "from sklearn import set_config\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Sampling\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Custom utilities\n",
    "from src.utils.preprocessing import (\n",
    "    load_processed_dataset,\n",
    "    DatasetSplit,\n",
    "    build_full_suffix as pp_build_full_suffix,\n",
    "    generate_all_processed_datasets,\n",
    "    _normalize_sampling_method_name,\n",
    "    _SAMPLING_REGISTRY\n",
    ")\n",
    "from src.utils.evaluation import evaluate_model\n",
    "from src.utils.model_saver import create_model_saver\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3a6a8",
   "metadata": {},
   "source": [
    "## 2. Constants & Param Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "SCORING = {'f1_macro': 'f1_macro', 'bal_acc': 'balanced_accuracy', 'f1_weighted': 'f1_weighted'}\n",
    "results_csv = \"reports/03_model_testing_results/04_01_model_comparison_grid_search_best_models.csv\"\n",
    "DATA_DIR = \"data/processed/mitbih\"\n",
    "\n",
    "PARAM_SPACES = {\n",
    "    \"XGBoost\": {\n",
    "        \"estimator\": xgb.XGBClassifier(\n",
    "            objective=\"multi:softmax\",\n",
    "            num_class=5,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=\"mlogloss\",\n",
    "        ),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [150, 200, 250, 350, 500],\n",
    "            \"max_depth\": [8, 9],\n",
    "            \"learning_rate\": [0.2],\n",
    "            \"subsample\": [0.7, 0.8],\n",
    "            \"colsample_bytree\": [0.9],\n",
    "            \"reg_alpha\": [0.1, 0.2],\n",
    "            \"reg_lambda\": [0.0, 0.05],\n",
    "            \"min_child_weight\": [5],\n",
    "            \"gamma\": [0.0, 0.05],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
    "    },\n",
    "    \"ANN\": {\n",
    "        \"estimator\": MLPClassifier(\n",
    "            max_iter=300,\n",
    "            early_stopping=True,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_iter_no_change=10,\n",
    "            solver=\"adam\",\n",
    "        ),\n",
    "        \"params\": {\n",
    "            \"hidden_layer_sizes\": [(128, 64)],\n",
    "            \"activation\": [\"relu\"],\n",
    "            \"alpha\": [3e-4],\n",
    "            \"learning_rate_init\": [0.001, 0.0015],\n",
    "            \"batch_size\": [96, 128],\n",
    "            \"beta_1\": [0.9, 0.91],\n",
    "            \"beta_2\": [0.97, 0.974],\n",
    "            \"validation_fraction\": [0.1],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
    "    },\n",
    "    # best: {'clf__kernel': 'rbf', 'clf__gamma': 0,5, 'clf__C': 10}\n",
    "    \"SVM\": {\n",
    "        \"estimator\": SVC(),\n",
    "        \"params\": {\n",
    "            \"kernel\": [\"rbf\"],\n",
    "            \"C\": [10],\n",
    "            \"gamma\": [0.4, 0.5, 0.6],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13354270",
   "metadata": {},
   "source": [
    "## 3. Methods used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f465817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty, Jupyter-native diagram (works in notebooks)\n",
    "def show_pipeline_diagram(pipe: Pipeline) -> None:\n",
    "    set_config(display=\"diagram\")\n",
    "    display(pipe)  # Jupyter display\n",
    "\n",
    "\n",
    "def create_leak_free_pipeline(\n",
    "    model_name: str,\n",
    "    estimator,\n",
    "    sampling_method: Optional[str] = \"none\",\n",
    "    sampler_kwargs: Optional[Dict] = None,\n",
    "    random_state: Optional[int] = 42,\n",
    ") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Build a leak-free pipeline:\n",
    "    - Using imblearn.Pipeline ensures fit/transform of SAMPLER happen within each CV fold on TRAIN only.\n",
    "    \"\"\"\n",
    "    sampler_kwargs = dict(sampler_kwargs or {})\n",
    "\n",
    "    # Provide a default random_state to samplers if not overridden\n",
    "    if random_state is not None and \"random_state\" not in sampler_kwargs:\n",
    "        sampler_kwargs[\"random_state\"] = random_state\n",
    "\n",
    "    internal_name = _normalize_sampling_method_name(sampling_method)\n",
    "\n",
    "    steps = []\n",
    "\n",
    "    SamplerClass = _SAMPLING_REGISTRY[internal_name]\n",
    "    steps.append((\"sampler\", SamplerClass(**sampler_kwargs)))\n",
    "\n",
    "    steps.append((\"classifier\", estimator))\n",
    "    display(steps)\n",
    "    return Pipeline(steps)\n",
    "\n",
    "\n",
    "def prepare_dataset_with_sampling(\n",
    "    data_dir: str = DATA_DIR,\n",
    "    sampling_method: str = \"No_Sampling\",\n",
    "    remove_outliers: bool = False\n",
    ") -> Tuple[np.ndarray, Optional[np.ndarray], np.ndarray, Optional[np.ndarray]]:\n",
    "    \"\"\"Load an existing processed dataset for the given configuration.\n",
    "\n",
    "    Datasets are assumed to be pre-generated by preprocessing utilities. This\n",
    "    function never overwrites or generates new data; it only loads.\n",
    "    \"\"\"\n",
    "    # Ensure all datasets are generated once (no-op if already done)\n",
    "    generate_all_processed_datasets(data_dir=data_dir, only_once=True)\n",
    "\n",
    "    full_suffix = pp_build_full_suffix(sampling_method, remove_outliers)\n",
    "    split = load_processed_dataset(data_dir=data_dir, sampling_suffix=full_suffix)\n",
    "\n",
    "    X_train_res = split.X_train.values\n",
    "    y_train_res = split.y_train.values\n",
    "    X_val = split.X_val.values if split.X_val is not None else None\n",
    "    y_val = split.y_val.values if split.y_val is not None else None\n",
    "\n",
    "    return X_train_res, X_val, y_train_res, y_val\n",
    "\n",
    "\n",
    "def run_grid_search(\n",
    "    model_name: str,\n",
    "    sampling_method: str = \"No_Sampling\",\n",
    "    remove_outliers: bool = False,\n",
    "    model_saver=None,\n",
    "    results_dir: str = \"reports/comprehensive_model_testing\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run GridSearchCV for a specific model and sampling method.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to train\n",
    "        sampling_method: Sampling method to use\n",
    "        remove_outliers: Whether to remove outliers\n",
    "        model_saver: Model saver instance\n",
    "        results_dir: Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running GridSearchCV for {model_name} with {sampling_method}\")\n",
    "    print(f\"Outlier removal: {remove_outliers}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get model configuration\n",
    "    model_config = PARAM_SPACES[model_name]\n",
    "    estimator = model_config[\"estimator\"]\n",
    "    params = model_config[\"params\"]\n",
    "    cv = model_config[\"cv\"]\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_val, y_train, y_val = prepare_dataset_with_sampling(\n",
    "        sampling_method=\"No_Sampling\", # using non-sampled method for training - apply sampling inside pipeline\n",
    "        remove_outliers=remove_outliers\n",
    "    )\n",
    "    \n",
    "    # Create leak-free pipeline\n",
    "    pipeline = create_leak_free_pipeline(model_name, estimator, sampling_method)\n",
    "    \n",
    "    # Adjust parameter names for pipeline\n",
    "    pipeline_params = {}\n",
    "    for param_name, param_values in params.items():\n",
    "        pipeline_params[f'classifier__{param_name}'] = param_values\n",
    "    \n",
    "    # Create experiment name\n",
    "    experiment_name = f\"{sampling_method.lower()}_outliers_{remove_outliers}\"\n",
    "    \n",
    "    # Check if model already exists\n",
    "    if model_saver and model_saver.model_exists(model_name, experiment_name):\n",
    "        print(f\"Model {model_name} already exists for experiment {experiment_name}. Skipping training and CSV append.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Training new model for {model_name}...\")\n",
    "        \n",
    "        # Run GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=pipeline_params,\n",
    "            scoring=SCORING,\n",
    "            refit='f1_macro',\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=3\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Save model if saver is provided\n",
    "        if model_saver:\n",
    "            metadata = {\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'best_score': grid_search.best_score_,\n",
    "                'cv_results': grid_search.cv_results_,\n",
    "                'experiment': experiment_name,\n",
    "                'classifier': model_name,\n",
    "                'sampling_method': sampling_method,\n",
    "                'remove_outliers': remove_outliers,\n",
    "            }\n",
    "            model_saver.save_model(model_name, grid_search, experiment_name, metadata)\n",
    "            print(f\"Model {model_name} saved successfully!\")\n",
    "    \n",
    "    print(f\"Evaluating {model_name} on validation set...\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # For evaluation, we need to fit the model again since pipeline might not be fitted\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_val, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Per-class metrics\n",
    "    labels = np.unique(np.concatenate([y_train, y_val]))\n",
    "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
    "        y_val, y_pred, average=None, labels=labels, zero_division=0\n",
    "    )\n",
    "    \n",
    "    confusion_mat = confusion_matrix(y_val, y_pred, labels=labels)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'sampling_method': sampling_method,\n",
    "        'remove_outliers': remove_outliers,\n",
    "        'best_cv_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'validation_accuracy': accuracy,\n",
    "        'validation_f1_macro': f1_macro,\n",
    "        'validation_precision_macro': precision_macro,\n",
    "        'validation_recall_macro': recall_macro,\n",
    "        'validation_f1_per_class': f1_per_class,\n",
    "        'validation_precision_per_class': precision_per_class,\n",
    "        'validation_recall_per_class': recall_per_class,\n",
    "        'validation_support_per_class': support_per_class,\n",
    "        'confusion_matrix': confusion_mat,\n",
    "        'labels': labels,\n",
    "    }\n",
    "    \n",
    "    print(f\"Validation F1-Macro: {f1_macro:.4f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Append to results CSV\n",
    "    row = {\n",
    "        'sampling_method': sampling_method,\n",
    "        'outliers_removed': remove_outliers,\n",
    "        'model': model_name,\n",
    "        'test_accuracy': round(float(accuracy), 4),\n",
    "        'test_f1_macro': round(float(f1_macro), 4),\n",
    "        'best_cv_score': round(float(grid_search.best_score_), 4),\n",
    "        'best_parameters': json.dumps(grid_search.best_params_),\n",
    "    }\n",
    "    # Add per-class F1 columns\n",
    "    for lbl, f1 in zip(labels, f1_per_class):\n",
    "        row[f'test_f1_cls_{lbl}'] = round(float(f1), 2)\n",
    "\n",
    "    os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "    header = not os.path.exists(results_csv)\n",
    "    pd.DataFrame([row]).to_csv(results_csv, mode='a', index=False, header=header)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b47a3",
   "metadata": {},
   "source": [
    "## 4. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Model Testing\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# Initialize model saver\n",
    "# Change to project root directory\n",
    "\n",
    "model_saver = create_model_saver(\"src/models/best_simple_models_testing\")\n",
    "\n",
    "# Define experiments to run\n",
    "experiments = [\n",
    "    # Without outlier removal\n",
    "    \n",
    "    #(\"XGBoost\", \"No_Sampling\", False),\n",
    "    #(\"ANN\", \"No_Sampling\", False),\n",
    "    #(\"SVM\", \"No_Sampling\", False), # left out sampling without SMOTE - computation time very long\n",
    "    \n",
    "    # With outlier removal\n",
    "    #(\"XGBoost\", \"No_Sampling\", True),\n",
    "    #(\"ANN\", \"No_Sampling\", True),\n",
    "    #(\"SVM\", \"No_Sampling\", True), # left out sampling without SMOTE - computation time very long\n",
    "    \n",
    "    # With sampling (no outlier removal)\n",
    "    (\"XGBoost\", \"SMOTE\", False),\n",
    "    #(\"ANN\", \"SMOTE\", False),\n",
    "    #(\"SVM\", \"SMOTE\", False),\n",
    "    \n",
    "    # With sampling (with outlier removal)\n",
    "    #(\"XGBoost\", \"SMOTE\", True),\n",
    "    #(\"ANN\", \"SMOTE\", True),\n",
    "    #(\"SVM\", \"SMOTE\", True),\n",
    "]\n",
    "\n",
    "# Run experiments\n",
    "all_results = []\n",
    "\n",
    "for model_name, sampling_method, remove_outliers in experiments:\n",
    "    try:\n",
    "        result = run_grid_search(\n",
    "            model_name=model_name,\n",
    "            sampling_method=sampling_method,\n",
    "            remove_outliers=remove_outliers,\n",
    "            model_saver=model_saver\n",
    "        )\n",
    "        if result is not None:  # Only append if result is not None\n",
    "            all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error running {model_name} with {sampling_method}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54f9c33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "BEST OVERALL RESULT (FROM ALL RUNS)\n",
      "====================================================================================================\n",
      "Best overall model: XGBoost\n",
      "Sampling Method: SMOTE\n",
      "Test F1-Macro: 0.9717\n",
      "Test Accuracy: 0.9773\n",
      "Best CV Score: 0.9642\n",
      "Best Parameters: {\"classifier__colsample_bytree\": 0.9, \"classifier__gamma\": 0.05, \"classifier__learning_rate\": 0.1, \"classifier__max_depth\": 9, \"classifier__min_child_weight\": 5, \"classifier__n_estimators\": 250, \"classifier__reg_alpha\": 0.1, \"classifier__reg_lambda\": 0.0, \"classifier__subsample\": 0.7}\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*100}\")\n",
    "print(\"BEST OVERALL RESULT (FROM ALL RUNS)\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# Load existing results from CSV to find truly best overall result\n",
    "existing_csv = results_csv\n",
    "if os.path.exists(existing_csv):\n",
    "    df_all_results = pd.read_csv(existing_csv)\n",
    "\n",
    "    if len(df_all_results) > 0:\n",
    "        best_idx = df_all_results['test_f1_macro'].idxmax()\n",
    "        best_result = df_all_results.loc[best_idx]\n",
    "        print(f\"Best overall model: {best_result['model']}\")\n",
    "        print(f\"Sampling Method: {best_result['sampling_method']}\")\n",
    "        print(f\"Test F1-Macro: {best_result['test_f1_macro']:.4f}\")\n",
    "        print(f\"Test Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "        print(f\"Best CV Score: {best_result['best_cv_score']}\")\n",
    "        print(f\"Best Parameters: {best_result['best_parameters']}\")\n",
    "    else:\n",
    "        print(\"No valid results found in existing CSV.\")\n",
    "else:\n",
    "    print(\"No existing results CSV found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f7ac70",
   "metadata": {},
   "source": [
    "### 5. Script to re-generate results based on created models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.model_saver:Model loaded: src/models/best_simple_models_testing/SVM_smote_outliers_False.joblib\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path=PosixPath('src/models/best_simple_models_testing/SVM_smote_outliers_False.joblib')\n",
      "Loading processed X_train dataset from: data/processed/mitbih/X_train.csv\n",
      "Loading processed y_train dataset from: data/processed/mitbih/y_train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.model_saver:Model loaded: src/models/best_simple_models_testing/SVM_smote_outliers_True.joblib\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote result for SVM / smote_outliers_False\n",
      "model_path=PosixPath('src/models/best_simple_models_testing/SVM_smote_outliers_True.joblib')\n",
      "Loading processed X_train dataset from: data/processed/mitbih/X_train_olr.csv\n",
      "Loading processed y_train dataset from: data/processed/mitbih/y_train_olr.csv\n",
      "Wrote result for SVM / smote_outliers_True\n",
      "Done rebuilding CSV.\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "from src.utils.model_saver import create_model_saver\n",
    "from src.utils.preprocessing import (\n",
    "    load_processed_dataset,\n",
    "    build_full_suffix as pp_build_full_suffix,   # if not exported, import as in your notebook (pp_build_full_suffix alias)\n",
    "    generate_all_processed_datasets,\n",
    ")\n",
    "\n",
    "# Paths and config\n",
    "results_csv = \"reports/03_model_testing_results/04_01_model_comparison_grid_search_best_models.csv\"\n",
    "DATA_DIR = \"data/processed/mitbih\"\n",
    "\n",
    "# The experiments you previously created (match what you trained)\n",
    "experiments = [\n",
    "    # Without outlier removal\n",
    "    (\"XGBoost\", \"No_Sampling\", False),\n",
    "    (\"ANN\", \"No_Sampling\", False),\n",
    "    #(\"SVM\", \"No_Sampling\", False),\n",
    "\n",
    "    # With outlier removal\n",
    "    (\"XGBoost\", \"No_Sampling\", True),\n",
    "    (\"ANN\", \"No_Sampling\", True),\n",
    "    #(\"SVM\", \"No_Sampling\", True),\n",
    "\n",
    "    # With sampling (no outlier removal)\n",
    "    (\"XGBoost\", \"SMOTE\", False),\n",
    "    (\"ANN\", \"SMOTE\", False),\n",
    "    (\"SVM\", \"SMOTE\", False),\n",
    "\n",
    "    # With sampling (with outlier removal)\n",
    "    (\"XGBoost\", \"SMOTE\", True),\n",
    "    (\"ANN\", \"SMOTE\", True),\n",
    "    (\"SVM\", \"SMOTE\", True),\n",
    "]\n",
    "\n",
    "# Create/access model saver in the same location as before\n",
    "model_saver = create_model_saver(\"src/models/best_simple_models_testing\")\n",
    "\n",
    "def prepare_no_leak_data(remove_outliers: bool):\n",
    "    \"\"\"\n",
    "    Load base processed data (no sampling) for training and validation.\n",
    "    Sampling (if any) is inside the saved pipeline, so we use unsampled data.\n",
    "    \"\"\"\n",
    "    generate_all_processed_datasets(data_dir=DATA_DIR, only_once=True)\n",
    "    full_suffix = pp_build_full_suffix(\"No_Sampling\", remove_outliers)\n",
    "    split = load_processed_dataset(data_dir=DATA_DIR, sampling_suffix=full_suffix)\n",
    "\n",
    "    X_train = split.X_train.values\n",
    "    y_train = split.y_train.values\n",
    "    X_val = split.X_val.values if split.X_val is not None else None\n",
    "    y_val = split.y_val.values if split.y_val is not None else None\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def evaluate_and_append(model_name: str, sampling_method: str, remove_outliers: bool):\n",
    "    \"\"\"\n",
    "    Load saved GridSearchCV, re-fit best pipeline on full train, evaluate on held-out val,\n",
    "    and append a row with per-class F1 to the CSV with the standardized schema.\n",
    "    \"\"\"\n",
    "    experiment_name = f\"{sampling_method.lower()}_outliers_{remove_outliers}\"\n",
    "    if not model_saver.model_exists(model_name, experiment_name):\n",
    "        print(f\"Skipping {model_name} / {experiment_name}: model not found.\")\n",
    "        return\n",
    "\n",
    "    # Load model (GridSearchCV)\n",
    "    gs = model_saver.load_model(model_name, experiment_name)\n",
    "    best_model = gs.best_estimator_\n",
    "\n",
    "    # Prepare data\n",
    "    X_train, y_train, X_val, y_val = prepare_no_leak_data(remove_outliers)\n",
    "    if X_val is None or y_val is None:\n",
    "        print(f\"No validation split found for {model_name} / {experiment_name}; writing row without test_* metrics.\")\n",
    "        row = {\n",
    "            \"sampling_method\": sampling_method,\n",
    "            \"outliers_removed\": remove_outliers,\n",
    "            \"model\": model_name,\n",
    "            \"test_accuracy\": None,\n",
    "            \"test_f1_macro\": None,\n",
    "            \"best_cv_score\": round(float(gs.best_score_), 4),\n",
    "            \"best_parameters\": json.dumps(gs.best_params_),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "        header = not os.path.exists(results_csv)\n",
    "        pd.DataFrame([row]).to_csv(results_csv, mode=\"a\", index=False, header=header)\n",
    "        return\n",
    "\n",
    "    # Re-fit on full training set and evaluate on held-out validation\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_val)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_val, y_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    labels = np.unique(np.concatenate([y_train, y_val]))\n",
    "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
    "        y_val, y_pred, average=None, labels=labels, zero_division=0\n",
    "    )\n",
    "    _ = confusion_matrix(y_val, y_pred, labels=labels)  # kept if you want to save later\n",
    "\n",
    "    # Append row in standardized schema (matches model_comparison_with_sampling_randomized_search.csv)\n",
    "    row = {\n",
    "        \"sampling_method\": sampling_method,\n",
    "        \"model\": model_name,\n",
    "        \"test_accuracy\": round(float(accuracy), 4),\n",
    "        \"test_f1_macro\": round(float(f1_macro), 4),\n",
    "        \"best_cv_score\": round(float(gs.best_score_), 4),\n",
    "        \"best_parameters\": json.dumps(gs.best_params_),\n",
    "    }\n",
    "    for lbl, f1 in zip(labels, f1_per_class):\n",
    "        row[f\"test_f1_cls_{lbl}\"] = round(float(f1), 2)\n",
    "\n",
    "    os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "    header = not os.path.exists(results_csv)\n",
    "    pd.DataFrame([row]).to_csv(results_csv, mode=\"a\", index=False, header=header)\n",
    "    print(f\"Wrote result for {model_name} / {experiment_name}\")\n",
    "\n",
    "# Run re-evaluation for all saved experiments\n",
    "for model_name, sampling_method, remove_outliers in experiments:\n",
    "    evaluate_and_append(model_name, sampling_method, remove_outliers)\n",
    "\n",
    "print(\"Done rebuilding CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f9100",
   "metadata": {},
   "source": [
    "# 5. Graphics for GridSearch Run Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "989e87a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute '_PredictScorer' on <module 'sklearn.metrics._scorer' from '/home/christianm/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# LOAD MODEL VIA YOUR MODELSAVER\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model_saver \u001b[38;5;241m=\u001b[39m create_model_saver(BASE_DIR)\n\u001b[0;32m---> 21\u001b[0m gs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEXPERIMENT_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(gs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgs\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/Repos/heartbeat_classification/src/utils/model_saver.py:112\u001b[0m, in \u001b[0;36mModelSaver.load_model\u001b[0;34m(self, classifier_name, experiment_name)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/joblib/numpy_pickle.py:749\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[1;32m    744\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m    746\u001b[0m             \u001b[38;5;66;03m# A memory-mapped array has to be mapped with the endianness\u001b[39;00m\n\u001b[1;32m    747\u001b[0m             \u001b[38;5;66;03m# it has been written with. Other arrays are coerced to the\u001b[39;00m\n\u001b[1;32m    748\u001b[0m             \u001b[38;5;66;03m# native endianness of the host system.\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m                \u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidated_mmap_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/joblib/numpy_pickle.py:626\u001b[0m, in \u001b[0;36m_unpickle\u001b[0;34m(fobj, ensure_native_byte_order, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    624\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 626\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[1;32m    628\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    630\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    634\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1213\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/pickle.py:1538\u001b[0m, in \u001b[0;36m_Unpickler.load_stack_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTACK_GLOBAL requires str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/pickle.py:1582\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28m__import__\u001b[39m(module, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m-> 1582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_getattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/pickle.py:331\u001b[0m, in \u001b[0;36m_getattribute\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    329\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, subpath)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt get attribute \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m                              \u001b[38;5;241m.\u001b[39mformat(name, obj)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, parent\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute '_PredictScorer' on <module 'sklearn.metrics._scorer' from '/home/christianm/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py'>"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.tri as tri\n",
    "from pathlib import Path\n",
    "\n",
    "# Import your existing utilities\n",
    "from src.utils.model_saver import create_model_saver\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ---------------------------------------------------------------------\n",
    "BASE_DIR = \"src/models/best_simple_models_testing\"\n",
    "MODEL_NAME = \"XGBoost\"                     # e.g., \"XGBoost\", \"ANN\", \"SVM\"\n",
    "EXPERIMENT_NAME = \"smote_outliers_False\"   # must match your saved model name\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# LOAD MODEL VIA YOUR MODELSAVER\n",
    "# ---------------------------------------------------------------------\n",
    "model_saver = create_model_saver(BASE_DIR)\n",
    "gs = model_saver.load_model(MODEL_NAME, EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"Loaded model type: {type(gs)}\")\n",
    "print(f\"Best parameters: {gs.best_params_}\")\n",
    "print(f\"Best CV score (refit metric): {gs.best_score_:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# LOAD CV RESULTS INTO DATAFRAME\n",
    "# ---------------------------------------------------------------------\n",
    "results_df = pd.DataFrame(gs.cv_results_)\n",
    "print(\"\\nAvailable test metrics:\")\n",
    "print([c for c in results_df.columns if c.startswith(\"mean_test_\")])\n",
    "\n",
    "# Ensure columns exist\n",
    "if \"mean_test_f1_macro\" not in results_df.columns:\n",
    "    raise KeyError(\"Missing column 'mean_test_f1_macro' in cv_results_. Check your SCORING dict.\")\n",
    "if \"mean_test_bal_acc\" not in results_df.columns:\n",
    "    raise KeyError(\"Missing column 'mean_test_bal_acc' in cv_results_. Check your SCORING dict.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# PLOT 1: Performance trade-off\n",
    "# ---------------------------------------------------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc = plt.scatter(\n",
    "    results_df[\"mean_test_bal_acc\"],\n",
    "    results_df[\"mean_test_f1_macro\"],\n",
    "    c=results_df[\"mean_fit_time\"],\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"k\"\n",
    ")\n",
    "plt.xlabel(\"Balanced Accuracy\")\n",
    "plt.ylabel(\"F1 Macro\")\n",
    "plt.title(f\"{MODEL_NAME}: Performance Trade-off\")\n",
    "plt.colorbar(sc, label=\"Fit Time (s)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# PLOT 2: Parameter influence (example: n_estimators)\n",
    "# ---------------------------------------------------------------------\n",
    "param_col = \"param_classifier__n_estimators\"\n",
    "if param_col in results_df.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sc = plt.scatter(\n",
    "        results_df[param_col].astype(float),\n",
    "        results_df[\"mean_test_f1_macro\"],\n",
    "        c=results_df[\"mean_fit_time\"],\n",
    "        cmap=\"plasma\",\n",
    "        alpha=0.8,\n",
    "        edgecolor=\"k\"\n",
    "    )\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"n_estimators (log scale)\")\n",
    "    plt.ylabel(\"F1 Macro\")\n",
    "    plt.title(f\"{MODEL_NAME}: Effect of n_estimators on F1 Macro\")\n",
    "    plt.colorbar(sc, label=\"Fit Time (s)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Parameter column '{param_col}' not found - skipping parameter influence plot.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
