{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14eab3c2",
   "metadata": {},
   "source": [
    "# 4 Find best \"simple\" model for given dataset + Feature Engineering\n",
    "\n",
    "This part provides a pipeline for heartbeat classification based on the requirements from `notebooks/03_model_testing_example_mit.ipynb`.\n",
    "\n",
    "### What the notebook does\n",
    "\n",
    "- The notebook builds a leak-free pipeline per experiment:\n",
    "  - [Optional] `StandardScaler` → [Optional] `Sampler` (e.g., SMOTE) → `Classifier`\n",
    "  - This pipeline is passed into `GridSearchCV` with `RepeatedStratifiedKFold`.\n",
    "\n",
    "- Data loading:\n",
    "  - Loads preprocessed MIT-BIH training split (`X_train`, `y_train`) and a held-out validation split (`X_val`, `y_val`) from `data/processed/mitbih/`.\n",
    "  - When `remove_outliers=True`, outlier removal is applied to the training split only; the validation split is never altered.\n",
    "  - Applies Transformations: Baseline Wander Removal, Denoising\n",
    "\n",
    "- Inside each CV fold (leak-free):\n",
    "  1. Split the training data into `train_fold` and `val_fold` (internal to CV).\n",
    "  2. Fit `StandardScaler` on `train_fold` only (If Scaling applies)\n",
    "  3. Fit the `Sampler` (e.g., SMOTE) on `train_fold` only.\n",
    "  4. Train the classifier on the resampled `train_fold`.\n",
    "  5. Evaluate on the untouched `val_fold`.\n",
    "  6. Repeat across folds; aggregate metrics and select best hyperparameters.\n",
    "\n",
    "- After CV:\n",
    "  - Refit the best pipeline on the full training data (`X_train`, `y_train`).\n",
    "  - Evaluate on the held-out validation set (`X_val`, `y_val`) which was never sampled or transformed using training information.\n",
    "  - Append results to `reports/03_model_testing_results/04_model_comparison_best_models.csv` and save the fitted model artifact.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- Sampling (oversampling/undersampling) is performed only after each fold’s split and only on the training fold within the CV loop.\n",
    "- The validation fold in CV and the final held-out validation set are kept untouched, preventing information leakage.\n",
    "- This follows best practices consistently recommended in the literature for imbalanced learning and model evaluation.\n",
    "- Accordingly, reported performance (e.g., accuracy/F1) reflects a trustworthy estimate; high scores are plausible on MIT-BIH when methodology is correct.\n",
    "\n",
    "### Minimal data-flow\n",
    "\n",
    "1. Load processed MIT-BIH data:\n",
    "   - `X_train`, `y_train` (base or with outlier removal applied to training only)\n",
    "   - `X_val`, `y_val` (always untouched)\n",
    "2. For each model/sampling setting:\n",
    "   - Construct pipeline: `[Scaler?] -> [Sampler?] -> Classifier`\n",
    "   - Run `GridSearchCV` with `RepeatedStratifiedKFold`\n",
    "     - Per fold: fit scaler/sampler on `train_fold` only; score on `val_fold`\n",
    "3. Select best params; refit on full `X_train`, `y_train`\n",
    "4. Evaluate on `X_val`, `y_val`; log metrics and save model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64239f57",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a1cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/christianm/Projects/Repos/heartbeat_classification\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# Add src to path\n",
    "print(os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    ")\n",
    "from sklearn import set_config\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Sampling\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Custom utilities\n",
    "from src.utils.preprocessing import (\n",
    "    load_processed_dataset,\n",
    "    DatasetSplit,\n",
    "    build_full_suffix as pp_build_full_suffix,\n",
    "    generate_all_processed_datasets,\n",
    "    _normalize_sampling_method_name,\n",
    "    _SAMPLING_REGISTRY\n",
    ")\n",
    "from src.utils.evaluation import evaluate_model\n",
    "from src.utils.model_saver import create_model_saver\n",
    "\n",
    "import numpy as np\n",
    "from typing import Optional, Dict, Union, Tuple\n",
    "from scipy.signal import butter, filtfilt, medfilt\n",
    "import pywt\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "from src.utils.model_saver import create_model_saver\n",
    "from src.utils.preprocessing import (\n",
    "    load_processed_dataset,\n",
    "    build_full_suffix as pp_build_full_suffix,   # if not exported, import as in your notebook (pp_build_full_suffix alias)\n",
    "    generate_all_processed_datasets,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ArrayLike = Union[np.ndarray, list]\n",
    "\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3a6a8",
   "metadata": {},
   "source": [
    "## 2. Constants & Param Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ec4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and config\n",
    "DATA_DIR = \"data/processed/mitbih\"\n",
    "RANDOM_STATE = 42\n",
    "SCORING = {'f1_macro': 'f1_macro', 'bal_acc': 'balanced_accuracy', 'f1_weighted': 'f1_weighted'}\n",
    "results_csv = \"reports/03_model_testing_results/04_02_model_comparison_grid_search_fe_best_models.csv\"\n",
    "\n",
    "PARAM_SPACES = {\n",
    "    \"XGBoost\": {\n",
    "        \"estimator\": xgb.XGBClassifier(\n",
    "            objective=\"multi:softmax\",\n",
    "            num_class=5,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=\"mlogloss\",\n",
    "        ),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [150, 200, 250],\n",
    "            \"max_depth\": [8, 9],\n",
    "            \"learning_rate\": [0.2],\n",
    "            \"subsample\": [0.7, 0.8],\n",
    "            \"colsample_bytree\": [0.9],\n",
    "            \"reg_alpha\": [0.1, 0.2],\n",
    "            \"reg_lambda\": [0.0, 0.05],\n",
    "            \"min_child_weight\": [5],\n",
    "            \"gamma\": [0.0, 0.05],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
    "    },\n",
    "    \"ANN\": {\n",
    "        \"estimator\": MLPClassifier(\n",
    "            max_iter=300,\n",
    "            early_stopping=True,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_iter_no_change=10,\n",
    "            solver=\"adam\",\n",
    "        ),\n",
    "        \"params\": {\n",
    "            \"hidden_layer_sizes\": [(128, 64)],\n",
    "            \"activation\": [\"relu\"],\n",
    "            \"alpha\": [3e-4],\n",
    "            \"learning_rate_init\": [0.001, 0.0015],\n",
    "            \"batch_size\": [96, 128],\n",
    "            \"beta_1\": [0.9, 0.91],\n",
    "            \"beta_2\": [0.97, 0.974],\n",
    "            \"validation_fraction\": [0.1],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
    "    },\n",
    "    # best: {'clf__kernel': 'rbf', 'clf__gamma': 0,5, 'clf__C': 10}\n",
    "    \"SVM\": {\n",
    "        \"estimator\": SVC(),\n",
    "        \"params\": {\n",
    "            \"kernel\": [\"rbf\"],\n",
    "            \"C\": [10],\n",
    "            \"gamma\": [0.4, 0.5, 0.6],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
    "    },\n",
    "}\n",
    "\n",
    "DATA_DIR = \"data/processed/mitbih\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13354270",
   "metadata": {},
   "source": [
    "## 3. Methods used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1201a0",
   "metadata": {},
   "source": [
    "\n",
    "- Baseline wander removal: removes slow drift (<0.5 Hz) caused by respiration/motion so classifiers see a stable isoelectric line. Typical approach is a high‑pass filter around 0.5 Hz or subtracting a smoothed baseline (median/LP filter).\n",
    "- Denoising: suppresses high‑frequency noise (muscle, mains, sensor). Typical approach is a low‑pass or band‑pass (e.g., 0.5–40 Hz) or wavelet soft‑thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b4536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------- Filter helpers ---------\n",
    "def _butter_highpass(cut_hz: float, fs: float, order: int = 3) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    nyq = 0.5 * fs\n",
    "    Wn = cut_hz / nyq\n",
    "    Wn = min(max(Wn, 1e-6), 0.999999)  # clamp\n",
    "    b, a = butter(order, Wn, btype=\"highpass\")\n",
    "    return b, a\n",
    "\n",
    "def _butter_bandpass(low_hz: float, high_hz: float, fs: float, order: int = 4) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    nyq = 0.5 * fs\n",
    "    low = max(low_hz / nyq, 1e-6)\n",
    "    high = min(high_hz / nyq, 0.999999)\n",
    "    if not (low < high):\n",
    "        # fallback to low-pass if bounds cross\n",
    "        b, a = butter(order, high, btype=\"lowpass\")\n",
    "    else:\n",
    "        b, a = butter(order, [low, high], btype=\"bandpass\")\n",
    "    return b, a\n",
    "\n",
    "# --------- Baseline wander removal ---------\n",
    "def remove_baseline(\n",
    "    x: ArrayLike,\n",
    "    fs: float = 125.0,\n",
    "    method: str = \"highpass\",\n",
    "    hp_cut_hz: float = 0.5,\n",
    "    hp_order: int = 3,\n",
    "    median_win_sec: float = 0.6,   # for 'median' method ~ respiratory baseline\n",
    "    median_smooth_sec: float = 0.2 # pre-smoothing for robustness\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    x: 1-D array (single beat) or 2-D array (n_samples, n_points)\n",
    "    method: 'highpass' or 'median'\n",
    "    \"\"\"\n",
    "    X = np.asarray(x, dtype=float)\n",
    "    if X.ndim == 1:\n",
    "        X = X[None, :]\n",
    "\n",
    "    X_out = np.empty_like(X)\n",
    "\n",
    "    if method.lower() == \"highpass\":\n",
    "        b, a = _butter_highpass(hp_cut_hz, fs, order=hp_order)\n",
    "        for i in range(X.shape[0]):\n",
    "            xi = X[i]\n",
    "            # zero-mean before filtering helps with stability on short segments\n",
    "            xi_d = xi - np.mean(xi)\n",
    "            X_out[i] = filtfilt(b, a, xi_d, method=\"gust\")\n",
    "    else:\n",
    "        # Two-stage median baseline estimate (popular ECG heuristic)\n",
    "        k1 = max(3, int(round(median_smooth_sec * fs)) | 1)  # odd\n",
    "        k2 = max(3, int(round(median_win_sec * fs)) | 1)\n",
    "        for i in range(X.shape[0]):\n",
    "            xi = X[i]\n",
    "            smooth = medfilt(xi, kernel_size=k1)\n",
    "            baseline = medfilt(smooth, kernel_size=k2)\n",
    "            X_out[i] = xi - baseline\n",
    "\n",
    "    return X_out.squeeze()\n",
    "\n",
    "# --------- Denoising ---------\n",
    "def denoise_signal(\n",
    "    x: ArrayLike,\n",
    "    fs: float = 125.0,\n",
    "    method: str = \"bandpass\",\n",
    "    bp_low_hz: float = 0.5,\n",
    "    bp_high_hz: float = 40.0,\n",
    "    bp_order: int = 4,\n",
    "    wavelet: str = \"db6\",\n",
    "    wavelet_level: Optional[int] = None,\n",
    "    wavelet_mode: str = \"soft\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    method: 'bandpass' (0.5-40 Hz typical ECG) or 'wavelet'\n",
    "    \"\"\"\n",
    "    X = np.asarray(x, dtype=float)\n",
    "    if X.ndim == 1:\n",
    "        X = X[None, :]\n",
    "\n",
    "    X_out = np.empty_like(X)\n",
    "\n",
    "    if method.lower() == \"bandpass\":\n",
    "        b, a = _butter_bandpass(bp_low_hz, bp_high_hz, fs, order=bp_order)\n",
    "        for i in range(X.shape[0]):\n",
    "            xi = X[i]\n",
    "            X_out[i] = filtfilt(b, a, xi, method=\"gust\")\n",
    "    else:\n",
    "        # Wavelet soft-thresholding (well-suited for ECG)\n",
    "        for i in range(X.shape[0]):\n",
    "            xi = X[i]\n",
    "            coeffs = pywt.wavedec(xi, wavelet=wavelet, level=wavelet_level)\n",
    "            # Universal threshold based on detail coeffs at finest scale\n",
    "            detail = coeffs[-1]\n",
    "            sigma = np.median(np.abs(detail)) / 0.6745 + 1e-12\n",
    "            thr = sigma * np.sqrt(2 * np.log(len(xi)))\n",
    "            coeffs_th = [coeffs[0]] + [pywt.threshold(c, thr, mode=wavelet_mode) for c in coeffs[1:]]\n",
    "            X_out[i] = pywt.waverec(coeffs_th, wavelet=wavelet)[: len(xi)]\n",
    "\n",
    "    return X_out.squeeze()\n",
    "\n",
    "# --------- Combined ---------\n",
    "def preprocess_ecg_segments(\n",
    "    X: np.ndarray,\n",
    "    fs: float = 125.0,\n",
    "    baseline_cfg: Optional[Dict] = None,\n",
    "    denoise_cfg: Optional[Dict] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply baseline removal then denoising to a batch of ECG segments.\n",
    "    Pass per-step configs like:\n",
    "      baseline_cfg={'method':'highpass', 'hp_cut_hz':0.5}\n",
    "      denoise_cfg={'method':'bandpass', 'bp_low_hz':0.5, 'bp_high_hz':40}\n",
    "    \"\"\"\n",
    "    Xp = np.asarray(X, dtype=float)\n",
    "\n",
    "    if baseline_cfg is not None:\n",
    "        Xp = remove_baseline(Xp, fs=fs, **baseline_cfg)\n",
    "\n",
    "    if denoise_cfg is not None:\n",
    "        Xp = denoise_signal(Xp, fs=fs, **denoise_cfg)\n",
    "\n",
    "    return Xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f465817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty, Jupyter-native diagram (works in notebooks)\n",
    "def show_pipeline_diagram(pipe: Pipeline) -> None:\n",
    "    set_config(display=\"diagram\")\n",
    "    display(pipe)  # Jupyter display\n",
    "\n",
    "\n",
    "def create_leak_free_pipeline(\n",
    "    model_name: str,\n",
    "    estimator,\n",
    "    sampling_method: Optional[str] = \"none\",\n",
    "    sampler_kwargs: Optional[Dict] = None,\n",
    "    random_state: Optional[int] = 42,\n",
    ") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Build a leak-free pipeline:\n",
    "    - Using imblearn.Pipeline ensures fit/transform of SAMPLER happen within each CV fold on TRAIN only.\n",
    "    \"\"\"\n",
    "    sampler_kwargs = dict(sampler_kwargs or {})\n",
    "\n",
    "    # Provide a default random_state to samplers if not overridden\n",
    "    if random_state is not None and \"random_state\" not in sampler_kwargs:\n",
    "        sampler_kwargs[\"random_state\"] = random_state\n",
    "\n",
    "    internal_name = _normalize_sampling_method_name(sampling_method)\n",
    "\n",
    "    steps = []\n",
    "\n",
    "    SamplerClass = _SAMPLING_REGISTRY[internal_name]\n",
    "    steps.append((\"sampler\", SamplerClass(**sampler_kwargs)))\n",
    "\n",
    "    steps.append((\"classifier\", estimator))\n",
    "    display(steps)\n",
    "    return Pipeline(steps)\n",
    "\n",
    "\n",
    "def prepare_dataset_with_sampling(\n",
    "    data_dir: str = DATA_DIR,\n",
    "    sampling_method: str = \"No_Sampling\",\n",
    "    remove_outliers: bool = False\n",
    ") -> Tuple[np.ndarray, Optional[np.ndarray], np.ndarray, Optional[np.ndarray]]:\n",
    "    \"\"\"Load an existing processed dataset for the given configuration.\n",
    "\n",
    "    Datasets are assumed to be pre-generated by preprocessing utilities. This\n",
    "    function never overwrites or generates new data; it only loads.\n",
    "    \"\"\"\n",
    "    # Ensure all datasets are generated once (no-op if already done)\n",
    "    generate_all_processed_datasets(data_dir=data_dir, only_once=True)\n",
    "\n",
    "    full_suffix = pp_build_full_suffix(sampling_method, remove_outliers)\n",
    "    split = load_processed_dataset(data_dir=data_dir, sampling_suffix=full_suffix)\n",
    "\n",
    "    X_train_res = split.X_train.values\n",
    "    y_train_res = split.y_train.values\n",
    "    X_val = split.X_val.values if split.X_val is not None else None\n",
    "    y_val = split.y_val.values if split.y_val is not None else None\n",
    "\n",
    "    return X_train_res, X_val, y_train_res, y_val\n",
    "\n",
    "\n",
    "def run_grid_search(\n",
    "    model_name: str,\n",
    "    sampling_method: str = \"No_Sampling\",\n",
    "    remove_outliers: bool = False,\n",
    "    model_saver=None,\n",
    "    results_dir: str = \"reports/comprehensive_model_testing\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run GridSearchCV for a specific model and sampling method.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to train\n",
    "        sampling_method: Sampling method to use\n",
    "        remove_outliers: Whether to remove outliers\n",
    "        model_saver: Model saver instance\n",
    "        results_dir: Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running GridSearchCV for {model_name} with {sampling_method}\")\n",
    "    print(f\"Outlier removal: {remove_outliers}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get model configuration\n",
    "    model_config = PARAM_SPACES[model_name]\n",
    "    estimator = model_config[\"estimator\"]\n",
    "    params = model_config[\"params\"]\n",
    "    cv = model_config[\"cv\"]\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_val, y_train, y_val = prepare_dataset_with_sampling(\n",
    "        sampling_method=\"No_Sampling\", # using non-sampled method for training - apply sampling inside pipeline\n",
    "        remove_outliers=remove_outliers\n",
    "    )\n",
    "\n",
    "    baseline_cfg = {'method': 'highpass', 'hp_cut_hz': 0.5, 'hp_order': 3}\n",
    "    denoise_cfg  = {'method': 'bandpass', 'bp_low_hz': 0.5, 'bp_high_hz': 40.0, 'bp_order': 4}\n",
    "\n",
    "    fs = 125.0  # set your actual sampling rate if known\n",
    "    # denoising\n",
    "    X_train = preprocess_ecg_segments(X_train, fs=fs, baseline_cfg=baseline_cfg, denoise_cfg=denoise_cfg)\n",
    "    X_val   = preprocess_ecg_segments(X_val,   fs=fs, baseline_cfg=baseline_cfg, denoise_cfg=denoise_cfg)\n",
    "\n",
    "    \n",
    "    # Create leak-free pipeline\n",
    "    pipeline = create_leak_free_pipeline(model_name, estimator, sampling_method)\n",
    "    \n",
    "    # Adjust parameter names for pipeline\n",
    "    pipeline_params = {}\n",
    "    for param_name, param_values in params.items():\n",
    "        pipeline_params[f'classifier__{param_name}'] = param_values\n",
    "    \n",
    "    # Create experiment name\n",
    "    experiment_name = f\"{sampling_method.lower()}_outliers_{remove_outliers}\"\n",
    "    \n",
    "    # Check if model already exists\n",
    "    if model_saver and model_saver.model_exists(model_name, experiment_name):\n",
    "        print(f\"Model {model_name} already exists for experiment {experiment_name}. Skipping training and CSV append.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Training new model for {model_name}...\")\n",
    "        \n",
    "        # Run GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=pipeline_params,\n",
    "            scoring=SCORING,\n",
    "            refit='f1_macro',\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=3\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Save model if saver is provided\n",
    "        if model_saver:\n",
    "            metadata = {\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'best_score': grid_search.best_score_,\n",
    "                'cv_results': grid_search.cv_results_,\n",
    "                'experiment': experiment_name,\n",
    "                'classifier': model_name,\n",
    "                'sampling_method': sampling_method,\n",
    "                'remove_outliers': remove_outliers,\n",
    "            }\n",
    "            model_saver.save_model(model_name, grid_search, experiment_name, metadata)\n",
    "            print(f\"Model {model_name} saved successfully!\")\n",
    "    \n",
    "    print(f\"Evaluating {model_name} on validation set...\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # For evaluation, we need to fit the model again since pipeline might not be fitted\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_val, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Per-class metrics\n",
    "    labels = np.unique(np.concatenate([y_train, y_val]))\n",
    "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
    "        y_val, y_pred, average=None, labels=labels, zero_division=0\n",
    "    )\n",
    "    \n",
    "    confusion_mat = confusion_matrix(y_val, y_pred, labels=labels)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'sampling_method': sampling_method,\n",
    "        'remove_outliers': remove_outliers,\n",
    "        'best_cv_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'validation_accuracy': accuracy,\n",
    "        'validation_f1_macro': f1_macro,\n",
    "        'validation_precision_macro': precision_macro,\n",
    "        'validation_recall_macro': recall_macro,\n",
    "        'validation_f1_per_class': f1_per_class,\n",
    "        'validation_precision_per_class': precision_per_class,\n",
    "        'validation_recall_per_class': recall_per_class,\n",
    "        'validation_support_per_class': support_per_class,\n",
    "        'confusion_matrix': confusion_mat,\n",
    "        'labels': labels,\n",
    "    }\n",
    "    \n",
    "    print(f\"Validation F1-Macro: {f1_macro:.4f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Append to results CSV\n",
    "    row = {\n",
    "        'sampling_method': sampling_method,\n",
    "        'outliers_removed': remove_outliers,\n",
    "        'model': model_name,\n",
    "        'test_accuracy': round(float(accuracy), 4),\n",
    "        'test_f1_macro': round(float(f1_macro), 4),\n",
    "        'best_cv_score': round(float(grid_search.best_score_), 4),\n",
    "        'best_parameters': json.dumps(grid_search.best_params_),\n",
    "    }\n",
    "    # Add per-class F1 columns\n",
    "    for lbl, f1 in zip(labels, f1_per_class):\n",
    "        row[f'test_f1_cls_{lbl}'] = round(float(f1), 2)\n",
    "\n",
    "    os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "    header = not os.path.exists(results_csv)\n",
    "    pd.DataFrame([row]).to_csv(results_csv, mode='a', index=False, header=header)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b47a3",
   "metadata": {},
   "source": [
    "## 4. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e765e084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Model Testing\n",
      "================================================================================\n",
      "/home/christianm/Projects/Repos/heartbeat_classification\n",
      "\n",
      "================================================================================\n",
      "Running GridSearchCV for SVM with SMOTE\n",
      "Outlier removal: False\n",
      "================================================================================\n",
      "Loading processed X_train dataset from: data/processed/mitbih/X_train.csv\n",
      "Loading processed y_train dataset from: data/processed/mitbih/y_train.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sampler', SMOTE(random_state=42)), ('classifier', SVC())]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path=PosixPath('src/models/best_simple_models_testing_fe/SVM_smote_outliers_False.joblib')\n",
      "Training new model for SVM...\n",
      "Fitting 15 folds for each of 3 candidates, totalling 45 fits\n",
      "[CV 15/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.899) f1_macro: (test=0.887) f1_weighted: (test=0.978) total time=65.0min\n",
      "[CV 9/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.890) f1_macro: (test=0.891) f1_weighted: (test=0.979) total time=65.3min\n",
      "[CV 6/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.894) f1_macro: (test=0.883) f1_weighted: (test=0.977) total time=65.5min\n",
      "[CV 14/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.895) f1_macro: (test=0.884) f1_weighted: (test=0.976) total time=66.7min\n",
      "[CV 12/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.875) f1_macro: (test=0.883) f1_weighted: (test=0.977) total time=66.9min\n",
      "[CV 8/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.902) f1_macro: (test=0.891) f1_weighted: (test=0.978) total time=67.0min\n",
      "[CV 5/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.890) f1_macro: (test=0.886) f1_weighted: (test=0.978) total time=67.2min\n",
      "[CV 2/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.889) f1_macro: (test=0.886) f1_weighted: (test=0.977) total time=69.4min\n",
      "[CV 5/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.888) f1_macro: (test=0.888) f1_weighted: (test=0.979) total time=69.4min\n",
      "[CV 2/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.892) f1_macro: (test=0.878) f1_weighted: (test=0.975) total time=69.5min\n",
      "[CV 10/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.906) f1_macro: (test=0.891) f1_weighted: (test=0.979) total time=70.7min\n",
      "[CV 8/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.893) f1_macro: (test=0.892) f1_weighted: (test=0.978) total time=70.9min\n",
      "[CV 11/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.889) f1_macro: (test=0.887) f1_weighted: (test=0.978) total time=70.9min\n",
      "[CV 13/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.894) f1_macro: (test=0.887) f1_weighted: (test=0.978) total time=71.0min\n",
      "[CV 7/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.888) f1_macro: (test=0.887) f1_weighted: (test=0.978) total time=71.0min\n",
      "[CV 7/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.892) f1_macro: (test=0.876) f1_weighted: (test=0.977) total time=71.3min\n",
      "[CV 1/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.887) f1_macro: (test=0.888) f1_weighted: (test=0.978) total time=71.7min\n",
      "[CV 4/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.909) f1_macro: (test=0.896) f1_weighted: (test=0.980) total time=71.9min\n",
      "[CV 1/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.893) f1_macro: (test=0.887) f1_weighted: (test=0.978) total time=72.4min\n",
      "[CV 3/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.897) f1_macro: (test=0.890) f1_weighted: (test=0.978) total time=73.0min\n",
      "[CV 4/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.910) f1_macro: (test=0.903) f1_weighted: (test=0.981) total time=76.6min\n",
      "[CV 9/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.889) f1_macro: (test=0.896) f1_weighted: (test=0.979) total time=76.8min\n",
      "[CV 6/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.893) f1_macro: (test=0.890) f1_weighted: (test=0.978) total time=76.9min\n",
      "[CV 3/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.885) f1_macro: (test=0.887) f1_weighted: (test=0.978) total time=79.4min\n",
      "[CV 10/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.903) f1_macro: (test=0.895) f1_weighted: (test=0.979) total time=62.9min\n",
      "[CV 15/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.895) f1_macro: (test=0.890) f1_weighted: (test=0.978) total time=62.0min\n",
      "[CV 12/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.875) f1_macro: (test=0.889) f1_weighted: (test=0.979) total time=64.1min\n",
      "[CV 13/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.892) f1_macro: (test=0.894) f1_weighted: (test=0.979) total time=64.4min\n",
      "[CV 14/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.893) f1_macro: (test=0.892) f1_weighted: (test=0.977) total time=66.8min\n",
      "[CV 11/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.889) f1_macro: (test=0.896) f1_weighted: (test=0.979) total time=70.4min\n",
      "[CV 2/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.892) f1_macro: (test=0.894) f1_weighted: (test=0.978) total time=67.1min\n",
      "[CV 9/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.884) f1_macro: (test=0.897) f1_weighted: (test=0.979) total time=65.7min\n",
      "[CV 3/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.889) f1_macro: (test=0.893) f1_weighted: (test=0.979) total time=67.7min\n",
      "[CV 11/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.891) f1_macro: (test=0.898) f1_weighted: (test=0.979) total time=66.2min\n",
      "[CV 5/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.882) f1_macro: (test=0.886) f1_weighted: (test=0.978) total time=67.4min\n",
      "[CV 4/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.907) f1_macro: (test=0.904) f1_weighted: (test=0.981) total time=68.9min\n",
      "[CV 14/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.889) f1_macro: (test=0.892) f1_weighted: (test=0.978) total time=65.5min\n",
      "[CV 8/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.893) f1_macro: (test=0.898) f1_weighted: (test=0.979) total time=67.5min\n",
      "[CV 13/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.885) f1_macro: (test=0.894) f1_weighted: (test=0.979) total time=66.5min\n",
      "[CV 6/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.894) f1_macro: (test=0.895) f1_weighted: (test=0.979) total time=68.3min\n",
      "[CV 7/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.888) f1_macro: (test=0.888) f1_weighted: (test=0.978) total time=68.6min\n",
      "[CV 10/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.899) f1_macro: (test=0.897) f1_weighted: (test=0.980) total time=68.5min\n",
      "[CV 12/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.871) f1_macro: (test=0.889) f1_weighted: (test=0.979) total time=67.9min\n",
      "[CV 15/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.893) f1_macro: (test=0.891) f1_weighted: (test=0.979) total time=63.5min\n",
      "[CV 1/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.887) f1_macro: (test=0.892) f1_weighted: (test=0.979) total time=73.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.model_saver:Model saved: src/models/best_simple_models_testing_fe/SVM_smote_outliers_False.joblib\n",
      "INFO:src.utils.model_saver:Metadata saved: src/models/best_simple_models_testing_fe/SVM_smote_outliers_False_metadata.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model SVM saved successfully!\n",
      "Evaluating SVM on validation set...\n",
      "Validation F1-Macro: 0.8941\n",
      "Validation Accuracy: 0.9797\n",
      "\n",
      "================================================================================\n",
      "Running GridSearchCV for SVM with SMOTE\n",
      "Outlier removal: True\n",
      "================================================================================\n",
      "Loading processed X_train dataset from: data/processed/mitbih/X_train_olr.csv\n",
      "Loading processed y_train dataset from: data/processed/mitbih/y_train_olr.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sampler', SMOTE(random_state=42)), ('classifier', SVC())]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path=PosixPath('src/models/best_simple_models_testing_fe/SVM_smote_outliers_True.joblib')\n",
      "Training new model for SVM...\n",
      "Fitting 15 folds for each of 3 candidates, totalling 45 fits\n",
      "[CV 7/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.904) f1_macro: (test=0.897) f1_weighted: (test=0.978) total time=60.9min\n",
      "[CV 14/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.902) f1_macro: (test=0.895) f1_weighted: (test=0.979) total time=62.3min\n",
      "[CV 6/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.884) f1_macro: (test=0.894) f1_weighted: (test=0.979) total time=62.6min\n",
      "[CV 3/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.903) f1_macro: (test=0.886) f1_weighted: (test=0.978) total time=62.8min\n",
      "[CV 11/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.891) f1_macro: (test=0.887) f1_weighted: (test=0.977) total time=63.1min\n",
      "[CV 1/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.904) f1_macro: (test=0.891) f1_weighted: (test=0.978) total time=64.1min\n",
      "[CV 9/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.902) f1_macro: (test=0.887) f1_weighted: (test=0.977) total time=64.4min\n",
      "[CV 5/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.888) f1_macro: (test=0.885) f1_weighted: (test=0.977) total time=65.6min\n",
      "[CV 13/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.902) f1_macro: (test=0.895) f1_weighted: (test=0.980) total time=65.7min\n",
      "[CV 2/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.892) f1_macro: (test=0.901) f1_weighted: (test=0.979) total time=66.5min\n",
      "[CV 12/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.888) f1_macro: (test=0.885) f1_weighted: (test=0.978) total time=67.1min\n",
      "[CV 4/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.894) f1_macro: (test=0.892) f1_weighted: (test=0.978) total time=67.1min\n",
      "[CV 4/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.896) f1_macro: (test=0.899) f1_weighted: (test=0.979) total time=67.8min\n",
      "[CV 9/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.895) f1_macro: (test=0.890) f1_weighted: (test=0.977) total time=68.0min\n",
      "[CV 8/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.895) f1_macro: (test=0.891) f1_weighted: (test=0.979) total time=68.1min\n",
      "[CV 15/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.895) f1_macro: (test=0.879) f1_weighted: (test=0.977) total time=68.1min\n",
      "[CV 5/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.883) f1_macro: (test=0.887) f1_weighted: (test=0.977) total time=68.2min\n",
      "[CV 2/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.893) f1_macro: (test=0.898) f1_weighted: (test=0.979) total time=68.4min\n",
      "[CV 3/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.900) f1_macro: (test=0.896) f1_weighted: (test=0.979) total time=69.9min\n",
      "[CV 1/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.907) f1_macro: (test=0.900) f1_weighted: (test=0.980) total time=70.1min\n",
      "[CV 8/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.896) f1_macro: (test=0.889) f1_weighted: (test=0.978) total time=70.8min\n",
      "[CV 10/15] END classifier__C=10, classifier__gamma=0.4, classifier__kernel=rbf; bal_acc: (test=0.910) f1_macro: (test=0.899) f1_weighted: (test=0.980) total time=72.3min\n",
      "[CV 7/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.903) f1_macro: (test=0.903) f1_weighted: (test=0.979) total time=72.7min\n",
      "[CV 6/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.879) f1_macro: (test=0.893) f1_weighted: (test=0.980) total time=73.7min\n",
      "[CV 14/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.898) f1_macro: (test=0.900) f1_weighted: (test=0.979) total time=59.8min\n",
      "[CV 15/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.891) f1_macro: (test=0.883) f1_weighted: (test=0.977) total time=58.9min\n",
      "[CV 11/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.891) f1_macro: (test=0.897) f1_weighted: (test=0.979) total time=60.8min\n",
      "[CV 10/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.905) f1_macro: (test=0.900) f1_weighted: (test=0.980) total time=62.9min\n",
      "[CV 12/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.887) f1_macro: (test=0.890) f1_weighted: (test=0.979) total time=62.8min\n",
      "[CV 13/15] END classifier__C=10, classifier__gamma=0.5, classifier__kernel=rbf; bal_acc: (test=0.904) f1_macro: (test=0.902) f1_weighted: (test=0.981) total time=66.2min\n",
      "[CV 8/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.890) f1_macro: (test=0.897) f1_weighted: (test=0.979) total time=62.5min\n",
      "[CV 7/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.897) f1_macro: (test=0.901) f1_weighted: (test=0.979) total time=63.2min\n",
      "[CV 6/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.876) f1_macro: (test=0.894) f1_weighted: (test=0.980) total time=64.6min\n",
      "[CV 15/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.883) f1_macro: (test=0.883) f1_weighted: (test=0.978) total time=61.2min\n",
      "[CV 9/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.893) f1_macro: (test=0.891) f1_weighted: (test=0.977) total time=64.2min\n",
      "[CV 10/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.904) f1_macro: (test=0.905) f1_weighted: (test=0.981) total time=65.1min\n",
      "[CV 14/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.897) f1_macro: (test=0.905) f1_weighted: (test=0.981) total time=63.2min\n",
      "[CV 1/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.904) f1_macro: (test=0.907) f1_weighted: (test=0.981) total time=69.1min\n",
      "[CV 12/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.886) f1_macro: (test=0.893) f1_weighted: (test=0.979) total time=65.2min\n",
      "[CV 3/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.894) f1_macro: (test=0.894) f1_weighted: (test=0.980) total time=68.2min\n",
      "[CV 5/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.880) f1_macro: (test=0.889) f1_weighted: (test=0.978) total time=66.9min\n",
      "[CV 4/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.889) f1_macro: (test=0.901) f1_weighted: (test=0.979) total time=67.5min\n",
      "[CV 11/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.891) f1_macro: (test=0.899) f1_weighted: (test=0.979) total time=65.9min\n",
      "[CV 2/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.884) f1_macro: (test=0.899) f1_weighted: (test=0.979) total time=68.6min\n",
      "[CV 13/15] END classifier__C=10, classifier__gamma=0.6, classifier__kernel=rbf; bal_acc: (test=0.899) f1_macro: (test=0.900) f1_weighted: (test=0.980) total time=64.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.utils.model_saver:Model saved: src/models/best_simple_models_testing_fe/SVM_smote_outliers_True.joblib\n",
      "INFO:src.utils.model_saver:Metadata saved: src/models/best_simple_models_testing_fe/SVM_smote_outliers_True_metadata.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model SVM saved successfully!\n",
      "Evaluating SVM on validation set...\n",
      "Validation F1-Macro: 0.8902\n",
      "Validation Accuracy: 0.9784\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Model Testing\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# Initialize model saver\n",
    "# Change to project root directory\n",
    "\n",
    "model_saver = create_model_saver(\"src/models/best_simple_models_testing_fe\")\n",
    "\n",
    "# Define experiments to run\n",
    "experiments = [\n",
    "    # Without outlier removal\n",
    "    \n",
    "    #(\"XGBoost\", \"No_Sampling\", False),\n",
    "    #(\"ANN\", \"No_Sampling\", False),\n",
    "    #(\"SVM\", \"No_Sampling\", False),\n",
    "    \n",
    "    # With outlier removal\n",
    "    #(\"XGBoost\", \"No_Sampling\", True),\n",
    "    #(\"ANN\", \"No_Sampling\", True),\n",
    "    #(\"SVM\", \"No_Sampling\", True),\n",
    "    \n",
    "    # With sampling (no outlier removal)\n",
    "    #(\"XGBoost\", \"SMOTE\", False),\n",
    "    #(\"ANN\", \"SMOTE\", False),\n",
    "    (\"SVM\", \"SMOTE\", False),\n",
    "    \n",
    "    # With sampling (with outlier removal)\n",
    "    #(\"XGBoost\", \"SMOTE\", True),\n",
    "    #(\"ANN\", \"SMOTE\", True),\n",
    "    (\"SVM\", \"SMOTE\", True),\n",
    "]\n",
    "\n",
    "# Run experiments\n",
    "all_results = []\n",
    "\n",
    "for model_name, sampling_method, remove_outliers in experiments:\n",
    "    try:\n",
    "        result = run_grid_search(\n",
    "            model_name=model_name,\n",
    "            sampling_method=sampling_method,\n",
    "            remove_outliers=remove_outliers,\n",
    "            model_saver=model_saver\n",
    "        )\n",
    "        if result is not None:  # Only append if result is not None\n",
    "            all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error running {model_name} with {sampling_method}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54f9c33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "BEST OVERALL RESULT (FROM ALL RUNS)\n",
      "====================================================================================================\n",
      "Best overall model: XGBoost\n",
      "Sampling Method: SMOTE\n",
      "Validation (test_*) F1-Macro: 0.9108\n",
      "Validation (test_*) Accuracy: 0.9837\n",
      "Best CV Score: 0.9131\n",
      "Best Parameters: {\"classifier__colsample_bytree\": 0.9, \"classifier__gamma\": 0.05, \"classifier__learning_rate\": 0.2, \"classifier__max_depth\": 9, \"classifier__min_child_weight\": 5, \"classifier__n_estimators\": 250, \"classifier__reg_alpha\": 0.1, \"classifier__reg_lambda\": 0.05, \"classifier__subsample\": 0.7}\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*100}\")\n",
    "print(\"BEST OVERALL RESULT (FROM ALL RUNS)\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# Load existing results from CSV to find truly best overall result\n",
    "if os.path.exists(results_csv):\n",
    "    df_all_results = pd.read_csv(results_csv)\n",
    "\n",
    "    if len(df_all_results) > 0:\n",
    "        best_idx = df_all_results['test_f1_macro'].idxmax()\n",
    "        best_result = df_all_results.loc[best_idx]\n",
    "        print(f\"Best overall model: {best_result['model']}\")\n",
    "        print(f\"Sampling Method: {best_result['sampling_method']}\")\n",
    "        print(f\"Validation (test_*) F1-Macro: {best_result['test_f1_macro']:.4f}\")\n",
    "        print(f\"Validation (test_*) Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "        print(f\"Best CV Score: {best_result['best_cv_score']:.4f}\")\n",
    "        print(f\"Best Parameters: {best_result['best_parameters']}\")\n",
    "    else:\n",
    "        print(\"No valid results found in existing CSV.\")\n",
    "else:\n",
    "    print(\"No existing results CSV found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f7ac70",
   "metadata": {},
   "source": [
    "### 5. Script to re-generate results based on created models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The experiments you previously created (match what you trained)\n",
    "experiments = [\n",
    "    # Without outlier removal\n",
    "    #(\"XGBoost\", \"No_Sampling\", False),\n",
    "    #(\"ANN\", \"No_Sampling\", False),\n",
    "    #(\"SVM\", \"No_Sampling\", False),\n",
    "\n",
    "    # With outlier removal\n",
    "    #(\"XGBoost\", \"No_Sampling\", True),\n",
    "    #(\"ANN\", \"No_Sampling\", True),\n",
    "    #(\"SVM\", \"No_Sampling\", True),\n",
    "\n",
    "    # With sampling (no outlier removal)\n",
    "    #(\"XGBoost\", \"SMOTE\", False),\n",
    "    #(\"ANN\", \"SMOTE\", False),\n",
    "    #(\"SVM\", \"SMOTE\", False),\n",
    "\n",
    "    # With sampling (with outlier removal)\n",
    "    #(\"XGBoost\", \"SMOTE\", True),\n",
    "    #(\"ANN\", \"SMOTE\", True),\n",
    "    #(\"SVM\", \"SMOTE\", True),\n",
    "]\n",
    "\n",
    "# Create/access model saver in the same location as before\n",
    "model_saver = create_model_saver(\"src/models/best_simple_models_testing\")\n",
    "\n",
    "def prepare_no_leak_data(remove_outliers: bool):\n",
    "    \"\"\"\n",
    "    Load base processed data (no sampling) for training and validation.\n",
    "    Sampling (if any) is inside the saved pipeline, so we use unsampled data.\n",
    "    \"\"\"\n",
    "    generate_all_processed_datasets(data_dir=DATA_DIR, only_once=True)\n",
    "    full_suffix = pp_build_full_suffix(\"No_Sampling\", remove_outliers)\n",
    "    split = load_processed_dataset(data_dir=DATA_DIR, sampling_suffix=full_suffix)\n",
    "\n",
    "    X_train = split.X_train.values\n",
    "    y_train = split.y_train.values\n",
    "    X_val = split.X_val.values if split.X_val is not None else None\n",
    "    y_val = split.y_val.values if split.y_val is not None else None\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def evaluate_and_append(model_name: str, sampling_method: str, remove_outliers: bool):\n",
    "    \"\"\"\n",
    "    Load saved GridSearchCV, re-fit best pipeline on full train, evaluate on held-out val,\n",
    "    and append a row with per-class F1 to the CSV with the standardized schema.\n",
    "    \"\"\"\n",
    "    experiment_name = f\"{sampling_method.lower()}_outliers_{remove_outliers}\"\n",
    "    if not model_saver.model_exists(model_name, experiment_name):\n",
    "        print(f\"Skipping {model_name} / {experiment_name}: model not found.\")\n",
    "        return\n",
    "\n",
    "    # Load model (GridSearchCV)\n",
    "    gs = model_saver.load_model(model_name, experiment_name)\n",
    "    best_model = gs.best_estimator_\n",
    "\n",
    "    # Prepare data\n",
    "    X_train, y_train, X_val, y_val = prepare_no_leak_data(remove_outliers)\n",
    "    if X_val is None or y_val is None:\n",
    "        print(f\"No validation split found for {model_name} / {experiment_name}; writing row without test_* metrics.\")\n",
    "        row = {\n",
    "            \"sampling_method\": sampling_method,\n",
    "            \"model\": model_name,\n",
    "            \"test_accuracy\": None,\n",
    "            \"test_f1_macro\": None,\n",
    "            \"best_cv_score\": round(float(gs.best_score_), 4),\n",
    "            \"best_parameters\": json.dumps(gs.best_params_),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "        header = not os.path.exists(results_csv)\n",
    "        pd.DataFrame([row]).to_csv(results_csv, mode=\"a\", index=False, header=header)\n",
    "        return\n",
    "\n",
    "    # Re-fit on full training set and evaluate on held-out validation\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_val)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_val, y_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    labels = np.unique(np.concatenate([y_train, y_val]))\n",
    "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
    "        y_val, y_pred, average=None, labels=labels, zero_division=0\n",
    "    )\n",
    "    _ = confusion_matrix(y_val, y_pred, labels=labels)  # kept if you want to save later\n",
    "\n",
    "    # Append row in standardized schema (matches model_comparison_with_sampling_randomized_search.csv)\n",
    "    row = {\n",
    "        \"sampling_method\": sampling_method,\n",
    "        \"model\": model_name,\n",
    "        \"test_accuracy\": round(float(accuracy), 4),\n",
    "        \"test_f1_macro\": round(float(f1_macro), 4),\n",
    "        \"best_cv_score\": round(float(gs.best_score_), 4),\n",
    "        \"best_parameters\": json.dumps(gs.best_params_),\n",
    "    }\n",
    "    for lbl, f1 in zip(labels, f1_per_class):\n",
    "        row[f\"test_f1_cls_{lbl}\"] = round(float(f1), 2)\n",
    "\n",
    "    os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "    header = not os.path.exists(results_csv)\n",
    "    pd.DataFrame([row]).to_csv(results_csv, mode=\"a\", index=False, header=header)\n",
    "    print(f\"Wrote result for {model_name} / {experiment_name}\")\n",
    "\n",
    "# Run re-evaluation for all saved experiments\n",
    "for model_name, sampling_method, remove_outliers in experiments:\n",
    "    evaluate_and_append(model_name, sampling_method, remove_outliers)\n",
    "\n",
    "print(\"Done rebuilding CSV.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
