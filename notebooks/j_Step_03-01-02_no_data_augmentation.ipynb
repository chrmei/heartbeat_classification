{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0840d64-8d86-4a9f-96ee-3ebcae41b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import time\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import ensemble\n",
    "from sklearn import model_selection\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from scipy.stats import uniform\n",
    "import skops.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aeb8748-d9a8-4f22-9d3a-45b443f4e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import MIT data \n",
    "df_mitbih_train = pd.read_csv('../data/original/mitbih_train.csv', header = None)\n",
    "df_mitbih_test = pd.read_csv('../data/original/mitbih_test.csv', header = None)\n",
    "\n",
    "#define train and test set\n",
    "X_train = df_mitbih_train.drop(187, axis = 1)\n",
    "y_train = df_mitbih_train[187]\n",
    "\n",
    "X_test = df_mitbih_test.drop(187, axis = 1)\n",
    "y_test = df_mitbih_test[187]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c288e16-9303-4fe0-aa13-76869b32a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO DATA AUGMENTATION\n",
    "\n",
    "\n",
    "#Models with parameters to be tested -> change for different training procedures with different models\n",
    "\n",
    "# linear_model.LogisticRegression(), {'C': [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "# LinearDiscriminantAnalysis(), {'solver': ['lsqr','eigen'],'shrinkage': [None, 'auto']}\n",
    "# neighbors.KNeighborsClassifier(), {'n_neighbors':range(2, 31), 'metric': ['minkowski', 'manhattan']}\n",
    "# DecisionTreeClassifier(), {'criterion': [\"gini\", \"entropy\"] , 'max_depth':range(2,20), 'min_samples_split': range(2, 10), 'min_samples_leaf': range(1, 5)}\n",
    "# ensemble.RandomForestClassifier(), = {'n_estimators': [100, 200, 300], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [2, 4], 'max_features': [\"sqrt\", \"log2\", None]}\n",
    "# svm.SVC(), {'C':[0.1,1,10], 'kernel':['rbf','linear', 'poly'], 'gamma':[0.001, 0.1, 0.5]}\n",
    "# MLPClassifier(max_iter=500, early_stopping=True), {'hidden_layer_sizes': [(50,), (100,), (100,50)],'activation': ['relu','tanh'],'solver': ['adam','sgd'],'alpha': [0.0001,0.001]}\n",
    "\n",
    "\n",
    "#RandomizedSearchCV to find best hyperparameter combination\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# ------------------------\n",
    "# 1. Define model\n",
    "# ------------------------\n",
    "clf = linear_model.LogisticRegression() # change for different model\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 2. Parameters to be tested\n",
    "# ------------------------\n",
    "parameters = {'C': [0.01, 0.1, 1, 10, 100, 1000]}  # change for different model\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 3. Cross-Validation: during training split training set in n_splits, one for testing (validation set) the rest for training\n",
    "# ------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 4. RandomizedSearchCV\n",
    "# ------------------------\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=parameters,\n",
    "    n_iter=10,\n",
    "    scoring='f1_macro',\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    return_train_score=True # save f1 scores on training and validation data during cross-validation\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 5. Fit models\n",
    "# ------------------------\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 6. Save RandomSearch results for cross-validation in csv file\n",
    "# ------------------------\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "csv_results_cv_name = 'lin_cv_nsplits5_randomsearch_niter6.csv' # change name of csv file for different model\n",
    "results.to_csv(csv_results_cv_name, index=False)\n",
    "print('RandomSearch results saved as: ', csv_results_cv_name)\n",
    "\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Average F1 on validation data over all 5 CV folds for best parameters: {random_search.best_score_:.4f}\") # Mean F1 on CV validation folds (best parameters)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 7. Application of best model on X_test and save results as confusion matrix and classification report\n",
    "# ------------------------\n",
    "y_pred = random_search.best_estimator_.predict(X_test) \n",
    "print('\\nClassification Report for X_test/y_test:')\n",
    "print(classification_report(y_test, y_pred, digits=6))\n",
    "\n",
    "txt_results_xtest_name = 'lin_cv_nsplits5_randomsearch_niter6_xtest.txt' # change name of txt file for different model\n",
    "\n",
    "with open(txt_results_xtest_name, 'w') as file:\n",
    "    file.write(\"Classifier:\")\n",
    "    file.write('clf = lin_model.LogisticRegression()') # change for used model \n",
    "\n",
    "    file.write(\"\\nData augmentation: No\\n\")\n",
    "\n",
    "    file.write(\"\\nConfusion Matrix:\\n\")\n",
    "    file.write(str(pd.crosstab(y_test, y_pred, colnames=['Predictions'])))\n",
    "\n",
    "    file.write(\"\\n\\nClassification Report for X_test/y_test:\\n\")\n",
    "    file.write(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 8. Save best model\n",
    "# ------------------------\n",
    "best_model = random_search.best_estimator_\n",
    "skops_best_model_name = 'lin_best_model_cv_nsplits5_randomsearch_niter6.skops' # change name of skops file for different model\n",
    "sio.dump(best_model, skops_best_model_name)\n",
    "print('Best model saved as: ', skops_best_model_name)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 9. Plot learning curve (Average Training F1 vs average Validation F1 from cross-validation) and save plot\n",
    "# ------------------------\n",
    "train_scores = results['mean_train_score'].values\n",
    "validation_scores = results['mean_test_score'].values\n",
    "params = [str(p) for p in results['params'].values]\n",
    "param_indices = np.arange(len(train_scores)) \n",
    "png_name_learning_curve_train_val = 'lin_cv_nsplits5_randomsearch_niter6_learning_curve_train_val.png' # change name of png file for different model\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(param_indices+1, train_scores, marker='o', label='Train F1')\n",
    "plt.plot(param_indices+1, validation_scores, marker='x', label='Validation F1')\n",
    "plt.xlabel(\"Parameter combination\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Learning curve: Average Training and Validation F1 during cross-validation\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(png_name_learning_curve_train_val, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\nTotal time: {end - start:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0753adc-a465-4742-bb85-4171900e15e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cba480-7744-486d-9fee-ff60a41b7db1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascientest_project",
   "language": "python",
   "name": "datascientest_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
