{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A 03 I: MIT Hyperparameter tuning: Grid Search for Best Model + Sampling on MIT\n",
        "\n",
        "Fine-tune the best model with the best sampling method from A 02 02.\n",
        "\n",
        "## Content\n",
        "\n",
        "A) MIT-BIH Arrhytmia Dataset\n",
        "\n",
        "1. train/test split: 80%, 20% -> as defined at the beginning of the project to ensure result reproducibility, no duplicates or missing values present\n",
        "2. Hyperparameter tuning using RandomizedSearch with cross validation for the mentioned baseline models and oversampling techniques\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "from typing import Dict, Optional\n",
        "import random \n",
        "\n",
        "from src.utils import evaluate_model\n",
        "from src.visualization import save_cv_diagnostics, save_overfit_diagnostic, save_model_diagnostics, save_roc_curve\n",
        "from src.utils.model_saver import create_model_saver\n",
        "\n",
        "# external \n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix\n",
        ")\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
        "from scipy.stats import loguniform, randint, uniform\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "\n",
        "# Samplers\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from src.utils.preprocessing import (\n",
        "    _normalize_sampling_method_name,\n",
        "    _SAMPLING_REGISTRY\n",
        ")\n",
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "random.seed(RANDOM_STATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_leak_free_pipeline(\n",
        "    model_name: str,\n",
        "    estimator,\n",
        "    sampling_method: Optional[str] = \"none\",\n",
        "    sampler_kwargs: Optional[Dict] = None,\n",
        "    random_state: Optional[int] = 42,\n",
        ") -> Pipeline:\n",
        "    \"\"\"\n",
        "    Build a leak-free pipeline:\n",
        "    - Using imblearn.Pipeline ensures fit/transform of SAMPLER happen within each CV fold on TRAIN only.\n",
        "    \"\"\"\n",
        "    sampler_kwargs = dict(sampler_kwargs or {})\n",
        "\n",
        "    # Provide a default random_state to samplers if not overridden\n",
        "    if random_state is not None and \"random_state\" not in sampler_kwargs:\n",
        "        sampler_kwargs[\"random_state\"] = random_state\n",
        "\n",
        "    internal_name = _normalize_sampling_method_name(sampling_method)\n",
        "\n",
        "    steps = []\n",
        "\n",
        "    SamplerClass = _SAMPLING_REGISTRY[internal_name]\n",
        "    steps.append((\"sampler\", SamplerClass(**sampler_kwargs)))\n",
        "\n",
        "    steps.append((\"classifier\", estimator))\n",
        "    display(steps)\n",
        "    return Pipeline(steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def configure_mlflow_for_minio(experiment_name, mlflow_tracking_uri, minio_endpoint_url):\n",
        "    \"\"\"Configure MLflow to use MinIO. Credentials loaded from .aws/credentials\"\"\"\n",
        "    import os\n",
        "    from configparser import ConfigParser\n",
        "    from pathlib import Path\n",
        "    \n",
        "    # Load credentials from .aws/credentials (project or home directory)\n",
        "    creds_file = Path('.aws/credentials') if Path('.aws/credentials').exists() else Path.home() / '.aws' / 'credentials'\n",
        "    if creds_file.exists():\n",
        "        config = ConfigParser()\n",
        "        config.read(creds_file)\n",
        "        if 'default' in config:\n",
        "            os.environ['AWS_ACCESS_KEY_ID'] = config['default'].get('aws_access_key_id', '')\n",
        "            os.environ['AWS_SECRET_ACCESS_KEY'] = config['default'].get('aws_secret_access_key', '')\n",
        "    else:\n",
        "        raise LookupError(\"No MINIO Credentials found!\")\n",
        "    \n",
        "    os.environ['MLFLOW_S3_ENDPOINT_URL'] = minio_endpoint_url\n",
        "\n",
        "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
        "\n",
        "    # Overwrites existing experiment\n",
        "\n",
        "\n",
        "    # Initialize client\n",
        "    client = MlflowClient(tracking_uri=mlflow_tracking_uri)\n",
        "\n",
        "    # First, get the experiment to find its ID#\n",
        "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "    if experiment:\n",
        "        experiment_id = experiment.experiment_id\n",
        "        print(f\"Found experiment: {experiment_name} (ID: {experiment_id})\")\n",
        "    else:\n",
        "        print(f\"Experiment '{experiment_name}' not found, creating new experiment!\")\n",
        "        experiment_id = client.create_experiment(experiment_name)\n",
        "        \n",
        "    return experiment_id "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_results_to_mlflow(\n",
        "    model_name: str,\n",
        "    search,\n",
        "    eval_results: dict,\n",
        "    summary: dict,\n",
        "    sampling_method: str,\n",
        "    remove_outliers: bool,\n",
        "    results_path: str,\n",
        "    experiment_id: str,\n",
        "    n_iter: int,\n",
        "    cv,\n",
        "    refit_metric: str,\n",
        "    random_state: int,\n",
        "    X_train,\n",
        "    dataset_name: str,\n",
        "    dataset_source: str,\n",
        "    mlflow_tracking_uri: str = \"http://mlflow.home.lan\",\n",
        "    sampler_kwargs: Optional[Dict] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Log experiment results to MLflow.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name of the model/classifier\n",
        "        search: Fitted RandomizedSearchCV object\n",
        "        eval_results: Dictionary with train/test evaluation results\n",
        "        summary: Dictionary with summary metrics\n",
        "        sampling_method: Name of sampling method used\n",
        "        sampler_kwargs: Dictionary with sampler parameters (e.g., k_neighbors, n_neighbors)\n",
        "        remove_outliers: Whether outliers were removed\n",
        "        results_path: Path to results CSV file (used to find plot files)\n",
        "        experiment_id: MLflow experiment ID\n",
        "        n_iter: Number of iterations in RandomizedSearchCV\n",
        "        cv: Cross-validation object\n",
        "        refit_metric: Metric used for refitting\n",
        "        random_state: Random state used\n",
        "        mlflow_tracking_uri: MLflow tracking server URI\n",
        "    \"\"\"\n",
        "    run_name = f\"{model_name}_{sampling_method}_outliers_{remove_outliers}\"\n",
        "    \n",
        "    with mlflow.start_run(run_name=run_name, experiment_id=experiment_id):\n",
        "        # Log tags\n",
        "        try:\n",
        "            dataset = mlflow.data.from_pandas(\n",
        "                X_train, \n",
        "                source=dataset_source,\n",
        "                name=f\"{dataset_name} Training Dataset\"\n",
        "            )\n",
        "            mlflow.log_input(dataset, context=\"training\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not log dataset: {e}\")\n",
        "\n",
        "        mlflow.set_tags({\n",
        "            \"dataset\": \"MIT-BIH\",\n",
        "            \"phase\": \"baseline_models\",\n",
        "            \"model_type\": model_name,\n",
        "            \"sampling_method\": sampling_method,\n",
        "            \"outlier_removal\": str(remove_outliers),\n",
        "        })\n",
        "        \n",
        "        # Log hyperparameters (best_params)\n",
        "        # Convert best_params to string format for MLflow (handles nested pipeline params)\n",
        "        mlflow_params = {}\n",
        "        for key, value in search.best_params_.items():\n",
        "            # Convert to string if needed (MLflow params must be strings)\n",
        "            if isinstance(value, (list, dict)):\n",
        "                mlflow_params[key] = json.dumps(value)\n",
        "            elif value is None:\n",
        "                mlflow_params[key] = \"None\"\n",
        "            else:\n",
        "                mlflow_params[key] = str(value)\n",
        "        \n",
        "        mlflow.log_params(mlflow_params)\n",
        "        \n",
        "        # Log experiment configuration parameters\n",
        "        mlflow.log_params({\n",
        "            \"sampling_method\": sampling_method,\n",
        "            \"remove_outliers\": str(remove_outliers),\n",
        "            \"n_iter\": str(n_iter),\n",
        "            \"cv_n_splits\": str(cv.n_splits),\n",
        "            \"refit_metric\": refit_metric,\n",
        "            \"random_state\": str(random_state),\n",
        "        })\n",
        "        \n",
        "        # Log sampler parameters if available\n",
        "        if sampler_kwargs:\n",
        "            sampler_params = {}\n",
        "            for key, value in sampler_kwargs.items():\n",
        "                # Handle special case where 'smote' is an object instance (for SMOTETomek, SMOTEENN)\n",
        "                if key == \"smote\" and hasattr(value, \"__class__\"):\n",
        "                    # Extract SMOTE parameters from the object\n",
        "                    sampler_params[\"sampler_smote_k_neighbors\"] = str(getattr(value, \"k_neighbors\", \"N/A\"))\n",
        "                    sampler_params[\"sampler_smote_random_state\"] = str(getattr(value, \"random_state\", \"N/A\"))\n",
        "                else:\n",
        "                    # Convert to string for MLflow\n",
        "                    if isinstance(value, (list, dict)):\n",
        "                        sampler_params[f\"sampler_{key}\"] = json.dumps(value)\n",
        "                    elif value is None:\n",
        "                        sampler_params[f\"sampler_{key}\"] = \"None\"\n",
        "                    elif hasattr(value, \"__class__\"):\n",
        "                        # For other object instances, log the class name\n",
        "                        sampler_params[f\"sampler_{key}\"] = value.__class__.__name__\n",
        "                    else:\n",
        "                        sampler_params[f\"sampler_{key}\"] = str(value)\n",
        "            \n",
        "            mlflow.log_params(sampler_params)\n",
        "        \n",
        "        # Log metrics (rest of the function remains the same)\n",
        "        mlflow.log_metrics({\n",
        "            \"best_cv_score\": summary[\"best_cv_score\"],\n",
        "            \"test_f1_macro\": summary[\"test_f1_macro\"],\n",
        "            \"train_f1_macro\": summary[\"train_f1_macro\"],\n",
        "            \"test_accuracy\": summary[\"test_accuracy\"],\n",
        "            \"train_test_diff\": summary[\"train_test_diff\"],\n",
        "            \"cv_mean_val_f1_macro\": summary[\"cv_mean_val_f1_macro\"],\n",
        "            \"cv_std_val_f1_macro\": summary[\"cv_std_val_f1_macro\"],\n",
        "            \"cv_mean_train_f1_macro\": summary[\"cv_mean_train_f1_macro\"],\n",
        "            \"cv_std_train_f1_macro\": summary[\"cv_std_train_f1_macro\"],\n",
        "            \"cv_diff_train_val_f1_macro\": summary[\"cv_diff_train_val_f1_macro\"],\n",
        "            \"cv_mean_val_bal_acc\": summary[\"cv_mean_val_bal_acc\"],\n",
        "            \"cv_std_val_bal_acc\": summary[\"cv_std_val_bal_acc\"],\n",
        "            \"mean_fit_time\": summary[\"mean_fit_time\"],\n",
        "            \"std_fit_time\": summary[\"std_fit_time\"],\n",
        "        })\n",
        "        \n",
        "        # Log ROC-AUC if available\n",
        "        if summary[\"roc_auc\"] is not None:\n",
        "            mlflow.log_metric(\"test_roc_auc\", summary[\"roc_auc\"])\n",
        "        \n",
        "        # Log per-class F1 scores as metrics\n",
        "        for lbl in eval_results[\"labels\"]:\n",
        "            mlflow.log_metric(f\"test_f1_class_{lbl}\", summary[f\"test_f1_class_{lbl}\"])\n",
        "            mlflow.log_metric(f\"train_f1_class_{lbl}\", summary[f\"train_f1_class_{lbl}\"])\n",
        "        \n",
        "        # Log artifacts (plots and CSV files)\n",
        "        base = results_path.replace(\".csv\", \"\")\n",
        "        \n",
        "        # Find all generated plot files\n",
        "        plot_patterns = [\n",
        "            f\"{base}_{model_name}_{sampling_method}_cv_tradeoff.png\",\n",
        "            f\"{base}_{model_name}_{sampling_method}_cv_spread.png\",\n",
        "            f\"{base}_{model_name}_{sampling_method}_cv_learning_curve.png\",\n",
        "            f\"{base}_{model_name}_{sampling_method}_overfit_diag.png\",\n",
        "            f\"{base}_{model_name}_{sampling_method}_roc_curve.png\",\n",
        "        ]\n",
        "        \n",
        "        # Log diagnostic plots\n",
        "        for plot_path in plot_patterns:\n",
        "            if os.path.exists(plot_path):\n",
        "                mlflow.log_artifact(plot_path, \"diagnostics\")\n",
        "        \n",
        "        # Log model diagnostics plot (if it exists)\n",
        "        model_diag_path = f\"{base}_{model_name}_{sampling_method}_model_diagnostics.png\"\n",
        "        if os.path.exists(model_diag_path):\n",
        "            mlflow.log_artifact(model_diag_path, \"diagnostics\")\n",
        "        \n",
        "        # Log CSV files\n",
        "        cv_full_path = results_path.replace(\".csv\", '_'+model_name+'_'+sampling_method.lower()+'_outliers_'+str(remove_outliers)+\"_cv_results.csv\")\n",
        "        if os.path.exists(cv_full_path):\n",
        "            mlflow.log_artifact(cv_full_path, \"data\")\n",
        "        if os.path.exists(results_path):\n",
        "            mlflow.log_artifact(results_path, \"data\")\n",
        "        \n",
        "        # Log the trained model\n",
        "        mlflow.sklearn.log_model(\n",
        "            search.best_estimator_,\n",
        "            name=\"model\",\n",
        "            input_example=X_train.iloc[:1].values if hasattr(X_train, 'iloc') else X_train[:1],\n",
        "            registered_model_name=f\"{model_name}_{sampling_method}\",\n",
        "        )\n",
        "        \n",
        "        run_id = mlflow.active_run().info.run_id\n",
        "        print(f\"âœ… Logged to MLflow: {run_name}\")\n",
        "        print(f\"   Run ID: {run_id}\")\n",
        "        print(f\"   View at: {mlflow_tracking_uri}/#/experiments/{experiment_id}/runs/{run_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_grid_search(model_name, \n",
        "                        estimator,\n",
        "                        params,\n",
        "                        X_train,\n",
        "                        y_train,\n",
        "                        X_test,\n",
        "                        y_test,\n",
        "                        cv,\n",
        "                        results_path,\n",
        "                        sampling_method,\n",
        "                        sampler_kwargs,\n",
        "                        remove_outliers,\n",
        "                        model_saver,\n",
        "                        SCORING,\n",
        "                        verbose,\n",
        "                        refit_metric,\n",
        "                        log_to_mlflow,\n",
        "                        dataset_name,\n",
        "                        dataset_source,\n",
        "                        mlflow_experiment_id,\n",
        "                        mlflow_tracking_uri=\"http://mlflow.home.lan\"\n",
        "                        ) -> Dict:\n",
        "    \"\"\"\n",
        "    Run GridSearchCV for a specific model and sampling method.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name of the model to train\n",
        "        sampling_method: Sampling method to use\n",
        "        remove_outliers: Whether to remove outliers\n",
        "        model_saver: Model saver instance\n",
        "        results_dir: Directory to save results\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with results\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Running GridSearchCV for {model_name} ({sampling_method})\")\n",
        "    print(f\"Outlier removal: {remove_outliers}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Create experiment name\n",
        "    experiment_name = f\"{sampling_method.lower()}_outliers_{remove_outliers}\"\n",
        "\n",
        "    # --- SKIP if model already exists ---\n",
        "    if model_saver and model_saver.model_exists(model_name, experiment_name):\n",
        "        print(f\"  Skipping {model_name} ({experiment_name}) - model already saved.\")\n",
        "        try:\n",
        "            meta = model_saver.load_metadata(model_name, experiment_name)\n",
        "            if meta:\n",
        "                print(f\"    Existing model best_score={meta.get('best_score'):.4f}, \"\n",
        "                      f\"params={meta.get('best_params')}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  (Could not load metadata: {e})\")\n",
        "        return None\n",
        "    # ---------------------------------------------------------------\n",
        "\n",
        "    # Create leak-free pipeline - only applies for sampling methods\n",
        "    if sampling_method != \"No_Sampling\":\n",
        "        estimator = create_leak_free_pipeline(model_name, estimator, sampling_method, sampler_kwargs)\n",
        "        # Adjust parameter names for pipeline\n",
        "        params = {f'classifier__{param_name}': param_values \n",
        "                        for param_name, param_values in params.items()}\n",
        "    \n",
        "\n",
        "    # Run the search\n",
        "    search = GridSearchCV(\n",
        "        estimator=estimator,\n",
        "        param_grid=params,\n",
        "        scoring=SCORING,\n",
        "        refit=refit_metric,\n",
        "        cv=cv,\n",
        "        n_jobs=-1,\n",
        "        verbose=verbose,\n",
        "        return_train_score=True,\n",
        "    )\n",
        "    search.fit(X_train, y_train)\n",
        "\n",
        "    # Save model\n",
        "    experiment_name = f\"{sampling_method.lower()}_outliers_{remove_outliers}\"\n",
        "\n",
        "    # Evaluate best model\n",
        "    eval_results = evaluate_model(search.best_estimator_, X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Summary table (1 row per model)\n",
        "    summary = {\n",
        "        \"model\": model_name,\n",
        "        \"sampling_method\": sampling_method,\n",
        "        \"remove_outliers\": remove_outliers,\n",
        "        \"best_cv_score\": round(search.best_score_, 4), # Best mean validation score from CV (based on refit_metric), higher better!\n",
        "        \"best_params\": json.dumps(search.best_params_), \n",
        "        \"train_f1_macro\": round(eval_results[\"train\"][\"f1_macro\"], 4), # Macro-F1 on training - how well model fits seen data across all classes\n",
        "        \"test_f1_macro\": round(eval_results[\"test\"][\"f1_macro\"], 4), # Macro-F1 on test data - balanced generalization to all classes?\n",
        "        \"test_accuracy\": round(eval_results[\"test\"][\"accuracy\"], 4), # overall proportion of correct predictions \n",
        "        \"train_test_diff\": round(eval_results[\"train\"][\"f1_macro\"] - eval_results[\"test\"][\"f1_macro\"], 4), # Gap between train and test: Over/Underfitting indicator: smaller better!\n",
        "        \"roc_auc\": round(eval_results[\"test\"][\"roc_auc\"], 4) if not np.isnan(eval_results[\"test\"][\"roc_auc\"]) else None, # ROC-AUC on test data: Class separation: closer to 1 better separation!\n",
        "    }\n",
        "\n",
        "    # Log cross-fold metrics for best model\n",
        "\n",
        "    cv_df = pd.DataFrame(search.cv_results_)\n",
        "    best_idx = search.best_index_\n",
        "    summary[\"cv_mean_train_f1_macro\"] = round(cv_df[\"mean_train_f1_macro\"][best_idx],4) # High: model fits training folds well, too hig vs validation: possible overfitting\n",
        "    summary[\"cv_std_train_f1_macro\"]  = round(cv_df[\"std_train_f1_macro\"][best_idx],4) # should be low: stable learning across folds\n",
        "    summary[\"cv_mean_val_f1_macro\"] = round(cv_df[\"mean_test_f1_macro\"][best_idx],4) # balanced per class performance\n",
        "    summary[\"cv_std_val_f1_macro\"] = round(cv_df[\"std_test_f1_macro\"][best_idx],4) # should be low\n",
        "    summary[\"cv_diff_train_val_f1_macro\"] = round(cv_df[\"mean_train_f1_macro\"][best_idx] - cv_df[\"mean_test_f1_macro\"][best_idx],4)\n",
        "    summary[\"cv_mean_val_bal_acc\"] = round(cv_df[\"mean_test_bal_acc\"][best_idx],4) # Higher better: class imbalance  by averaging recall per class\n",
        "    summary[\"cv_std_val_bal_acc\"] = round(cv_df[\"std_test_bal_acc\"][best_idx],4) # should be low\n",
        "    summary[\"mean_fit_time\"] = round(cv_df[\"mean_fit_time\"][best_idx],4) \n",
        "    summary[\"std_fit_time\"] = round(cv_df[\"std_fit_time\"][best_idx],4)\n",
        "\n",
        "    for lbl, f1_val in zip(eval_results[\"labels\"], eval_results[\"test\"][\"f1_per_class\"]):\n",
        "        summary[f\"test_f1_class_{lbl}\"] = round(float(f1_val), 4)\n",
        "\n",
        "    for lbl, f1_val in zip(eval_results[\"labels\"], eval_results[\"train\"][\"f1_per_class\"]):\n",
        "        summary[f\"train_f1_class_{lbl}\"] = round(float(f1_val), 4)\n",
        "\n",
        "    os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
        "    pd.DataFrame([summary]).to_csv(results_path, mode=\"a\", header=not os.path.exists(results_path), index=False)\n",
        "\n",
        "    # Save full CV results for analysis\n",
        "    cv_full_path = results_path.replace(\".csv\", '_'+model_name+'_'+experiment_name+\"_cv_results.csv\")\n",
        "    cv_df.to_csv(cv_full_path, index=False)\n",
        "\n",
        "    # Generate diagnostics / graphics\n",
        "    save_overfit_diagnostic(cv_df, model_name, sampling_method, results_path)\n",
        "    save_cv_diagnostics(cv_df, model_name, sampling_method, results_path)\n",
        "    save_model_diagnostics(eval_results, model_name, sampling_method, results_path)\n",
        "    save_roc_curve(search.best_estimator_, X_test, y_test, model_name, sampling_method, results_path)\n",
        "\n",
        "    print(f\"Saved unified results to {results_path}\")\n",
        "\n",
        "    # Log to MLflow if enabled\n",
        "    if log_to_mlflow and mlflow_experiment_id is not None:\n",
        "        log_results_to_mlflow(\n",
        "            model_name=model_name,\n",
        "            search=search,\n",
        "            eval_results=eval_results,\n",
        "            summary=summary,\n",
        "            sampling_method=sampling_method,\n",
        "            sampler_kwargs=sampler_kwargs,  # ADD THIS LINE\n",
        "            remove_outliers=remove_outliers,\n",
        "            results_path=results_path,\n",
        "            experiment_id=mlflow_experiment_id,\n",
        "            n_iter=0,\n",
        "            cv=cv,\n",
        "            refit_metric=refit_metric,\n",
        "            random_state=RANDOM_STATE,\n",
        "            dataset_name=dataset_name, \n",
        "            dataset_source=dataset_source,\n",
        "            X_train=X_train,\n",
        "            mlflow_tracking_uri=mlflow_tracking_uri,\n",
        "        )\n",
        "\n",
        "    if model_saver:\n",
        "        meta = {\n",
        "            \"best_params\": search.best_params_,\n",
        "            \"best_score\": search.best_score_,\n",
        "            \"cv_results\": search.cv_results_,\n",
        "            \"experiment\": experiment_name,\n",
        "            \"classifier\": model_name,\n",
        "        }\n",
        "        model_saver.save_model(model_name, search, experiment_name, meta)\n",
        "    \n",
        "    print(f\"Saved model {model_name} ({experiment_name})!\")\n",
        "\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Constants & Param Spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "PARAM_SPACES = {\n",
        "    \"XGBoost\": {\n",
        "        \"estimator\": xgb.XGBClassifier(\n",
        "            objective=\"multi:softmax\",\n",
        "            num_class=5,\n",
        "            random_state=RANDOM_STATE,\n",
        "            n_jobs=-1,\n",
        "            eval_metric=\"mlogloss\",\n",
        "        ),\n",
        "        \"params\": {\n",
        "            \"n_estimators\": [150, 200, 250, 350, 500],\n",
        "            \"max_depth\": [8, 9],\n",
        "            \"learning_rate\": [0.2],\n",
        "            \"subsample\": [0.7, 0.8],\n",
        "            \"colsample_bytree\": [0.9],\n",
        "            \"reg_alpha\": [0.1, 0.2],\n",
        "            \"reg_lambda\": [0.0, 0.05],\n",
        "            \"min_child_weight\": [5],\n",
        "            \"gamma\": [0.0, 0.05],\n",
        "        },\n",
        "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
        "    },\n",
        "    \"ANN\": {\n",
        "        \"estimator\": MLPClassifier(\n",
        "            max_iter=300,\n",
        "            early_stopping=True,\n",
        "            random_state=RANDOM_STATE,\n",
        "            n_iter_no_change=10,\n",
        "            solver=\"adam\",\n",
        "        ),\n",
        "        \"params\": {\n",
        "            \"hidden_layer_sizes\": [(128, 64)],\n",
        "            \"activation\": [\"relu\"],\n",
        "            \"alpha\": [3e-4],\n",
        "            \"learning_rate_init\": [0.001, 0.0015],\n",
        "            \"batch_size\": [96, 128],\n",
        "            \"beta_1\": [0.9, 0.91],\n",
        "            \"beta_2\": [0.97, 0.974],\n",
        "            \"validation_fraction\": [0.1],\n",
        "        },\n",
        "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
        "    },\n",
        "    # best: {'clf__kernel': 'rbf', 'clf__gamma': 0,5, 'clf__C': 10}\n",
        "    \"SVM\": {\n",
        "        \"estimator\": SVC(),\n",
        "        \"params\": {\n",
        "            \"kernel\": [\"rbf\"],\n",
        "            \"C\": [10],\n",
        "            \"gamma\": [0.4, 0.5, 0.6],\n",
        "        },\n",
        "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
        "    },\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RANDOM_STATE = 42\n",
        "dataset_name=\"MIT-BIH\"\n",
        "EXPERIMENT_NAME = \"MIT_03_01_GS_SAMPLING\"\n",
        "REDUCED_DATASET = True # 5% of original for testing\n",
        "dirname = 'MIT_03_01_baseline_models_grid_search'\n",
        "results_csv = f\"reports/03_model_testing_results/{dirname}.csv\"\n",
        "SCORING = {'f1_macro': 'f1_macro', 'bal_acc': 'balanced_accuracy', 'f1_weighted': 'f1_weighted'}\n",
        "RESULTS_PATH = f\"reports/03_baseline_models/{EXPERIMENT_NAME}/A_03_01.csv\"\n",
        "minio_endpoint_url = \"http://192.168.178.78:9500\"\n",
        "MLFLOW_TRACKING_URI = \"http://mlflow.home.lan\"#\n",
        "\n",
        "dataset_train=\"data/original/mitbih_train.csv\"\n",
        "\n",
        "\n",
        "#import MIT data \n",
        "df_mitbih_train = pd.read_csv('data/original/mitbih_train.csv', header = None)\n",
        "df_mitbih_test = pd.read_csv('data/original/mitbih_test.csv', header = None)\n",
        "\n",
        "X_train = pd.read_csv('data/processed/mitbih/X_train.csv')\n",
        "y_train = pd.read_csv('data/processed/mitbih/y_train.csv')\n",
        "y_train = y_train['187']\n",
        "\n",
        "X_train_sm = pd.read_csv('data/processed/mitbih/X_train_sm.csv')\n",
        "y_train_sm = pd.read_csv('data/processed/mitbih/y_train_sm.csv')\n",
        "y_train_sm = y_train_sm['187']\n",
        "\n",
        "X_val = pd.read_csv('data/processed/mitbih/X_val.csv')\n",
        "y_val = pd.read_csv('data/processed/mitbih/y_val.csv')\n",
        "\n",
        "X_test = df_mitbih_test.drop(187, axis = 1)\n",
        "y_test = df_mitbih_test[187]\n",
        "\n",
        "print(\"MITBIH dataset - SMOTE\")\n",
        "print(f\"\\tTraining ORIG  size: {df_mitbih_train.shape}\")\n",
        "print(f\"\\tTraining SMOTE size: {X_train_sm.shape}, {y_train_sm.shape}\")\n",
        "print(f\"\\tTest size: {X_test.shape}, {y_test.shape}\")\n",
        "print(f\"\\tVal size: {X_val.shape}, {y_val.shape}\")\n",
        "\n",
        "\n",
        "if REDUCED_DATASET:\n",
        "    EXPERIMENT_NAME + '_RED'\n",
        "\n",
        "    # Subsample training set to 10 % (keeping all classes)\n",
        "    X_train_small, _, y_train_small, _ = train_test_split(\n",
        "        X_train, y_train,\n",
        "        train_size=0.05,\n",
        "        stratify=y_train,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Subsample test set to 10 % as well\n",
        "    X_test_small, _, y_test_small, _ = train_test_split(\n",
        "        X_test, y_test,\n",
        "        train_size=0.05,\n",
        "        stratify=y_test,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(\"Reduced MIT-BIH dataset\")\n",
        "    print(f\"\\tTraining size: {X_train_small.shape}, {y_train_small.shape}\")\n",
        "    print(f\"\\tTest size: {X_test_small.shape}, {y_test_small.shape}\")\n",
        "\n",
        "    # Assign back for your pipeline\n",
        "    X_train, y_train = X_train_small, y_train_small\n",
        "    X_test,  y_test  = X_test_small,  y_test_small\n",
        "\n",
        "\n",
        "experiment_id = configure_mlflow_for_minio(EXPERIMENT_NAME, MLFLOW_TRACKING_URI, minio_endpoint_url)\n",
        "\n",
        "model_saver = create_model_saver(f\"src/models/{dirname}\")\n",
        "\n",
        "sampling_methods = {\n",
        "    'SMOTE': {\"random_state\": RANDOM_STATE, \"k_neighbors\": 5}, \n",
        "}\n",
        "\n",
        "best_models = [\"XGBoost\", \"SVM\", \"KNN\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model_name, param_dict in PARAM_SPACES.items():\n",
        "    for sampler_name, sampler_kwargs in sampling_methods.items():\n",
        "        if sampler_name == \"SMOTE\":\n",
        "            X_train_ = X_train_sm\n",
        "            y_train_ = y_train_sm\n",
        "        elif sampler_name == \"No_Sampling\":\n",
        "            X_train_ = X_train\n",
        "            y_train_ = y_train\n",
        "        elif sampler_name != \"No_Sampling\":\n",
        "            raise ValueError(f\"Unsupported sampling method: {sampler_name}\")\n",
        "            \n",
        "        run_grid_search(model_name, \n",
        "                        estimator=param_dict[\"estimator\"],\n",
        "                        params=param_dict[\"params\"],\n",
        "                        X_train=X_train_,  # Use the loop variable\n",
        "                        y_train=y_train_,  # Use the loop variable\n",
        "                        X_test=X_test,\n",
        "                        y_test=y_test,\n",
        "                        cv=param_dict[\"cv\"],\n",
        "                        results_path=RESULTS_PATH,\n",
        "                        sampling_method=sampler_name,\n",
        "                        sampler_kwargs=sampler_kwargs,\n",
        "                        remove_outliers=False,\n",
        "                        model_saver=model_saver,\n",
        "                        SCORING=SCORING,\n",
        "                        verbose=3,\n",
        "                        refit_metric=\"f1_macro\",\n",
        "                        log_to_mlflow=True,\n",
        "                        dataset_name=dataset_name,\n",
        "                        dataset_source=dataset_train,\n",
        "                        mlflow_experiment_id=experiment_id\n",
        "                        )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
