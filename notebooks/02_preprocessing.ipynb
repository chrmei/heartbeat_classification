{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36877d6",
   "metadata": {},
   "source": [
    "# First Models\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "First: apply preprocessing steps derived from the `notebooks/01_data_exploration.ipynb` analysis so new modeling notebooks can import and reuse consistent logic.\n",
    "\n",
    "Key capabilities:\n",
    "- Load PTBDB (normal/abnormal) and MITBIH (train/test) CSV datasets\n",
    "- Drop duplicates in PTBDB partitions\n",
    "- Provide features/targets split with column 187 as target\n",
    "- Optional float32 downcasting to reduce memory\n",
    "- Compute class weights for imbalanced classification\n",
    "- Provide stratified train/val split helpers\n",
    "- Compute zero-padding start index per row (for diagnostics/feature eng)\n",
    "\n",
    "Signals are already normalized to [0, 1] per the source datasets.\n",
    "Everything developed here is put into src/utils/preprocessing for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f92fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7839c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN_INDEX: int = 187\n",
    "FEATURE_COLUMN_RANGE: slice = slice(0, TARGET_COLUMN_INDEX)  # 0..186\n",
    "RANDOM_STATE: int = 42\n",
    "\n",
    "# MITBIH label mapping for reporting/plots (keep numeric labels for modeling)\n",
    "MITBIH_LABELS_MAP: Dict[int, str] = {0: \"N\", 1: \"S\", 2: \"V\", 3: \"F\", 4: \"Q\"}\n",
    "MITBIH_LABELS_TO_DESC: Dict[str, str] = {\n",
    "    \"N\": \"Normal\",\n",
    "    \"S\": \"Supraventricular premature beat\",\n",
    "    \"V\": \"Premature ventricular contraction\",\n",
    "    \"F\": \"Fusion of V+N\",\n",
    "    \"Q\": \"Unclassified\",\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetSplit:\n",
    "    X_train: pd.DataFrame\n",
    "    X_val: Optional[pd.DataFrame]\n",
    "    X_test: Optional[pd.DataFrame]\n",
    "    y_train: pd.Series\n",
    "    y_val: Optional[pd.Series]\n",
    "    y_test: Optional[pd.Series]\n",
    "    class_weight: Optional[Dict[int, float]]\n",
    "    \n",
    "    # Note: Outlier removal (if enabled) is applied after splitting. Class\n",
    "    # weights are computed on the final training labels so training loop\n",
    "    # can pass them directly to supported estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff1f441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_csv(path: Union[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"Read a CSV without header where each row is a 1D signal of length 188.\n",
    "\n",
    "    Column 187 is the target label.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(str(path), header=None)\n",
    "\n",
    "\n",
    "def load_ptbdb(\n",
    "    data_dir: Union[str, Path] = \"../data/original\",\n",
    "    drop_duplicates: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Load and combine PTBDB normal/abnormal datasets into a single DataFrame.\n",
    "\n",
    "    Returns a single DataFrame with features in columns 0..186 and target in 187.\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    normal = _load_csv(data_dir / \"ptbdb_normal.csv\")\n",
    "    abnormal = _load_csv(data_dir / \"ptbdb_abnormal.csv\")\n",
    "\n",
    "    if drop_duplicates:\n",
    "        # Duplicates were found in exploration; remove them\n",
    "        normal = normal.drop_duplicates()\n",
    "        abnormal = abnormal.drop_duplicates()\n",
    "\n",
    "    df = pd.concat([abnormal, normal], axis=0, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_mitbih(\n",
    "    data_dir: Union[str, Path] = \"../data/original\",\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load MITBIH train and test DataFrames (kept as provided).\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    train = _load_csv(data_dir / \"mitbih_train.csv\")\n",
    "    test = _load_csv(data_dir / \"mitbih_test.csv\")\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def split_features_target(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Return (X, y) where y is column 187 and X are columns 0..186.\"\"\"\n",
    "    X = df.iloc[:, FEATURE_COLUMN_RANGE]\n",
    "    y = df.iloc[:, TARGET_COLUMN_INDEX].astype(int)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def compute_balanced_class_weight(y: Union[pd.Series, np.ndarray]) -> Dict[int, float]:\n",
    "    \"\"\"Compute class weights to counter class imbalance. Useful for many models.\"\"\"\n",
    "    classes = np.unique(y)\n",
    "    weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y)\n",
    "    return {int(cls): float(w) for cls, w in zip(classes, weights)}\n",
    "\n",
    "\n",
    "def find_zero_padding_start(sequence_row: Union[pd.Series, np.ndarray]) -> int:\n",
    "    \"\"\"Return the first index after the last non-zero value scanning from the end.\n",
    "\n",
    "    This matches the exploratory notebook's logic to estimate the beginning of\n",
    "    right-side zero-padding per row.\n",
    "    \"\"\"\n",
    "    if isinstance(sequence_row, pd.Series):\n",
    "        values = sequence_row.values\n",
    "    else:\n",
    "        values = sequence_row\n",
    "\n",
    "    first_zero_index = 0\n",
    "    for i in range(len(values) - 1, -1, -1):\n",
    "        if values[i] != 0:\n",
    "            first_zero_index = ( i + 1 ) / 1.2 # pre-defined from dataset\n",
    "            break\n",
    "    return int(first_zero_index)\n",
    "\n",
    "\n",
    "def compute_zero_padding_feature(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Compute `zero_pad_start` for each row based on feature columns 0..186.\"\"\"\n",
    "    X = df.iloc[:, FEATURE_COLUMN_RANGE]\n",
    "    return X.apply(lambda row: find_zero_padding_start(row), axis=1)\n",
    "\n",
    "\n",
    "def stratified_train_val_split(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    val_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"Create a stratified train/validation split preserving class distribution.\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=val_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y,\n",
    "    )\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "\n",
    "def drop_zero_pad_outliers_with_bounds(\n",
    "    df: pd.DataFrame,\n",
    "    bounds: pd.DataFrame,\n",
    "    zero_pad_start: Optional[pd.Series] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Drop rows whose `zero_pad_start` is outside class-specific bounds.\n",
    "\n",
    "    Rows with unseen classes (not present in `bounds`) are kept unchanged.\n",
    "    \"\"\"\n",
    "    if zero_pad_start is None:\n",
    "        zero_pad_start = compute_zero_padding_feature(df)\n",
    "    target = df.iloc[:, TARGET_COLUMN_INDEX].astype(int)\n",
    "\n",
    "    temp = pd.DataFrame(\n",
    "        {\"zero_pad_start\": zero_pad_start, \"target\": target.values}, index=df.index\n",
    "    )\n",
    "    temp = temp.join(bounds, on=\"target\", how=\"left\")\n",
    "    keep_mask = temp[\"lower\"].isna() | (\n",
    "        (temp[\"zero_pad_start\"] >= temp[\"lower\"]) & (temp[\"zero_pad_start\"] <= temp[\"upper\"])\n",
    "    )\n",
    "    return df.loc[keep_mask]\n",
    "\n",
    "\n",
    "def fit_zero_pad_whisker_bounds(\n",
    "    df: pd.DataFrame,\n",
    "    zero_pad_start: Optional[pd.Series] = None,\n",
    "    whisker_k: float = 1.5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Fit per-class Tukey whisker bounds for `zero_pad_start` on the given df.\n",
    "\n",
    "    Returns a DataFrame indexed by class with columns `lower` and `upper`.\n",
    "    \"\"\"\n",
    "    if zero_pad_start is None:\n",
    "        zero_pad_start = compute_zero_padding_feature(df)\n",
    "    target = df.iloc[:, TARGET_COLUMN_INDEX].astype(int)\n",
    "\n",
    "    temp = pd.DataFrame({\n",
    "        \"zero_pad_start\": zero_pad_start,\n",
    "        \"target\": target.values,\n",
    "    })\n",
    "\n",
    "    quantiles = (\n",
    "        temp.groupby(\"target\")[\"zero_pad_start\"].quantile([0.25, 0.75]).unstack()\n",
    "    )\n",
    "    quantiles = quantiles.rename(columns={0.25: \"q1\", 0.75: \"q3\"})\n",
    "    quantiles[\"iqr\"] = quantiles[\"q3\"] - quantiles[\"q1\"]\n",
    "    quantiles[\"lower\"] = quantiles[\"q1\"] - whisker_k * quantiles[\"iqr\"]\n",
    "    quantiles[\"upper\"] = quantiles[\"q3\"] + whisker_k * quantiles[\"iqr\"]\n",
    "    return quantiles[[\"lower\", \"upper\"]]\n",
    "\n",
    "\n",
    "def prepare_mitbih(\n",
    "    data_dir: Union[str, Path] = \"../data/original\",\n",
    "    val_size: float = 0.1,\n",
    "    random_state: int = 42,\n",
    "    remove_outliers: bool = False,\n",
    "    whisker_k: float = 1.5,\n",
    ") -> DatasetSplit:\n",
    "    \"\"\"Load MITBIH train/test, produce train/val split and class weights.\n",
    "\n",
    "    The original test set is kept for final evaluation. A validation set is\n",
    "    carved out of the provided training set using stratification.\n",
    "    \"\"\"\n",
    "    train_df, test_df = load_mitbih(data_dir=data_dir)\n",
    "    X_train_full, y_train_full = split_features_target(train_df)\n",
    "    X_test, y_test = split_features_target(test_df)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = stratified_train_val_split(\n",
    "        X_train_full, y_train_full, val_size=val_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    if remove_outliers:\n",
    "        # Reassemble dfs to compute zero_pad and apply bounds\n",
    "        train_df = pd.concat([X_train, y_train.rename(\"target\")], axis=1)\n",
    "        val_df = pd.concat([X_val, y_val.rename(\"target\")], axis=1)\n",
    "        test_df = pd.concat([X_test, y_test.rename(\"target\")], axis=1)\n",
    "\n",
    "        zp_train = compute_zero_padding_feature(train_df)\n",
    "        bounds = fit_zero_pad_whisker_bounds(train_df, zp_train, whisker_k=whisker_k)\n",
    "\n",
    "        train_df = drop_zero_pad_outliers_with_bounds(train_df, bounds, zp_train)\n",
    "        val_df = drop_zero_pad_outliers_with_bounds(val_df, bounds)\n",
    "        test_df = drop_zero_pad_outliers_with_bounds(test_df, bounds)\n",
    "\n",
    "        # Split back to X/y\n",
    "        X_train, y_train = split_features_target(train_df)\n",
    "        X_val, y_val = split_features_target(val_df)\n",
    "        X_test, y_test = split_features_target(test_df)\n",
    "\n",
    "    weight_map = compute_balanced_class_weight(y_train)\n",
    "\n",
    "    return DatasetSplit(\n",
    "        X_train=X_train,\n",
    "        X_val=X_val,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_val=y_val,\n",
    "        y_test=y_test,\n",
    "        class_weight=weight_map,\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_ptbdb(\n",
    "    data_dir: Union[str, Path] = \"../data/original\",\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.1,\n",
    "    random_state: int = 42,\n",
    "    remove_outliers: bool = False,\n",
    "    whisker_k: float = 1.5,\n",
    ") -> DatasetSplit:\n",
    "    \"\"\"Load PTBDB and produce stratified train/val/test splits and class weights.\"\"\"\n",
    "    df = load_ptbdb(data_dir=data_dir, drop_duplicates=True)\n",
    "    X, y = split_features_target(df)\n",
    "\n",
    "    # First split: train vs test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Second split: train vs val (from the train_val portion)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        test_size=val_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y_train_val,\n",
    "    )\n",
    "\n",
    "    if remove_outliers:\n",
    "        train_df = pd.concat([X_train, y_train.rename(\"target\")], axis=1)\n",
    "        val_df = pd.concat([X_val, y_val.rename(\"target\")], axis=1)\n",
    "        test_df = pd.concat([X_test, y_test.rename(\"target\")], axis=1)\n",
    "\n",
    "        zp_train = compute_zero_padding_feature(train_df)\n",
    "        bounds = fit_zero_pad_whisker_bounds(train_df, zp_train, whisker_k=whisker_k)\n",
    "\n",
    "        train_df = drop_zero_pad_outliers_with_bounds(train_df, bounds, zp_train)\n",
    "        val_df = drop_zero_pad_outliers_with_bounds(val_df, bounds)\n",
    "        test_df = drop_zero_pad_outliers_with_bounds(test_df, bounds)\n",
    "\n",
    "        X_train, y_train = split_features_target(train_df)\n",
    "        X_val, y_val = split_features_target(val_df)\n",
    "        X_test, y_test = split_features_target(test_df)\n",
    "\n",
    "    weight_map = compute_balanced_class_weight(y_train)\n",
    "\n",
    "    return DatasetSplit(\n",
    "        X_train=X_train,\n",
    "        X_val=X_val,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_val=y_val,\n",
    "        y_test=y_test,\n",
    "        class_weight=weight_map,\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8500b1",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62a0d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb = prepare_ptbdb()  # optional: remove_outliers=True\n",
    "mit = prepare_mitbih() # optional: remove_outliers=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "810b4610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6222</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.782719</td>\n",
       "      <td>0.403109</td>\n",
       "      <td>0.263557</td>\n",
       "      <td>0.172813</td>\n",
       "      <td>0.105206</td>\n",
       "      <td>0.068330</td>\n",
       "      <td>0.076283</td>\n",
       "      <td>0.056038</td>\n",
       "      <td>0.040130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.633437</td>\n",
       "      <td>0.325834</td>\n",
       "      <td>0.081071</td>\n",
       "      <td>0.124515</td>\n",
       "      <td>0.141195</td>\n",
       "      <td>0.129170</td>\n",
       "      <td>0.125291</td>\n",
       "      <td>0.140807</td>\n",
       "      <td>0.120248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14461</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.761387</td>\n",
       "      <td>0.214027</td>\n",
       "      <td>0.088271</td>\n",
       "      <td>0.138251</td>\n",
       "      <td>0.182588</td>\n",
       "      <td>0.165256</td>\n",
       "      <td>0.158001</td>\n",
       "      <td>0.154776</td>\n",
       "      <td>0.157598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7915</th>\n",
       "      <td>0.992216</td>\n",
       "      <td>0.762874</td>\n",
       "      <td>0.368263</td>\n",
       "      <td>0.298802</td>\n",
       "      <td>0.265868</td>\n",
       "      <td>0.149102</td>\n",
       "      <td>0.180838</td>\n",
       "      <td>0.157485</td>\n",
       "      <td>0.192216</td>\n",
       "      <td>0.159880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846457</td>\n",
       "      <td>0.707537</td>\n",
       "      <td>0.425197</td>\n",
       "      <td>0.232283</td>\n",
       "      <td>0.179415</td>\n",
       "      <td>0.170979</td>\n",
       "      <td>0.143982</td>\n",
       "      <td>0.136108</td>\n",
       "      <td>0.141170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "6222   1.000000  0.782719  0.403109  0.263557  0.172813  0.105206  0.068330   \n",
       "2240   1.000000  0.633437  0.325834  0.081071  0.124515  0.141195  0.129170   \n",
       "14461  1.000000  0.761387  0.214027  0.088271  0.138251  0.182588  0.165256   \n",
       "7915   0.992216  0.762874  0.368263  0.298802  0.265868  0.149102  0.180838   \n",
       "2972   1.000000  0.846457  0.707537  0.425197  0.232283  0.179415  0.170979   \n",
       "\n",
       "            7         8         9    ...  177  178  179  180  181  182  183  \\\n",
       "6222   0.076283  0.056038  0.040130  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2240   0.125291  0.140807  0.120248  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "14461  0.158001  0.154776  0.157598  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "7915   0.157485  0.192216  0.159880  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2972   0.143982  0.136108  0.141170  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "       184  185  186  \n",
       "6222   0.0  0.0  0.0  \n",
       "2240   0.0  0.0  0.0  \n",
       "14461  0.0  0.0  0.0  \n",
       "7915   0.0  0.0  0.0  \n",
       "2972   0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 187 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptb.X_test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
