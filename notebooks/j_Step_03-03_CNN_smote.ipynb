{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d328f-d220-4722-a362-25e4bd96fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run on Google Colab to use GPU T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c93d0b-2da6-494a-be5c-c0b962c399b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOversampler\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3585c9f-5063-4232-9b19-8de0daafe70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import MIT data\n",
    "df_mitbih_test = pd.read_csv('data/original/mitbih_test.csv', header = None)\n",
    "\n",
    "X_train = pd.read_csv('data/processed/X_train.csv')\n",
    "y_train = pd.read_csv('data/processed/y_train.csv')\n",
    "y_train = y_train['187']\n",
    "\n",
    "X_train_sm = pd.read_csv('data/processed/X_train_sm.csv')\n",
    "y_train_sm = pd.read_csv('data/processed/y_train_sm.csv')\n",
    "y_train_sm = y_train_sm['187']\n",
    "\n",
    "X_val = pd.read_csv('data/processed/X_val.csv')\n",
    "y_val = pd.read_csv('data/processed/y_val.csv')\n",
    "y_val = y_val['187']\n",
    "\n",
    "X_test = df_mitbih_test.drop(187, axis = 1)\n",
    "y_test = df_mitbih_test[187]\n",
    "\n",
    "\n",
    "# Reshape the data for 1D CNN\n",
    "X_train_sm_cnn = np.expand_dims(X_train_sm, axis=2)\n",
    "X_val_cnn = np.expand_dims(X_val, axis=2)\n",
    "X_test_cnn = np.expand_dims(X_test, axis=2) \n",
    "\n",
    "display(X_train_sm_cnn.shape)\n",
    "display(X_val_cnn.shape)\n",
    "display(X_test_cnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59285ef6-c735-471f-a682-ef14b89432c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot and save validation accuracy and validation loss over epochs from history\n",
    "def plot_training_history(history, save_dir, prefix): \n",
    "    hist = history.history\n",
    "    metrics = [m for m in hist.keys() if not m.startswith('val_')]  \n",
    "\n",
    "    # Create the output folder if it does not exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for m in metrics:\n",
    "        plt.figure()\n",
    "        plt.plot(hist[m], label=f'Train {m}')\n",
    "        if f'val_{m}' in hist:\n",
    "            plt.plot(hist[f'val_{m}'], label=f'Val {m}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(m)\n",
    "        plt.title(f'{m} over epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Construct filename with prefix and filepath with directory and filename\n",
    "        filename = f\"{prefix}_{m}.png\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save the figure\n",
    "        plt.savefig(filepath, format='png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {filepath}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0663cbd-09fa-4067-af23-875237210c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used DNNs\n",
    "\n",
    "#CNN1\n",
    "cnn1 = Sequential([\n",
    "    # First 1D Conv layer\n",
    "    Input((187, 1)),\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Second 1D Conv layer\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Third 1D Conv layer\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(5, activation='softmax')  \n",
    "])\n",
    "\n",
    "\n",
    "#CNN2, CNN4 Paper 2020\n",
    "cnn2 = Sequential([\n",
    "    Input((187, 1)),\n",
    "    \n",
    "    # First Conv1D layer\n",
    "    Conv1D(filters=32, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Third Conv1D layer\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Fourth Conv1D layer (256 filters, kernel size 3)\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "\n",
    "    # Output layer for 5 classes\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "#CNN3, CNN4-2 Paper 2020, BatchNormalization and Dropout layers added\n",
    "cnn3 = Sequential([\n",
    "    Input((187, 1)),\n",
    "    \n",
    "    # First Conv1D layer\n",
    "    Conv1D(filters=32, kernel_size=5, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Third Conv1D layer\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Fourth Conv1D layer (256 filters, kernel size 3)\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Output layer for 5 classes\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "#CNN4, CNN4-3 Paper 2020, more BatchNormalization and Dropout layers added\n",
    "cnn4 = Sequential([\n",
    "    Input((187, 1)),\n",
    "    # First Conv1D layer\n",
    "    Conv1D(filters=32, kernel_size=5, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Third Conv1D layer\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Fourth Conv1D layer (256 filters, kernel size 3)\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Output layer for 5 classes\n",
    "    Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f3101-3dea-4302-9ba0-89798614a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model summary\n",
    "cnn1.summary()\n",
    "\n",
    "cnn2.summary()\n",
    "\n",
    "cnn3.summary()\n",
    "\n",
    "cnn4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529bdb97-73e2-4584-9062-34d6e754d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce lr when plateau\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    factor=0.1,          # Factor by which the learning rate will be reduced\n",
    "    patience=5,          # Number of epochs with no improvement after which learning rate is reduced\n",
    "    min_lr=1e-6,         # Minimum learning rate\n",
    "    verbose=1           \n",
    ")\n",
    "\n",
    "\n",
    "#Compile model, change model when needed\n",
    "cnn1.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=Adam(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"]) \n",
    "\n",
    "\n",
    "#Define where and how to save the best model, note lr and bs\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='../CNN_output/cnn1_sm_lr_bs_epoch_{epoch:02d}_valloss_{val_loss:.4f}.keras',\n",
    "    monitor='val_loss',        # metric to monitor\n",
    "    mode='min',                # because higher accuracy is better\n",
    "    save_best_only=True,       # only save when val_accuracy improves\n",
    "    verbose=1                  # print message when a model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4442cb5-66a8-4cdf-93ee-e99b72ebef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "history = cnn1.fit(                      # change cnn1 when using different model architecture\n",
    "    X_train_sm_cnn,\n",
    "    y_train_sm,\n",
    "    epochs=100,                          # change when needed\n",
    "    batch_size=128,                      # change when needed\n",
    "    validation_data=(X_val_cnn, y_val),  # unaltered validation set\n",
    "    callbacks=[checkpoint]               # add here reduce_lr when needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca563f08-e666-43be-b470-e0e9e8a3eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save training history\n",
    "with open(\"../CNN_output/cnn1_sm_lr_bs_epoch__valloss_.pkl\", \"wb\") as f: #change for model\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "    \n",
    "best_model = load_model('../CNN_output/cnn1_sm_lr_bs_epoch__valloss_.keras') #change for model\n",
    "\n",
    "\n",
    "#prediction of test data\n",
    "test_pred = best_model.predict(X_test_cnn)\n",
    "y_test_class = y_test\n",
    "y_pred_class = np.argmax(test_pred, axis=1)\n",
    "\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_test_class, y_pred_class, digits=4))\n",
    "\n",
    "\n",
    "#confusion matrix\n",
    "print(pd.crosstab(y_test_class, y_pred_class, colnames=['Predictions']))\n",
    "\n",
    "\n",
    "#save results of metrics\n",
    "with open(\"../CNN_output/cnn1_sm_lr_bs_epoch__valloss_.txt\", \"w\") as file: #change for model\n",
    "    \n",
    "    file.write(\"\\nModel: CNN1\\n\")#change for model\n",
    "        \n",
    "    file.write(\"\\nData augmentation: Smote\\n\")\n",
    "    \n",
    "    file.write(\"\\nConfusion Matrix on test set:\\n\")\n",
    "    file.write(str(pd.crosstab(y_test_class, y_pred_class, colnames=['Predictions'])))\n",
    "    \n",
    "    file.write(\"\\n\\nClassification Report on test set:\\n\")\n",
    "    file.write(classification_report(y_test_class, y_pred_class, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab9e7a-3d2c-4f98-8e2c-3e2814934d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save validation accuracy and validation loss over epochs from history\n",
    "plot_training_history(history, save_dir=\"../CNN_output\", prefix=\"cnn1_sm_lr_bs_epoch__valloss_\") #change for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a51331-35be-439a-8a86-bc0417ef783f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascientest_project",
   "language": "python",
   "name": "datascientest_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
