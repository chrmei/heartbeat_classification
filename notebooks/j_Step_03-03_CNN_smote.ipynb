{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d328f-d220-4722-a362-25e4bd96fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run on Google Colab to use GPU T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c93d0b-2da6-494a-be5c-c0b962c399b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Conv1D, MaxPooling1D, Flatten, Add, ReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOversampler\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3585c9f-5063-4232-9b19-8de0daafe70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import MIT data\n",
    "df_mitbih_test = pd.read_csv('data/original/mitbih_test.csv', header = None)\n",
    "\n",
    "X_train = pd.read_csv('data/processed/X_train.csv')\n",
    "y_train = pd.read_csv('data/processed/y_train.csv')\n",
    "y_train = y_train['187']\n",
    "\n",
    "X_train_sm = pd.read_csv('data/processed/X_train_sm.csv')\n",
    "y_train_sm = pd.read_csv('data/processed/y_train_sm.csv')\n",
    "y_train_sm = y_train_sm['187']\n",
    "\n",
    "X_val = pd.read_csv('data/processed/X_val.csv')\n",
    "y_val = pd.read_csv('data/processed/y_val.csv')\n",
    "y_val = y_val['187']\n",
    "\n",
    "X_test = df_mitbih_test.drop(187, axis = 1)\n",
    "y_test = df_mitbih_test[187]\n",
    "\n",
    "\n",
    "# Reshape the data for 1D CNN\n",
    "X_train_sm_cnn = np.expand_dims(X_train_sm, axis=2)\n",
    "X_val_cnn = np.expand_dims(X_val, axis=2)\n",
    "X_test_cnn = np.expand_dims(X_test, axis=2) \n",
    "\n",
    "display(X_train_sm_cnn.shape)\n",
    "display(X_val_cnn.shape)\n",
    "display(X_test_cnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59285ef6-c735-471f-a682-ef14b89432c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot and save validation accuracy and validation loss over epochs from history\n",
    "def plot_training_history(history, save_dir, prefix): \n",
    "    hist = history.history\n",
    "    metrics = [m for m in hist.keys() if not m.startswith('val_')]  \n",
    "\n",
    "    # Create the output folder if it does not exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for m in metrics:\n",
    "        plt.figure()\n",
    "        plt.plot(hist[m], label=f'Train {m}')\n",
    "        if f'val_{m}' in hist:\n",
    "            plt.plot(hist[f'val_{m}'], label=f'Val {m}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(m)\n",
    "        plt.title(f'{m} over epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Construct filename with prefix and filepath with directory and filename\n",
    "        filename = f\"{prefix}_{m}.png\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save the figure\n",
    "        plt.savefig(filepath, format='png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {filepath}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0663cbd-09fa-4067-af23-875237210c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used CNNs\n",
    "\n",
    "#CNN1\n",
    "cnn1 = Sequential([\n",
    "    # First 1D Conv layer\n",
    "    Input((187, 1)),\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Second 1D Conv layer\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Third 1D Conv layer\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(5, activation='softmax')  \n",
    "])\n",
    "\n",
    "\n",
    "#CNN2, CNN4 Paper 2020\n",
    "cnn2 = Sequential([\n",
    "    Input((187, 1)),\n",
    "    \n",
    "    # First Conv1D layer\n",
    "    Conv1D(filters=32, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Third Conv1D layer\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Fourth Conv1D layer (256 filters, kernel size 3)\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "\n",
    "    # Output layer for 5 classes\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "#CNN3, CNN4-2 Paper 2020, BatchNormalization and Dropout layers added\n",
    "cnn3 = Sequential([\n",
    "    Input((187, 1)),\n",
    "    \n",
    "    # First Conv1D layer\n",
    "    Conv1D(filters=32, kernel_size=5, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Third Conv1D layer\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Fourth Conv1D layer (256 filters, kernel size 3)\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Output layer for 5 classes\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "#CNN4, CNN4-3 Paper 2020, more BatchNormalization and Dropout layers added\n",
    "cnn4 = Sequential([\n",
    "    Input((187, 1)),\n",
    "    # First Conv1D layer\n",
    "    Conv1D(filters=32, kernel_size=5, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Third Conv1D layer\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Fourth Conv1D layer (256 filters, kernel size 3)\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Output layer for 5 classes\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "#CNN5, LENET\n",
    "cnn5 = Sequential([\n",
    "    # First Conv1D layer\n",
    "    Conv1D(filters=32, kernel_size=15, activation='relu', input_shape=(187, 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    Conv1D(filters=64, kernel_size=16, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # Flatten and Dense layers\n",
    "    Flatten(),\n",
    "    #(Dropout(rate=0.2))\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(5, activation='softmax') \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f3101-3dea-4302-9ba0-89798614a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model summary\n",
    "cnn1.summary()\n",
    "\n",
    "cnn2.summary()\n",
    "\n",
    "cnn3.summary()\n",
    "\n",
    "cnn4.summary()\n",
    "\n",
    "cnn5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529bdb97-73e2-4584-9062-34d6e754d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce lr when plateau\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    factor=0.1,          # Factor by which the learning rate will be reduced\n",
    "    patience=5,          # Number of epochs with no improvement after which learning rate is reduced\n",
    "    min_lr=1e-6,         # Minimum learning rate\n",
    "    verbose=1           \n",
    ")\n",
    "\n",
    "\n",
    "#Compile model, change model when needed\n",
    "cnn1.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=Adam(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"]) \n",
    "\n",
    "\n",
    "#Define where and how to save the best model, note lr and bs\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='../CNN_output/cnn1_sm_lr_bs_epoch_{epoch:02d}_valloss_{val_loss:.4f}.keras',\n",
    "    monitor='val_loss',        # metric to monitor\n",
    "    mode='min',                # because higher accuracy is better\n",
    "    save_best_only=True,       # only save when val_accuracy improves\n",
    "    verbose=1                  # print message when a model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4442cb5-66a8-4cdf-93ee-e99b72ebef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "history = cnn1.fit(                      # change cnn1 when using different model architecture\n",
    "    X_train_sm_cnn,\n",
    "    y_train_sm,\n",
    "    epochs=100,                          # change when needed\n",
    "    batch_size=128,                      # change when needed\n",
    "    validation_data=(X_val_cnn, y_val),  # unaltered validation set\n",
    "    callbacks=[checkpoint]               # add here reduce_lr when needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca563f08-e666-43be-b470-e0e9e8a3eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save training history\n",
    "with open(\"../CNN_output/cnn1_sm_lr_bs_epoch__valloss_.pkl\", \"wb\") as f: #change for model\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "    \n",
    "best_model = load_model('../CNN_output/cnn1_sm_lr_bs_epoch__valloss_.keras') #change for model\n",
    "\n",
    "\n",
    "#prediction of test data\n",
    "test_pred = best_model.predict(X_test_cnn)\n",
    "y_test_class = y_test\n",
    "y_pred_class = np.argmax(test_pred, axis=1)\n",
    "\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_test_class, y_pred_class, digits=4))\n",
    "\n",
    "\n",
    "#confusion matrix\n",
    "print(pd.crosstab(y_test_class, y_pred_class, colnames=['Predictions']))\n",
    "\n",
    "\n",
    "#save results of metrics\n",
    "with open(\"../CNN_output/cnn1_sm_lr_bs_epoch__valloss_.txt\", \"w\") as file: #change for model\n",
    "    \n",
    "    file.write(\"\\nModel: CNN1\\n\")#change for model\n",
    "        \n",
    "    file.write(\"\\nData augmentation: Smote\\n\")\n",
    "    \n",
    "    file.write(\"\\nConfusion Matrix on test set:\\n\")\n",
    "    file.write(str(pd.crosstab(y_test_class, y_pred_class, colnames=['Predictions'])))\n",
    "    \n",
    "    file.write(\"\\n\\nClassification Report on test set:\\n\")\n",
    "    file.write(classification_report(y_test_class, y_pred_class, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab9e7a-3d2c-4f98-8e2c-3e2814934d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save validation accuracy and validation loss over epochs from history\n",
    "plot_training_history(history, save_dir=\"../CNN_output\", prefix=\"cnn1_sm_lr_bs_epoch__valloss_\") #change for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a51331-35be-439a-8a86-bc0417ef783f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e579f1-3027-459b-ac3e-5465b578aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Paper 2018\n",
    "#CNN6, Paper 2018\n",
    "# Input layer\n",
    "input_layer = Input(shape=(187, 1))\n",
    "\n",
    "# First Residual Block\n",
    "conv1_1 = Conv1D(filters=32, kernel_size=5, padding='same')(input_layer)\n",
    "relu1_1 = ReLU()(conv1_1)\n",
    "conv2_1 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_1)\n",
    "skip_connection_1 = Add()([input_layer, conv2_1])\n",
    "relu2_1 = ReLU()(skip_connection_1)\n",
    "pool_1 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_1)\n",
    "\n",
    "# Second Residual Block\n",
    "conv1_2 = Conv1D(filters=32, kernel_size=5, padding='same')(pool_1)\n",
    "relu1_2 = ReLU()(conv1_2)\n",
    "conv2_2 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_2)\n",
    "skip_connection_2 = Add()([pool_1, conv2_2])\n",
    "relu2_2 = ReLU()(skip_connection_2)\n",
    "pool_2 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_2)\n",
    "\n",
    "# Third Residual Block\n",
    "conv1_3 = Conv1D(filters=32, kernel_size=5, padding='same')(pool_2)\n",
    "relu1_3 = ReLU()(conv1_3)\n",
    "conv2_3 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_3)\n",
    "skip_connection_3 = Add()([pool_2, conv2_3])\n",
    "relu2_3 = ReLU()(skip_connection_3)\n",
    "pool_3 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_3)\n",
    "\n",
    "# Fourth Residual Block\n",
    "conv1_4 = Conv1D(filters=32, kernel_size=5, padding='same')(pool_3)\n",
    "relu1_4 = ReLU()(conv1_4)\n",
    "conv2_4 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_4)\n",
    "skip_connection_4 = Add()([pool_3, conv2_4])\n",
    "relu2_4 = ReLU()(skip_connection_4)\n",
    "pool_4 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_4)\n",
    "\n",
    "# Fifth Residual Block\n",
    "conv1_5 = Conv1D(filters=32, kernel_size=5, padding='same')(pool_4)\n",
    "relu1_5 = ReLU()(conv1_5)\n",
    "conv2_5 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_5)\n",
    "skip_connection_5 = Add()([pool_4, conv2_5])\n",
    "relu2_5 = ReLU()(skip_connection_5)\n",
    "pool_5 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_5)\n",
    "\n",
    "# Fully connected layers\n",
    "flatten = Flatten()(pool_5)\n",
    "fc1 = Dense(32, activation='relu')(flatten)\n",
    "fc2 = Dense(32, activation='relu')(fc1)\n",
    "\n",
    "# Softmax layer\n",
    "output_layer = Dense(5, activation='softmax')(fc2) \n",
    "\n",
    "# Model\n",
    "cnn6 = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5437d4-464f-4e92-9a96-0bde6306da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec9628f-b33e-45af-9a17-f3db3955605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate with exponential decay\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.75,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Adam optimizer with specified hyperparameters\n",
    "optimizer = Adam(\n",
    "    learning_rate=lr_schedule,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "cnn6.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# Define where and how to save the best model\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='../CNN_output/cnn6_sm_lr_bs_epoch_{epoch:02d}_valloss_{val_loss:.4f}.keras',   # file path (can be .keras or .h5)\n",
    "    monitor='val_loss',        # metric to monitor\n",
    "    mode='min',                    # because higher accuracy is better\n",
    "    save_best_only=True,           # only save when val_accuracy improves\n",
    "    verbose=1                      # print message when a model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3f990-e9cc-48a0-9435-2645f35069de",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn6.fit(\n",
    "    X_train_sm_cnn,\n",
    "    y_train_sm,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_cnn, y_val),  \n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f5597-0c9f-4ced-b3f5-9dfd0ffd27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save training history\n",
    "with open(\"../CNN_output/cnn6_sm_lr_bs_epoch__valloss_.pkl\", \"wb\") as f: #change for model\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "    \n",
    "best_model = load_model('../CNN_output/cnn6_sm_lr_bs_epoch__valloss_.keras') #change for model\n",
    "\n",
    "\n",
    "#prediction of test data\n",
    "test_pred = best_model.predict(X_test_cnn)\n",
    "y_test_class = y_test\n",
    "y_pred_class = np.argmax(test_pred, axis=1)\n",
    "\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_test_class, y_pred_class, digits=4))\n",
    "\n",
    "\n",
    "#confusion matrix\n",
    "print(pd.crosstab(y_test_class, y_pred_class, colnames=['Predictions']))\n",
    "\n",
    "\n",
    "#save results of metrics\n",
    "with open(\"../CNN_output/cnn6_sm_lr_bs_epoch__valloss_.txt\", \"w\") as file: #change for model\n",
    "    \n",
    "    file.write(\"\\nModel: CNN6\\n\")#change for model\n",
    "        \n",
    "    file.write(\"\\nData augmentation: Smote\\n\")\n",
    "    \n",
    "    file.write(\"\\nConfusion Matrix on test set:\\n\")\n",
    "    file.write(str(pd.crosstab(y_test_class, y_pred_class, colnames=['Predictions'])))\n",
    "    \n",
    "    file.write(\"\\n\\nClassification Report on test set:\\n\")\n",
    "    file.write(classification_report(y_test_class, y_pred_class, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260b586-4df4-44b9-8b34-413a46424740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save validation accuracy and validation loss over epochs from history\n",
    "plot_training_history(history, save_dir=\"../CNN_output\", prefix=\"cnn6_sm_lr_bs_epoch__valloss_\") #change for model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascientest_project",
   "language": "python",
   "name": "datascientest_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
