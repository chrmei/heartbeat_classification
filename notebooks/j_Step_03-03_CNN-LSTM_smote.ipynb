{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ec3a8-c7d6-4d69-807a-f2dcc8e0e7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Conv1D, MaxPooling1D, Flatten, Add, ReLU, LSTM, Reshape\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOversampler\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80f5f1-cf8a-4b16-a726-47ec715edaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import MIT data\n",
    "df_mitbih_test = pd.read_csv('data/original/mitbih_test.csv', header = None)\n",
    "\n",
    "X_train = pd.read_csv('data/processed/X_train.csv')\n",
    "y_train = pd.read_csv('data/processed/y_train.csv')\n",
    "y_train = y_train['187']\n",
    "\n",
    "X_train_sm = pd.read_csv('data/processed/X_train_sm.csv')\n",
    "y_train_sm = pd.read_csv('data/processed/y_train_sm.csv')\n",
    "y_train_sm = y_train_sm['187']\n",
    "\n",
    "X_val = pd.read_csv('data/processed/X_val.csv')\n",
    "y_val = pd.read_csv('data/processed/y_val.csv')\n",
    "y_val = y_val['187']\n",
    "\n",
    "X_test = df_mitbih_test.drop(187, axis = 1)\n",
    "y_test = df_mitbih_test[187]\n",
    "\n",
    "\n",
    "# Reshape the data for 1D CNN\n",
    "X_train_sm_cnn = np.expand_dims(X_train_sm, axis=2)\n",
    "X_val_cnn = np.expand_dims(X_val, axis=2)\n",
    "X_test_cnn = np.expand_dims(X_test, axis=2) \n",
    "\n",
    "display(X_train_sm_cnn.shape)\n",
    "display(X_val_cnn.shape)\n",
    "display(X_test_cnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ddbb7-d23d-4689-9cb6-23a1a1a029e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot and save validation accuracy and validation loss over epochs from history\n",
    "def plot_training_history(history, save_dir, prefix): \n",
    "    hist = history.history\n",
    "    metrics = [m for m in hist.keys() if not m.startswith('val_')]  \n",
    "\n",
    "    # Create the output folder if it does not exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for m in metrics:\n",
    "        plt.figure()\n",
    "        plt.plot(hist[m], label=f'Train {m}')\n",
    "        if f'val_{m}' in hist:\n",
    "            plt.plot(hist[f'val_{m}'], label=f'Val {m}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(m)\n",
    "        plt.title(f'{m} over epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Construct filename with prefix and filepath with directory and filename\n",
    "        filename = f\"{prefix}_{m}.png\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save the figure\n",
    "        plt.savefig(filepath, format='png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {filepath}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82af94-c4e6-4a68-9593-78d9ccd9f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used models\n",
    "#CNN4, CNN4-3 Paper 2020, more BatchNormalization and Dropout layers added, LSTM3, 10 layers\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(187, 1))\n",
    "\n",
    "# CNN model without the final Dense layers\n",
    "cnn_output = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=5, activation='relu', input_shape=(187, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3)\n",
    "])(inputs)\n",
    "\n",
    "# Reshape CNN output for LSTM input\n",
    "reshaped_cnn_output = Reshape((-1, 256))(cnn_output)  \n",
    "\n",
    "# LSTM model without the initial Input layer and final Dense layers\n",
    "lstm_output = Sequential([\n",
    "    LSTM(units=32, return_sequences=True, input_shape=reshaped_cnn_output.shape[1:]),\n",
    "    LSTM(units=32, return_sequences=True),\n",
    "    LSTM(units=32, return_sequences=True),\n",
    "    LSTM(units=32, return_sequences=True),\n",
    "    LSTM(units=32, return_sequences=True),\n",
    "    LSTM(units=32, return_sequences=False)\n",
    "])(reshaped_cnn_output)\n",
    "\n",
    "# Fully connected layers\n",
    "x = Dense(64, activation='relu')(lstm_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Output layer for 5 classes\n",
    "outputs = Dense(5, activation='softmax')(x)\n",
    "\n",
    "# Create combined model\n",
    "cnn_lstm = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed81ae6f-e162-415b-a8c5-fba4f9ca55fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1f94c4-1cdb-479b-84e2-dd888726c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model, change model when needed\n",
    "cnn_lstm.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=Adam(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"]) \n",
    "\n",
    "\n",
    "#Define where and how to save the best model, note lr and bs\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='../CNNLSTM_output/cnnlstm_sm_lr_bs_epoch_{epoch:02d}_valloss_{val_loss:.4f}.keras',\n",
    "    monitor='val_loss',        # metric to monitor\n",
    "    mode='min',                # because higher accuracy is better\n",
    "    save_best_only=True,       # only save when val_accuracy improves\n",
    "    verbose=1                  # print message when a model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf411ad-ac2d-410c-a8e9-086ec30458d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn_lstm.fit(\n",
    "    X_train_sm_cnn,\n",
    "    y_train_sm,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_cnn, y_val),  # original, unaltered validation set\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d1163-cbb1-4bd3-ba7a-fd869016d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save training history\n",
    "with open(\"../CNNLSTM_output/cnnlstm_sm_lr_bs_epoch__valloss_.pkl\", \"wb\") as f: #change for model\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "    \n",
    "best_model = load_model('../CNNLSTM_output/cnnlstm_sm_lr_bs_epoch__valloss_.keras') #change for model\n",
    "\n",
    "\n",
    "#prediction of test data\n",
    "test_pred = best_model.predict(X_test_cnn)\n",
    "y_test_class = y_test\n",
    "y_pred_class = np.argmax(test_pred, axis=1)\n",
    "\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_test_class, y_pred_class, digits=4))\n",
    "\n",
    "\n",
    "#confusion matrix\n",
    "print(pd.crosstab(y_test_class, y_pred_class, colnames=['Predictions']))\n",
    "\n",
    "\n",
    "#save results of metrics\n",
    "with open(\"../CNNLSTM_output/cnnlstm_sm_lr_bs_epoch__valloss_.txt\", \"w\") as file: #change for model\n",
    "    \n",
    "    file.write(\"\\nModel: CNN-LSTM\\n\")#change for model\n",
    "        \n",
    "    file.write(\"\\nData augmentation: Smote\\n\")\n",
    "    \n",
    "    file.write(\"\\nConfusion Matrix on test set:\\n\")\n",
    "    file.write(str(pd.crosstab(y_test_class, y_pred_class, colnames=['Predictions'])))\n",
    "    \n",
    "    file.write(\"\\n\\nClassification Report on test set:\\n\")\n",
    "    file.write(classification_report(y_test_class, y_pred_class, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73209848-a2f9-4007-8def-3064e12675aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save validation accuracy and validation loss over epochs from history\n",
    "plot_training_history(history, save_dir=\"../CNNLSTM_output\", prefix=\"cnnlstm_sm_lr_bs_epoch__valloss_\") #change for model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascientest_project",
   "language": "python",
   "name": "datascientest_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
