{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Testing\n",
        "\n",
        "Questions to be answered:\n",
        "\n",
        "- Remove outliers?\n",
        "- Which Sampling method to use?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "scoring = {'f1_macro': 'f1_macro', 'bal_acc': 'balanced_accuracy', 'f1_weighted': 'f1_weighted'}\n",
        "random_state = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os \n",
        "sys.path.append('..')\n",
        "\n",
        "from src.utils.preprocessing import (\n",
        "    prepare_mitbih, \n",
        "    prepare_ptbdb,\n",
        "    resample_training\n",
        ")\n",
        "from src.utils.evaluation import eval_model\n",
        "from src.visualization import plot_confusion_matrix\n",
        "from src.utils.model_saver import create_model_saver\n",
        "\n",
        "# external \n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
        "from scipy.stats import loguniform, randint, uniform\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# Samplers\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Init model saver\n",
        "model_saver = create_model_saver(\"../src/models/exploration_phase\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MITBIH dataset prepared:\n",
            "  Training size: (87554, 187)\n",
            "  Test size: (21892, 187)\n",
            "Note: No validation set - using train/test split only. Cross-validation handles train/val splitting.\n"
          ]
        }
      ],
      "source": [
        "# Prepare datasets\n",
        "mitbih = prepare_mitbih(remove_outliers=False)\n",
        "\n",
        "print(\"MITBIH dataset prepared:\")\n",
        "print(f\"  Training size: {mitbih.X_train.shape}\")\n",
        "print(f\"  Test size: {mitbih.X_test.shape if mitbih.X_test is not None else 'None'}\")\n",
        "print(\"Note: No validation set - using train/test split only. Cross-validation handles train/val splitting.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test = mitbih.X_train.values, mitbih.X_test.values\n",
        "y_train = mitbih.y_train.astype(int).values\n",
        "y_test = mitbih.y_test.astype(int).values\n",
        "\n",
        "# Scale features using train fit only\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Param Spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_spaces = {\n",
        "    \"LogisticRegression\": {\n",
        "        \"estimator\": LogisticRegression(max_iter=10000, multi_class='multinomial', solver='lbfgs', n_jobs=-1),\n",
        "        \"params\": {\n",
        "            \"C\": loguniform(1e-3, 1e3),      # Big C = less penalty on large weights (more freedom, risk of overfitting). \n",
        "                                             # Small C = more penalty (more discipline, less overfitting).\n",
        "                                             # loguniform = means we try values spread across tiny to big scales (e.g., 0.001 up to 100), not just small steps.\n",
        "            \"penalty\": [\"l2\"], # gently pushes weights toward zero, which keeps the model simpler and more stable.\n",
        "            \"solver\": [\"lbfgs\"],\n",
        "        },\n",
        "        \"cv\": StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 5,\n",
        "        \"create_new_model\": False,\n",
        "    },\n",
        "    \"KNN\": {\n",
        "        \"estimator\": KNeighborsClassifier(n_jobs=-1),\n",
        "        \"params\": {\n",
        "            \"n_neighbors\": randint(1, 51),\n",
        "            \"weights\": [\"uniform\", \"distance\"],\n",
        "            \"metric\": [\"minkowski\", \"manhattan\", \"euclidean\"],\n",
        "            \"p\": [1, 2],\n",
        "        },\n",
        "        \"cv\": StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 5,\n",
        "    },\n",
        "    \"RandomForest\": {\n",
        "        \"estimator\": RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
        "        \"params\": {\n",
        "            \"n_estimators\": [100, 200, 300],\n",
        "            \"max_depth\": [10, 15, 20],\n",
        "            \"min_samples_split\": [2, 5, 10, 20, 50],\n",
        "            \"min_samples_leaf\": [1, 2, 4, 8],\n",
        "            \"max_features\": [\"sqrt\", \"log2\", None],\n",
        "            \"bootstrap\": [True],\n",
        "            \"class_weight\": [\"balanced\", None],\n",
        "            \"criterion\": [\"gini\", \"entropy\"],\n",
        "        },\n",
        "        \"cv\": StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 5,\n",
        "    },\n",
        "    \"SVM\": {\n",
        "        \"estimator\": SVC(),\n",
        "        \"params\": {\n",
        "            \"kernel\": [\"rbf\", \"poly\"],\n",
        "            \"C\": [0.1, 1, 10],\n",
        "            \"gamma\": [0.001, 0.01, 0.1, 0.5, 0.9],\n",
        "        },\n",
        "        \"cv\": StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 5,\n",
        "    },\n",
        "    \"DecisionTree\": {\n",
        "        \"estimator\": DecisionTreeClassifier(random_state=random_state),\n",
        "        \"params\": {\n",
        "            \"max_depth\": [None, 5, 10, 15, 20, 25, 30],\n",
        "            \"min_samples_split\": [2, 5, 10, 20, 50],\n",
        "            \"min_samples_leaf\": [1, 2, 4, 8, 16],\n",
        "            \"max_features\": [\"sqrt\", \"log2\", None],\n",
        "            \"criterion\": [\"gini\", \"entropy\"],\n",
        "            \"class_weight\": [\"balanced\", None],\n",
        "            \"splitter\": [\"best\", \"random\"],\n",
        "        },\n",
        "        \"cv\": StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 100,\n",
        "    },\n",
        "    \"XGBoost\": {\n",
        "        \"estimator\": xgb.XGBClassifier(\n",
        "            objective=\"multi:softmax\",\n",
        "            num_class=5,\n",
        "            random_state=random_state,\n",
        "            n_jobs=-1,\n",
        "            eval_metric=\"mlogloss\",\n",
        "        ),\n",
        "        \"params\": {\n",
        "            \"n_estimators\": [100, 200, 300, 500],\n",
        "            \"max_depth\": [3, 4, 5, 6, 7, 8],\n",
        "            \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "            \"subsample\": [0.8, 0.9, 1.0],\n",
        "            \"colsample_bytree\": [0.8, 0.9, 1.0],\n",
        "            \"reg_alpha\": [0, 0.1, 0.5, 1.0],\n",
        "            \"reg_lambda\": [0, 0.1, 0.5, 1.0],\n",
        "            \"min_child_weight\": [1, 3, 5, 7],\n",
        "            \"gamma\": [0, 0.1, 0.2, 0.3],\n",
        "        },\n",
        "        \"cv\": StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 40,\n",
        "    },\n",
        "    \"LDA\": {\n",
        "        \"estimator\": LinearDiscriminantAnalysis(),\n",
        "        \"params\": [\n",
        "            {\"solver\": [\"svd\"], \"store_covariance\": [False, True], \"tol\": [1e-4, 1e-3, 1e-2]},\n",
        "            {\"solver\": [\"lsqr\", \"eigen\"], \"shrinkage\": [None, \"auto\", 0.0, 0.05, 0.1, 0.15, 0.25, 0.35, 0.5, 0.65, 0.75, 0.85, 0.9], \"tol\": [1e-4, 1e-3, 1e-2]},\n",
        "        ],\n",
        "        \"cv\": StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 50,\n",
        "    },\n",
        "    \"ANN\": {\n",
        "        \"estimator\": MLPClassifier(\n",
        "            max_iter=300,\n",
        "            early_stopping=True,\n",
        "            random_state=random_state,\n",
        "            n_iter_no_change=10,\n",
        "            solver=\"adam\",\n",
        "        ),\n",
        "        \"params\": {\n",
        "            \"hidden_layer_sizes\": [(64,), (128,), (128, 64)],\n",
        "            \"activation\": [\"relu\"],\n",
        "            \"alpha\": loguniform(1e-4, 1e-2),\n",
        "            \"learning_rate_init\": loguniform(1e-3, 1e-2),\n",
        "            \"batch_size\": randint(64, 129),\n",
        "            \"beta_1\": uniform(0.9, 0.09),\n",
        "            \"beta_2\": uniform(0.95, 0.049),\n",
        "            \"validation_fraction\": [0.1, 0.15],\n",
        "        },\n",
        "        \"cv\": StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 100,\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test models with Randomized Search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Without outlier removal or sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/LogisticRegression_no_sampling.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model LogisticRegression already exists for experiment no_sampling. Loading...\n",
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Check if model already exists\n",
        "classifier_name = \"LogisticRegression\"\n",
        "experiment_name = \"no_sampling\"\n",
        "create_new = param_spaces[classifier_name].get('create_new_model', False)\n",
        "\n",
        "if not create_new and model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_logreg = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    logreg = param_spaces[classifier_name]['estimator']\n",
        "\n",
        "    param_dist_logreg = param_spaces[classifier_name]['params']\n",
        "\n",
        "    rs_logreg = RandomizedSearchCV(\n",
        "        estimator=logreg,\n",
        "        param_distributions=param_dist_logreg,\n",
        "        n_iter=param_spaces[classifier_name]['n_iter'],\n",
        "        scoring=scoring,\n",
        "        refit='f1_macro',\n",
        "        cv=param_spaces[classifier_name]['cv'],\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=2\n",
        "    )\n",
        "    rs_logreg.fit(X_train_s, y_train)\n",
        "    \n",
        "    # Save the trained model\n",
        "    metadata = {\n",
        "        'best_params': rs_logreg.best_params_,\n",
        "        'best_score': rs_logreg.best_score_,\n",
        "        'cv_results': rs_logreg.cv_results_,\n",
        "        'experiment': experiment_name,\n",
        "        'classifier': classifier_name\n",
        "    }\n",
        "    model_saver.save_model(classifier_name, rs_logreg, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m best_logreg = rs_logreg.best_estimator_\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mLogisticRegression\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbest_logreg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_test_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/notebooks/../src/utils/evaluation.py:20\u001b[39m, in \u001b[36meval_model\u001b[39m\u001b[34m(model, X_tr, y_tr, X_te, y_te)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34meval_model\u001b[39m(model, X_tr, y_tr, X_te, y_te):\n\u001b[32m      8\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Evaluate model on training and test sets.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[33;03m        Dictionary with evaluation results for train and test sets\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     yt = model.predict(X_te)\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Choose a consistent label order (dynamic)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1145\u001b[39m     estimator._validate_params()\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1148\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1149\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1150\u001b[39m     )\n\u001b[32m   1151\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1303\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1301\u001b[39m     n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1303\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1306\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1328\u001b[39m fold_coefs_, _, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n\u001b[32m   1329\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, \u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     60\u001b[39m config = get_config()\n\u001b[32m     61\u001b[39m iterable_with_config = (\n\u001b[32m     62\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     64\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1861\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1863\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1865\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1866\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1867\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1868\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1869\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1870\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1790\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1792\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1794\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m     config = {}\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:452\u001b[39m, in \u001b[36m_logistic_regression_path\u001b[39m\u001b[34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[39m\n\u001b[32m    448\u001b[39m l2_reg_strength = \u001b[32m1.0\u001b[39m / C\n\u001b[32m    449\u001b[39m iprint = [-\u001b[32m1\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m101\u001b[39m][\n\u001b[32m    450\u001b[39m     np.searchsorted(np.array([\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m]), verbose)\n\u001b[32m    451\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m opt_res = \u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL-BFGS-B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43miprint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgtol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxiter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m n_iter_i = _check_optimize_result(\n\u001b[32m    461\u001b[39m     solver,\n\u001b[32m    462\u001b[39m     opt_res,\n\u001b[32m    463\u001b[39m     max_iter,\n\u001b[32m    464\u001b[39m     extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[32m    465\u001b[39m )\n\u001b[32m    466\u001b[39m w0, loss = opt_res.x, opt_res.fun\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/scipy/optimize/_minimize.py:710\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    707\u001b[39m     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[32m    708\u001b[39m                              **options)\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33ml-bfgs-b\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m     res = \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mtnc\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    713\u001b[39m     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[32m    714\u001b[39m                         **options)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/scipy/optimize/_lbfgsb_py.py:365\u001b[39m, in \u001b[36m_minimize_lbfgsb\u001b[39m\u001b[34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[39m\n\u001b[32m    359\u001b[39m task_str = task.tobytes()\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task_str.startswith(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFG\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    361\u001b[39m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[32m    362\u001b[39m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[32m    363\u001b[39m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[32m    364\u001b[39m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     f, g = \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m task_str.startswith(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33mNEW_X\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    367\u001b[39m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[32m    368\u001b[39m     n_iterations += \u001b[32m1\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:285\u001b[39m, in \u001b[36mScalarFunction.fun_and_grad\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.array_equal(x, \u001b[38;5;28mself\u001b[39m.x):\n\u001b[32m    284\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_x_impl(x)\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[38;5;28mself\u001b[39m._update_grad()\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f, \u001b[38;5;28mself\u001b[39m.g\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[39m, in \u001b[36mScalarFunction._update_fun\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f_updated:\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m         \u001b[38;5;28mself\u001b[39m.f_updated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[39m, in \u001b[36mScalarFunction.__init__.<locals>.update_fun\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_fun\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28mself\u001b[39m.f = \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[39m, in \u001b[36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m.nfev += \u001b[32m1\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m fx = \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isscalar(fx):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/scipy/optimize/_optimize.py:77\u001b[39m, in \u001b[36mMemoizeJac.__call__\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, *args):\n\u001b[32m     76\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/scipy/optimize/_optimize.py:71\u001b[39m, in \u001b[36mMemoizeJac._compute_if_needed\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(x == \u001b[38;5;28mself\u001b[39m.x) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mself\u001b[39m.x = np.asarray(x).copy()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     fg = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mself\u001b[39m.jac = fg[\u001b[32m1\u001b[39m]\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mself\u001b[39m._value = fg[\u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/sklearn/linear_model/_linear_loss.py:298\u001b[39m, in \u001b[36mLinearModelLoss.loss_gradient\u001b[39m\u001b[34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[39m\n\u001b[32m    296\u001b[39m grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit_intercept:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     grad[:, -\u001b[32m1\u001b[39m] = \u001b[43mgrad_pointwise\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m coef.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    300\u001b[39m     grad = grad.ravel(order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/numpy/core/_methods.py:47\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_amin\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     44\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_minimum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     48\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prod\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     52\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "best_logreg = rs_logreg.best_estimator_\n",
        "results['LogisticRegression'] = eval_model(\n",
        "    best_logreg,\n",
        "    X_train_s, y_train,\n",
        "    X_test_s, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=4.0428727350273315, max_iter=10000,\n",
              "                   multi_class=&#x27;multinomial&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=4.0428727350273315, max_iter=10000,\n",
              "                   multi_class=&#x27;multinomial&#x27;)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression(C=4.0428727350273315, max_iter=10000,\n",
              "                   multi_class='multinomial')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'labels': array([0, 1, 2, 3, 4]),\n",
              " 'val': {'accuracy': 0.9176564641388762,\n",
              "  'precision_macro': 0.800141874074688,\n",
              "  'recall_macro': 0.6105422563887755,\n",
              "  'f1_macro': 0.6765546028672075,\n",
              "  'precision_per_class': array([0.92888831, 0.81679389, 0.65064103, 0.65625   , 0.94813614]),\n",
              "  'recall_per_class': array([0.98220199, 0.48198198, 0.35060449, 0.328125  , 0.90979782]),\n",
              "  'f1_per_class': array([0.9548015 , 0.60623229, 0.45566779, 0.4375    , 0.92857143]),\n",
              "  'support_per_class': array([7248,  222,  579,   64,  643]),\n",
              "  'confusion_matrix': array([[7119,   18,   91,    1,   19],\n",
              "         [ 106,  107,    7,    1,    1],\n",
              "         [ 350,    5,  203,    9,   12],\n",
              "         [  38,    0,    5,   21,    0],\n",
              "         [  51,    1,    6,    0,  585]])},\n",
              " 'test': {'accuracy': 0.9151288141786954,\n",
              "  'precision_macro': 0.7885019903691001,\n",
              "  'recall_macro': 0.5967700974401475,\n",
              "  'f1_macro': 0.6633702561560696,\n",
              "  'precision_per_class': array([0.92469552, 0.82105263, 0.66344828, 0.57843137, 0.95488215]),\n",
              "  'recall_per_class': array([0.98476653, 0.42086331, 0.33218232, 0.36419753, 0.8818408 ]),\n",
              "  'f1_per_class': array([0.95378612, 0.55648038, 0.44270594, 0.4469697 , 0.91690915]),\n",
              "  'support_per_class': array([18118,   556,  1448,   162,  1608]),\n",
              "  'confusion_matrix': array([[17842,    36,   192,     9,    39],\n",
              "         [  293,   234,    26,     2,     1],\n",
              "         [  894,    14,   481,    32,    27],\n",
              "         [   95,     0,     8,    59,     0],\n",
              "         [  171,     1,    18,     0,  1418]])}}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display(best_logreg)\n",
        "results['LogisticRegression']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.2 KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.models.exploration_phase.model_saver:Model loaded: src/models/exploration_phase/KNN_no_sampling.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model KNN already exists for experiment no_sampling. Loading...\n",
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Check if model already exists\n",
        "classifier_name = \"KNN\"\n",
        "experiment_name = \"no_sampling\"\n",
        "\n",
        "if model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_knn = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    knn = KNeighborsClassifier()\n",
        "    param_dist_knn = {\n",
        "        'n_neighbors': randint(1, 51),\n",
        "        'weights': ['uniform', 'distance'],           # helps with imbalance; 'distance' often better\n",
        "        'metric': ['minkowski', 'manhattan', 'euclidean'],\n",
        "        'p': [1,2],                           # used only for minkowski, if left out it defaults to euclidean\n",
        "    }\n",
        "\n",
        "    rs_knn = RandomizedSearchCV(\n",
        "        estimator=knn,\n",
        "        param_distributions=param_dist_knn,\n",
        "        n_iter=20,\n",
        "        scoring=scoring,\n",
        "        refit='f1_macro',\n",
        "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    rs_knn.fit(X_train_s, y_train)\n",
        "    \n",
        "    # Save the trained model\n",
        "    metadata = {\n",
        "        'best_params': rs_knn.best_params_,\n",
        "        'best_score': rs_knn.best_score_,\n",
        "        'cv_results': rs_knn.cv_results_,\n",
        "        'experiment': experiment_name,\n",
        "        'classifier': classifier_name\n",
        "    }\n",
        "    model_saver.save_model(classifier_name, rs_knn, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_knn = rs_knn.best_estimator_\n",
        "results['KNN'] = eval_model(\n",
        "    best_knn,\n",
        "    X_train_s, y_train,\n",
        "    X_test_s, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(metric=&#x27;manhattan&#x27;, n_neighbors=4, p=1, weights=&#x27;distance&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(metric=&#x27;manhattan&#x27;, n_neighbors=4, p=1, weights=&#x27;distance&#x27;)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "KNeighborsClassifier(metric='manhattan', n_neighbors=4, p=1, weights='distance')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'labels': array([0, 1, 2, 3, 4]),\n",
              " 'val': {'accuracy': 0.9798994974874372,\n",
              "  'precision_macro': 0.9391433113355262,\n",
              "  'recall_macro': 0.8599185488349104,\n",
              "  'f1_macro': 0.8949289961165755,\n",
              "  'precision_per_class': array([0.98375205, 0.88826816, 0.95087719, 0.87755102, 0.99526814]),\n",
              "  'recall_per_class': array([0.99406733, 0.71621622, 0.93609672, 0.671875  , 0.98133748]),\n",
              "  'f1_per_class': array([0.98888279, 0.79301746, 0.94342907, 0.76106195, 0.98825372]),\n",
              "  'support_per_class': array([7248,  222,  579,   64,  643]),\n",
              "  'confusion_matrix': array([[7205,   18,   20,    3,    2],\n",
              "         [  61,  159,    2,    0,    0],\n",
              "         [  33,    0,  542,    3,    1],\n",
              "         [  16,    0,    5,   43,    0],\n",
              "         [   9,    2,    1,    0,  631]])},\n",
              " 'test': {'accuracy': 0.9775260369084597,\n",
              "  'precision_macro': 0.920137365015206,\n",
              "  'recall_macro': 0.8528821323680518,\n",
              "  'f1_macro': 0.8832154533279242,\n",
              "  'precision_per_class': array([0.98254894, 0.88167053, 0.94080338, 0.80141844, 0.99424552]),\n",
              "  'recall_per_class': array([0.99442543, 0.68345324, 0.92196133, 0.69753086, 0.9670398 ]),\n",
              "  'f1_per_class': array([0.98845152, 0.77001013, 0.93128706, 0.74587459, 0.98045397]),\n",
              "  'support_per_class': array([18118,   556,  1448,   162,  1608]),\n",
              "  'confusion_matrix': array([[18017,    46,    42,     8,     5],\n",
              "         [  158,   380,    17,     0,     1],\n",
              "         [   85,     5,  1335,    20,     3],\n",
              "         [   32,     0,    17,   113,     0],\n",
              "         [   45,     0,     8,     0,  1555]])}}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display(best_knn)\n",
        "results['KNN']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.3 Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.models.exploration_phase.model_saver:Model loaded: src/models/exploration_phase/RandomForest_no_sampling.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model RandomForest already exists for experiment no_sampling. Loading...\n",
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Check if model already exists\n",
        "classifier_name = \"RandomForest\"\n",
        "experiment_name = \"no_sampling\"\n",
        "\n",
        "if model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_rf = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    rf = RandomForestClassifier(random_state=random_state, n_jobs=-1)\n",
        "    param_dist_rf = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 15, 20], # prevent overfitting majority class\n",
        "        \n",
        "        'min_samples_split': [2, 5, 10, 20, 50],\n",
        "        'min_samples_leaf': [1, 2, 4, 8], # higher = better regularization\n",
        "        \n",
        "        'max_features': ['sqrt', 'log2', None],\n",
        "        'bootstrap': [True], # better generalization\n",
        "        \n",
        "        'class_weight': ['balanced', None], # for imbalanced data\n",
        "        \n",
        "        # Split criterion: entropy can help with imbalanced classes\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "    }\n",
        "\n",
        "    rs_rf = RandomizedSearchCV(\n",
        "        estimator=rf,\n",
        "        param_distributions=param_dist_rf,\n",
        "        n_iter=20,\n",
        "        scoring=scoring,\n",
        "        refit='f1_macro',\n",
        "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "    )\n",
        "\n",
        "    rs_rf.fit(X_train, y_train) # using unscaled data - RF is not sensitive to feature scaling\n",
        "    \n",
        "    # Save the trained model\n",
        "    metadata = {\n",
        "        'best_params': rs_rf.best_params_,\n",
        "        'best_score': rs_rf.best_score_,\n",
        "        'cv_results': rs_rf.cv_results_,\n",
        "        'experiment': experiment_name,\n",
        "        'classifier': classifier_name\n",
        "    }\n",
        "    model_saver.save_model(classifier_name, rs_rf, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_rf = rs_rf.best_estimator_\n",
        "results['RandomForest'] = eval_model( \n",
        "    best_rf, \n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, criterion=&#x27;entropy&#x27;,\n",
              "                       max_depth=20, min_samples_leaf=8, min_samples_split=20,\n",
              "                       n_estimators=200, n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, criterion=&#x27;entropy&#x27;,\n",
              "                       max_depth=20, min_samples_leaf=8, min_samples_split=20,\n",
              "                       n_estimators=200, n_jobs=-1, random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "RandomForestClassifier(class_weight='balanced', criterion='entropy',\n",
              "                       max_depth=20, min_samples_leaf=8, min_samples_split=20,\n",
              "                       n_estimators=200, n_jobs=-1, random_state=42)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'labels': array([0, 1, 2, 3, 4]),\n",
              " 'val': {'accuracy': 0.9785290086797624,\n",
              "  'precision_macro': 0.9160207060087867,\n",
              "  'recall_macro': 0.8648953216324944,\n",
              "  'f1_macro': 0.8885240457117929,\n",
              "  'precision_per_class': array([0.98467013, 0.88172043, 0.94210526, 0.78571429, 0.98589342]),\n",
              "  'recall_per_class': array([0.99254967, 0.73873874, 0.92746114, 0.6875    , 0.97822706]),\n",
              "  'f1_per_class': array([0.9885942 , 0.80392157, 0.93472585, 0.73333333, 0.98204528]),\n",
              "  'support_per_class': array([7248,  222,  579,   64,  643]),\n",
              "  'confusion_matrix': array([[7194,   20,   20,    8,    6],\n",
              "         [  54,  164,    3,    0,    1],\n",
              "         [  34,    2,  537,    4,    2],\n",
              "         [  14,    0,    6,   44,    0],\n",
              "         [  10,    0,    4,    0,  629]])},\n",
              " 'test': {'accuracy': 0.9748309884889458,\n",
              "  'precision_macro': 0.901553466600747,\n",
              "  'recall_macro': 0.8709987482601733,\n",
              "  'f1_macro': 0.8848821016625263,\n",
              "  'precision_per_class': array([0.98157362, 0.8516129 , 0.94247159, 0.74556213, 0.98654709]),\n",
              "  'recall_per_class': array([0.99083784, 0.71223022, 0.91643646, 0.77777778, 0.95771144]),\n",
              "  'f1_per_class': array([0.98618398, 0.77571009, 0.92927171, 0.76132931, 0.97191543]),\n",
              "  'support_per_class': array([18118,   556,  1448,   162,  1608]),\n",
              "  'confusion_matrix': array([[17952,    66,    64,    21,    15],\n",
              "         [  158,   396,     1,     0,     1],\n",
              "         [   92,     2,  1327,    22,     5],\n",
              "         [   27,     0,     9,   126,     0],\n",
              "         [   60,     1,     7,     0,  1540]])}}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display(best_rf)\n",
        "results['RandomForest']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.4 SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.models.exploration_phase.model_saver:Model loaded: src/models/exploration_phase/SVM_no_sampling.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model SVM already exists for experiment no_sampling. Loading...\n",
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Check if model already exists\n",
        "classifier_name = \"SVM\"\n",
        "experiment_name = \"no_sampling\"\n",
        "\n",
        "if model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_svm = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    svm = SVC()\n",
        "    param_dist_svm = {\n",
        "        'kernel': ['rbf', 'poly'],\n",
        "        'C': [0.1, 1, 10],\n",
        "        'gamma': [0.001, 0.01, 0.1, 0.5, 1],\n",
        "    }\n",
        "    rs_svm = RandomizedSearchCV(\n",
        "        estimator=svm,\n",
        "        param_distributions=param_dist_svm,\n",
        "        n_iter=15,\n",
        "        scoring=scoring,\n",
        "        refit='f1_macro',\n",
        "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state),\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "    )\n",
        "    rs_svm.fit(X_train_s, y_train)\n",
        "    \n",
        "    # Save the trained model\n",
        "    metadata = {\n",
        "        'best_params': rs_svm.best_params_,\n",
        "        'best_score': rs_svm.best_score_,\n",
        "        'cv_results': rs_svm.cv_results_,\n",
        "        'experiment': experiment_name,\n",
        "        'classifier': classifier_name\n",
        "    }\n",
        "    model_saver.save_model(classifier_name, rs_svm, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_svm = rs_svm.best_estimator_\n",
        "results['SVM'] = eval_model(\n",
        "    best_svm,\n",
        "    X_train_s, y_train,\n",
        "    X_test_s, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, gamma=0.01, kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, gamma=0.01, kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SVC(C=10, gamma=0.01, kernel='poly')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'labels': array([0, 1, 2, 3, 4]),\n",
              " 'val': {'accuracy': 0.9745317496573778,\n",
              "  'precision_macro': 0.9163739251346034,\n",
              "  'recall_macro': 0.8540245675024011,\n",
              "  'f1_macro': 0.8822902628187215,\n",
              "  'precision_per_class': array([0.98194995, 0.84615385, 0.91872792, 0.8490566 , 0.98598131]),\n",
              "  'recall_per_class': array([0.99075607, 0.69369369, 0.89810017, 0.703125  , 0.9844479 ]),\n",
              "  'f1_per_class': array([0.98633336, 0.76237624, 0.90829694, 0.76923077, 0.98521401]),\n",
              "  'support_per_class': array([7248,  222,  579,   64,  643]),\n",
              "  'confusion_matrix': array([[7181,   23,   35,    3,    6],\n",
              "         [  65,  154,    3,    0,    0],\n",
              "         [  46,    5,  520,    5,    3],\n",
              "         [  12,    0,    7,   45,    0],\n",
              "         [   9,    0,    1,    0,  633]])},\n",
              " 'test': {'accuracy': 0.9740544491138315,\n",
              "  'precision_macro': 0.9049136882358036,\n",
              "  'recall_macro': 0.8394617444559591,\n",
              "  'f1_macro': 0.8684037795992783,\n",
              "  'precision_per_class': array([0.98054602, 0.87347932, 0.92642857, 0.75675676, 0.98735777]),\n",
              "  'recall_per_class': array([0.99315598, 0.64568345, 0.89571823, 0.69135802, 0.97139303]),\n",
              "  'f1_per_class': array([0.98681072, 0.74250259, 0.91081461, 0.72258065, 0.97931034]),\n",
              "  'support_per_class': array([18118,   556,  1448,   162,  1608]),\n",
              "  'confusion_matrix': array([[17994,    32,    67,    12,    13],\n",
              "         [  182,   359,    14,     1,     0],\n",
              "         [  102,    19,  1297,    23,     7],\n",
              "         [   34,     0,    16,   112,     0],\n",
              "         [   39,     1,     6,     0,  1562]])}}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display(best_svm)\n",
        "results['SVM']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.5 Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.models.exploration_phase.model_saver:Model loaded: src/models/exploration_phase/DecisionTree_no_sampling.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model DecisionTree already exists for experiment no_sampling. Loading...\n",
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Check if model already exists\n",
        "classifier_name = \"DecisionTree\"\n",
        "experiment_name = \"no_sampling\"\n",
        "\n",
        "if model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_dt = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    dt = DecisionTreeClassifier(random_state=random_state)\n",
        "\n",
        "    param_dist_dt = {\n",
        "        'max_depth': [None, 5, 10, 15, 20, 25, 30],\n",
        "        'min_samples_split': [2, 5, 10, 20, 50],\n",
        "        'min_samples_leaf': [1, 2, 4, 8, 16],\n",
        "        'max_features': ['sqrt', 'log2', None],\n",
        "        'criterion': ['gini', 'entropy'],  \n",
        "        'class_weight': ['balanced', None],\n",
        "        'splitter': ['best', 'random'],  # Split strategy\n",
        "    }\n",
        "\n",
        "    rs_dt = RandomizedSearchCV(\n",
        "        estimator=dt,\n",
        "        param_distributions=param_dist_dt,\n",
        "        n_iter=100,  \n",
        "        scoring=scoring,\n",
        "        refit='f1_macro',\n",
        "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "    )\n",
        "\n",
        "    rs_dt.fit(X_train, y_train)  # Using unscaled data - DT doesn't need scaling\n",
        "    \n",
        "    # Save the trained model\n",
        "    metadata = {\n",
        "        'best_params': rs_dt.best_params_,\n",
        "        'best_score': rs_dt.best_score_,\n",
        "        'cv_results': rs_dt.cv_results_,\n",
        "        'experiment': experiment_name,\n",
        "        'classifier': classifier_name\n",
        "    }\n",
        "    model_saver.save_model(classifier_name, rs_dt, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_dt = rs_dt.best_estimator_\n",
        "results['DecisionTree'] = eval_model(\n",
        "    best_dt,\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=25, min_samples_leaf=2,\n",
              "                       min_samples_split=20, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=25, min_samples_leaf=2,\n",
              "                       min_samples_split=20, random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "DecisionTreeClassifier(criterion='entropy', max_depth=25, min_samples_leaf=2,\n",
              "                       min_samples_split=20, random_state=42)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'labels': array([0, 1, 2, 3, 4]),\n",
              " 'val': {'accuracy': 0.9611694837825491,\n",
              "  'precision_macro': 0.8286714991952492,\n",
              "  'recall_macro': 0.8113420712229086,\n",
              "  'f1_macro': 0.8192777361500603,\n",
              "  'precision_per_class': array([0.97892852, 0.7357513 , 0.86677909, 0.61290323, 0.94899536]),\n",
              "  'recall_per_class': array([0.98068433, 0.63963964, 0.88773748, 0.59375   , 0.95489891]),\n",
              "  'f1_per_class': array([0.97980564, 0.68433735, 0.87713311, 0.6031746 , 0.95193798]),\n",
              "  'support_per_class': array([7248,  222,  579,   64,  643]),\n",
              "  'confusion_matrix': array([[7108,   41,   58,   15,   26],\n",
              "         [  71,  142,    7,    0,    2],\n",
              "         [  47,    5,  514,    9,    4],\n",
              "         [  16,    1,    8,   38,    1],\n",
              "         [  19,    4,    6,    0,  614]])},\n",
              " 'test': {'accuracy': 0.9594372373469761,\n",
              "  'precision_macro': 0.8282860612296581,\n",
              "  'recall_macro': 0.7983514845333289,\n",
              "  'f1_macro': 0.8124014481179902,\n",
              "  'precision_per_class': array([0.97427195, 0.72995781, 0.8837535 , 0.59119497, 0.96225208]),\n",
              "  'recall_per_class': array([0.98233801, 0.62230216, 0.87154696, 0.58024691, 0.93532338]),\n",
              "  'f1_per_class': array([0.97828835, 0.67184466, 0.87760779, 0.58566978, 0.94859666]),\n",
              "  'support_per_class': array([18118,   556,  1448,   162,  1608]),\n",
              "  'confusion_matrix': array([[17798,   114,   116,    45,    45],\n",
              "         [  192,   346,    12,     0,     6],\n",
              "         [  148,    11,  1262,    19,     8],\n",
              "         [   49,     0,    19,    94,     0],\n",
              "         [   81,     3,    19,     1,  1504]])}}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display(best_dt)\n",
        "results['DecisionTree']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.6 XGBoost / Gradien Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.models.exploration_phase.model_saver:Model loaded: src/models/exploration_phase/XGBoost_no_sampling.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model XGBoost already exists for experiment no_sampling. Loading...\n",
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Check if model already exists\n",
        "classifier_name = \"XGBoost\"\n",
        "experiment_name = \"no_sampling\"\n",
        "\n",
        "if model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_xgb = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        objective='multi:softmax',\n",
        "        num_class=5,  # no of classes\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        eval_metric='mlogloss'\n",
        "    )\n",
        "\n",
        "    param_dist_xgb = {\n",
        "        'n_estimators': [100, 200, 300, 500],\n",
        "        'max_depth': [3, 4, 5, 6, 7, 8],\n",
        "        'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "        'subsample': [0.8, 0.9, 1.0],\n",
        "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "        'reg_alpha': [0, 0.1, 0.5, 1.0],  # L1 regularization\n",
        "        'reg_lambda': [0, 0.1, 0.5, 1.0],  # L2 regularization\n",
        "        'min_child_weight': [1, 3, 5, 7],\n",
        "        'gamma': [0, 0.1, 0.2, 0.3],  # Minimum loss reduction\n",
        "    }\n",
        "\n",
        "    rs_xgb = RandomizedSearchCV(\n",
        "        estimator=xgb_model,\n",
        "        param_distributions=param_dist_xgb,\n",
        "        n_iter=30,  \n",
        "        scoring=scoring,\n",
        "        refit='f1_macro',\n",
        "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "    )\n",
        "\n",
        "    rs_xgb.fit(X_train, y_train)  # XGBoost handles scaling internally\n",
        "    \n",
        "    # Save the trained model\n",
        "    metadata = {\n",
        "        'best_params': rs_xgb.best_params_,\n",
        "        'best_score': rs_xgb.best_score_,\n",
        "        'cv_results': rs_xgb.cv_results_,\n",
        "        'experiment': experiment_name,\n",
        "        'classifier': classifier_name\n",
        "    }\n",
        "    model_saver.save_model(classifier_name, rs_xgb, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_xgb = rs_xgb.best_estimator_\n",
        "results['XGBoost'] = eval_model(\n",
        "    best_xgb,\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=0.9, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
              "              feature_types=None, gamma=0, grow_policy=None,\n",
              "              importance_type=None, interaction_constraints=None,\n",
              "              learning_rate=0.2, max_bin=None, max_cat_threshold=None,\n",
              "              max_cat_to_onehot=None, max_delta_step=None, max_depth=8,\n",
              "              max_leaves=None, min_child_weight=5, missing=nan,\n",
              "              monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
              "              n_jobs=-1, num_class=5, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=0.9, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
              "              feature_types=None, gamma=0, grow_policy=None,\n",
              "              importance_type=None, interaction_constraints=None,\n",
              "              learning_rate=0.2, max_bin=None, max_cat_threshold=None,\n",
              "              max_cat_to_onehot=None, max_delta_step=None, max_depth=8,\n",
              "              max_leaves=None, min_child_weight=5, missing=nan,\n",
              "              monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
              "              n_jobs=-1, num_class=5, num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=0.9, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric='mlogloss',\n",
              "              feature_types=None, gamma=0, grow_policy=None,\n",
              "              importance_type=None, interaction_constraints=None,\n",
              "              learning_rate=0.2, max_bin=None, max_cat_threshold=None,\n",
              "              max_cat_to_onehot=None, max_delta_step=None, max_depth=8,\n",
              "              max_leaves=None, min_child_weight=5, missing=nan,\n",
              "              monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
              "              n_jobs=-1, num_class=5, num_parallel_tree=None, ...)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'labels': array([0, 1, 2, 3, 4]),\n",
              " 'val': {'accuracy': 0.9816126084970306,\n",
              "  'precision_macro': 0.9503427741288547,\n",
              "  'recall_macro': 0.8653017509227453,\n",
              "  'f1_macro': 0.9026234500019875,\n",
              "  'precision_per_class': array([0.98499318, 0.92397661, 0.94903339, 0.9       , 0.99371069]),\n",
              "  'recall_per_class': array([0.99613687, 0.71171171, 0.93264249, 0.703125  , 0.98289269]),\n",
              "  'f1_per_class': array([0.99053368, 0.80407125, 0.94076655, 0.78947368, 0.98827209]),\n",
              "  'support_per_class': array([7248,  222,  579,   64,  643]),\n",
              "  'confusion_matrix': array([[7220,   11,   13,    2,    2],\n",
              "         [  62,  158,    1,    0,    1],\n",
              "         [  34,    1,  540,    3,    1],\n",
              "         [   7,    0,   12,   45,    0],\n",
              "         [   7,    1,    3,    0,  632]])},\n",
              " 'test': {'accuracy': 0.9818198428649735,\n",
              "  'precision_macro': 0.9545382548756202,\n",
              "  'recall_macro': 0.8654063021130722,\n",
              "  'f1_macro': 0.9046987238778078,\n",
              "  'precision_per_class': array([0.98292922, 0.95588235, 0.97383721, 0.86764706, 0.99239544]),\n",
              "  'recall_per_class': array([0.99790264, 0.70143885, 0.92541436, 0.72839506, 0.9738806 ]),\n",
              "  'f1_per_class': array([0.99035933, 0.80912863, 0.9490085 , 0.79194631, 0.98305085]),\n",
              "  'support_per_class': array([18118,   556,  1448,   162,  1608]),\n",
              "  'confusion_matrix': array([[18080,    17,    13,     2,     6],\n",
              "         [  160,   390,     5,     0,     1],\n",
              "         [   86,     1,  1340,    16,     5],\n",
              "         [   31,     0,    13,   118,     0],\n",
              "         [   37,     0,     5,     0,  1566]])}}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display(best_xgb)\n",
        "results['XGBoost']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.7 Linear Discriminant Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model LDA not found. Training new model...\n",
            "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
            "[CV] END .............shrinkage=0.85, solver=lsqr, tol=0.001; total time=   1.7s\n",
            "[CV] END .............shrinkage=0.0, solver=lsqr, tol=0.0001; total time=   1.8s\n",
            "[CV] END .............shrinkage=0.85, solver=lsqr, tol=0.001; total time=   2.1s\n",
            "[CV] END .............shrinkage=0.0, solver=lsqr, tol=0.0001; total time=   2.2s\n",
            "[CV] END ............shrinkage=0.75, solver=eigen, tol=0.001; total time=   2.3s\n",
            "[CV] END .............shrinkage=0.0, solver=eigen, tol=0.001; total time=   2.4s\n",
            "[CV] END .............shrinkage=0.0, solver=lsqr, tol=0.0001; total time=   2.2s\n",
            "[CV] END .............shrinkage=0.5, solver=eigen, tol=0.001; total time=   2.4s\n",
            "[CV] END .............shrinkage=0.85, solver=lsqr, tol=0.001; total time=   2.5s\n",
            "[CV] END ............shrinkage=0.75, solver=eigen, tol=0.001; total time=   2.5s\n",
            "[CV] END ............shrinkage=None, solver=eigen, tol=0.001; total time=   2.7s\n",
            "[CV] END ............shrinkage=None, solver=eigen, tol=0.001; total time=   2.7s\n",
            "[CV] END .............shrinkage=0.0, solver=eigen, tol=0.001; total time=   2.8s\n",
            "[CV] END ............shrinkage=None, solver=eigen, tol=0.001; total time=   2.8s\n",
            "[CV] END ............shrinkage=0.75, solver=eigen, tol=0.001; total time=   2.9s\n",
            "[CV] END .............shrinkage=0.0, solver=eigen, tol=0.001; total time=   3.0s\n",
            "[CV] END .............shrinkage=0.5, solver=eigen, tol=0.001; total time=   3.1s\n",
            "[CV] END .............shrinkage=0.5, solver=eigen, tol=0.001; total time=   3.2s\n",
            "[CV] END ............shrinkage=auto, solver=lsqr, tol=0.0001; total time=   3.3s\n",
            "[CV] END ............shrinkage=auto, solver=lsqr, tol=0.0001; total time=   3.3s\n",
            "[CV] END ............shrinkage=auto, solver=lsqr, tol=0.0001; total time=   3.7s\n",
            "[CV] END .............shrinkage=0.65, solver=eigen, tol=0.01; total time=   1.9s\n",
            "[CV] END .............shrinkage=0.65, solver=eigen, tol=0.01; total time=   2.0s\n",
            "[CV] END ............shrinkage=0.15, solver=eigen, tol=0.001; total time=   2.4s\n",
            "[CV] END .............shrinkage=0.25, solver=eigen, tol=0.01; total time=   2.4s\n",
            "[CV] END ..............shrinkage=0.1, solver=eigen, tol=0.01; total time=   2.3s\n",
            "[CV] END .............shrinkage=0.25, solver=eigen, tol=0.01; total time=   2.9s\n",
            "[CV] END ............shrinkage=0.15, solver=eigen, tol=0.001; total time=   3.2s\n",
            "[CV] END .............shrinkage=0.65, solver=eigen, tol=0.01; total time=   3.5s\n",
            "[CV] END ............shrinkage=0.15, solver=eigen, tol=0.001; total time=   3.4s\n",
            "[CV] END ............shrinkage=0.9, solver=eigen, tol=0.0001; total time=   2.9s\n",
            "[CV] END ............shrinkage=0.9, solver=eigen, tol=0.0001; total time=   2.9s\n",
            "[CV] END ..............shrinkage=0.1, solver=eigen, tol=0.01; total time=   3.4s\n",
            "[CV] END .............shrinkage=0.25, solver=eigen, tol=0.01; total time=   3.5s\n",
            "[CV] END ..............shrinkage=0.5, solver=lsqr, tol=0.001; total time=   3.0s\n",
            "[CV] END ..............shrinkage=0.5, solver=lsqr, tol=0.001; total time=   3.1s\n",
            "[CV] END .....solver=svd, store_covariance=False, tol=0.0001; total time=   6.5s\n",
            "[CV] END ..............shrinkage=0.5, solver=lsqr, tol=0.001; total time=   2.7s\n",
            "[CV] END ..............shrinkage=0.1, solver=eigen, tol=0.01; total time=   3.6s\n",
            "[CV] END .............shrinkage=0.5, solver=lsqr, tol=0.0001; total time=   2.2s\n",
            "[CV] END .............shrinkage=0.5, solver=lsqr, tol=0.0001; total time=   2.4s\n",
            "[CV] END ............shrinkage=0.9, solver=eigen, tol=0.0001; total time=   3.7s\n",
            "[CV] END ............shrinkage=0.1, solver=eigen, tol=0.0001; total time=   2.4s\n",
            "[CV] END .....solver=svd, store_covariance=False, tol=0.0001; total time=   8.1s\n",
            "[CV] END ............shrinkage=0.1, solver=eigen, tol=0.0001; total time=   3.1s\n",
            "[CV] END ............shrinkage=0.1, solver=eigen, tol=0.0001; total time=   3.1s\n",
            "[CV] END .............shrinkage=0.5, solver=lsqr, tol=0.0001; total time=   3.5s\n",
            "[CV] END ............shrinkage=0.85, solver=eigen, tol=0.001; total time=   2.7s\n",
            "[CV] END .............shrinkage=0.1, solver=lsqr, tol=0.0001; total time=   2.2s\n",
            "[CV] END .............shrinkage=0.1, solver=lsqr, tol=0.0001; total time=   2.3s\n",
            "[CV] END ............shrinkage=0.25, solver=lsqr, tol=0.0001; total time=   3.1s\n",
            "[CV] END .............shrinkage=0.35, solver=lsqr, tol=0.001; total time=   2.8s\n",
            "[CV] END .............shrinkage=0.35, solver=lsqr, tol=0.001; total time=   2.7s\n",
            "[CV] END ............shrinkage=0.25, solver=lsqr, tol=0.0001; total time=   3.1s\n",
            "[CV] END .............shrinkage=0.35, solver=lsqr, tol=0.001; total time=   2.8s\n",
            "[CV] END ............shrinkage=0.25, solver=lsqr, tol=0.0001; total time=   3.0s\n",
            "[CV] END .............shrinkage=None, solver=eigen, tol=0.01; total time=   2.7s\n",
            "[CV] END ............shrinkage=0.85, solver=eigen, tol=0.001; total time=   3.6s\n",
            "[CV] END ............shrinkage=0.85, solver=eigen, tol=0.001; total time=   3.6s\n",
            "[CV] END .......solver=svd, store_covariance=True, tol=0.001; total time=   7.8s\n",
            "[CV] END .............shrinkage=None, solver=eigen, tol=0.01; total time=   3.0s\n",
            "[CV] END .............shrinkage=0.1, solver=lsqr, tol=0.0001; total time=   2.7s\n",
            "[CV] END .....solver=svd, store_covariance=False, tol=0.0001; total time=   9.6s\n",
            "[CV] END .............shrinkage=None, solver=eigen, tol=0.01; total time=   3.1s\n",
            "[CV] END ...........shrinkage=0.15, solver=eigen, tol=0.0001; total time=   2.3s\n",
            "[CV] END ...........shrinkage=0.15, solver=eigen, tol=0.0001; total time=   2.2s\n",
            "[CV] END ............shrinkage=0.05, solver=eigen, tol=0.001; total time=   2.0s\n",
            "[CV] END .......solver=svd, store_covariance=True, tol=0.001; total time=   8.7s\n",
            "[CV] END ..............shrinkage=0.75, solver=lsqr, tol=0.01; total time=   1.6s\n",
            "[CV] END ..............shrinkage=0.75, solver=lsqr, tol=0.01; total time=   1.6s\n",
            "[CV] END ..............shrinkage=0.75, solver=lsqr, tol=0.01; total time=   1.8s\n",
            "[CV] END .......solver=svd, store_covariance=True, tol=0.001; total time=   8.9s\n",
            "[CV] END ............shrinkage=0.05, solver=eigen, tol=0.001; total time=   2.6s\n",
            "[CV] END ............shrinkage=0.05, solver=eigen, tol=0.001; total time=   2.6s\n",
            "[CV] END ...........shrinkage=None, solver=eigen, tol=0.0001; total time=   2.3s\n",
            "[CV] END ...........shrinkage=0.15, solver=eigen, tol=0.0001; total time=   2.9s\n",
            "[CV] END ..............shrinkage=0.1, solver=lsqr, tol=0.001; total time=   1.8s\n",
            "[CV] END ..............shrinkage=0.1, solver=lsqr, tol=0.001; total time=   1.8s\n",
            "[CV] END ..............shrinkage=0.1, solver=lsqr, tol=0.001; total time=   1.8s\n",
            "[CV] END ...........shrinkage=0.25, solver=eigen, tol=0.0001; total time=   2.0s\n",
            "[CV] END ...........shrinkage=0.25, solver=eigen, tol=0.0001; total time=   1.9s\n",
            "[CV] END ...........shrinkage=None, solver=eigen, tol=0.0001; total time=   2.4s\n",
            "[CV] END .............shrinkage=0.1, solver=eigen, tol=0.001; total time=   2.1s\n",
            "[CV] END ...........shrinkage=None, solver=eigen, tol=0.0001; total time=   2.8s\n",
            "[CV] END ...........shrinkage=0.25, solver=eigen, tol=0.0001; total time=   2.6s\n",
            "[CV] END .............shrinkage=0.1, solver=eigen, tol=0.001; total time=   1.8s\n",
            "[CV] END .............shrinkage=0.1, solver=eigen, tol=0.001; total time=   1.7s\n",
            "[CV] END ..............shrinkage=0.25, solver=lsqr, tol=0.01; total time=   1.7s\n",
            "[CV] END ..............shrinkage=0.25, solver=lsqr, tol=0.01; total time=   1.9s\n",
            "[CV] END ..............shrinkage=0.25, solver=lsqr, tol=0.01; total time=   2.1s\n",
            "[CV] END .............shrinkage=None, solver=lsqr, tol=0.001; total time=   1.9s\n",
            "[CV] END .............shrinkage=None, solver=lsqr, tol=0.001; total time=   2.0s\n",
            "[CV] END ..............shrinkage=0.9, solver=eigen, tol=0.01; total time=   2.5s\n",
            "[CV] END ..............shrinkage=0.9, solver=eigen, tol=0.01; total time=   2.7s\n",
            "[CV] END ..............shrinkage=0.9, solver=eigen, tol=0.01; total time=   2.7s\n",
            "[CV] END ............shrinkage=0.65, solver=eigen, tol=0.001; total time=   2.4s\n",
            "[CV] END .............shrinkage=None, solver=lsqr, tol=0.001; total time=   2.3s\n",
            "[CV] END .............shrinkage=0.75, solver=lsqr, tol=0.001; total time=   2.4s\n",
            "[CV] END .............shrinkage=0.75, solver=lsqr, tol=0.001; total time=   2.3s\n",
            "[CV] END ............shrinkage=0.65, solver=eigen, tol=0.001; total time=   2.8s\n",
            "[CV] END ...............shrinkage=0.5, solver=lsqr, tol=0.01; total time=   2.3s\n",
            "[CV] END ...............shrinkage=0.5, solver=lsqr, tol=0.01; total time=   2.3s\n",
            "[CV] END ..............shrinkage=0.0, solver=lsqr, tol=0.001; total time=   2.0s\n",
            "[CV] END ............shrinkage=0.65, solver=eigen, tol=0.001; total time=   3.3s\n",
            "[CV] END ...............shrinkage=0.5, solver=lsqr, tol=0.01; total time=   2.4s\n",
            "[CV] END ..............shrinkage=0.0, solver=lsqr, tol=0.001; total time=   2.1s\n",
            "[CV] END .............shrinkage=0.75, solver=lsqr, tol=0.001; total time=   2.8s\n",
            "[CV] END ........solver=svd, store_covariance=True, tol=0.01; total time=   5.5s\n",
            "[CV] END ..............shrinkage=0.0, solver=lsqr, tol=0.001; total time=   2.3s\n",
            "[CV] END ............shrinkage=auto, solver=eigen, tol=0.001; total time=   4.4s\n",
            "[CV] END ............shrinkage=auto, solver=eigen, tol=0.001; total time=   4.5s\n",
            "[CV] END ...........shrinkage=0.75, solver=eigen, tol=0.0001; total time=   2.2s\n",
            "[CV] END .............shrinkage=0.05, solver=lsqr, tol=0.001; total time=   1.9s\n",
            "[CV] END ............shrinkage=auto, solver=eigen, tol=0.001; total time=   4.5s\n",
            "[CV] END ...........shrinkage=0.75, solver=eigen, tol=0.0001; total time=   2.3s\n",
            "[CV] END .............shrinkage=0.05, solver=lsqr, tol=0.001; total time=   2.2s\n",
            "[CV] END .............shrinkage=0.05, solver=lsqr, tol=0.001; total time=   2.2s\n",
            "[CV] END ...........shrinkage=0.75, solver=eigen, tol=0.0001; total time=   2.5s\n",
            "[CV] END .............shrinkage=0.35, solver=eigen, tol=0.01; total time=   2.2s\n",
            "[CV] END ........solver=svd, store_covariance=True, tol=0.01; total time=   7.0s\n",
            "[CV] END .............shrinkage=0.35, solver=eigen, tol=0.01; total time=   2.6s\n",
            "[CV] END ..............shrinkage=0.65, solver=lsqr, tol=0.01; total time=   1.8s\n",
            "[CV] END .............shrinkage=0.35, solver=eigen, tol=0.01; total time=   2.3s\n",
            "[CV] END ..............shrinkage=0.65, solver=lsqr, tol=0.01; total time=   2.0s\n",
            "[CV] END ..............shrinkage=0.65, solver=lsqr, tol=0.01; total time=   1.9s\n",
            "[CV] END ........solver=svd, store_covariance=True, tol=0.01; total time=   7.3s\n",
            "[CV] END .............shrinkage=auto, solver=lsqr, tol=0.001; total time=   2.6s\n",
            "[CV] END .............shrinkage=auto, solver=lsqr, tol=0.001; total time=   2.4s\n",
            "[CV] END .............shrinkage=auto, solver=lsqr, tol=0.001; total time=   2.5s\n",
            "[CV] END ..............shrinkage=None, solver=lsqr, tol=0.01; total time=   1.4s\n",
            "[CV] END ..............shrinkage=0.15, solver=lsqr, tol=0.01; total time=   1.6s\n",
            "[CV] END ..............shrinkage=0.15, solver=lsqr, tol=0.01; total time=   1.7s\n",
            "[CV] END ..............shrinkage=0.15, solver=lsqr, tol=0.01; total time=   1.9s\n",
            "[CV] END ..............shrinkage=None, solver=lsqr, tol=0.01; total time=   2.0s\n",
            "[CV] END ..............shrinkage=None, solver=lsqr, tol=0.01; total time=   1.7s\n",
            "[CV] END ............shrinkage=None, solver=lsqr, tol=0.0001; total time=   1.5s\n",
            "[CV] END ............shrinkage=0.75, solver=lsqr, tol=0.0001; total time=   1.8s\n",
            "[CV] END ............shrinkage=0.75, solver=lsqr, tol=0.0001; total time=   2.1s\n",
            "[CV] END ............shrinkage=0.15, solver=lsqr, tol=0.0001; total time=   1.6s\n",
            "[CV] END ............shrinkage=None, solver=lsqr, tol=0.0001; total time=   1.7s\n",
            "[CV] END ............shrinkage=0.75, solver=lsqr, tol=0.0001; total time=   1.9s\n",
            "[CV] END ............shrinkage=0.15, solver=lsqr, tol=0.0001; total time=   1.6s\n",
            "[CV] END ............shrinkage=0.15, solver=lsqr, tol=0.0001; total time=   1.7s\n",
            "[CV] END ............shrinkage=None, solver=lsqr, tol=0.0001; total time=   1.9s\n",
            "[CV] END .............shrinkage=auto, solver=eigen, tol=0.01; total time=   3.1s\n",
            "[CV] END .............shrinkage=auto, solver=eigen, tol=0.01; total time=   2.9s\n",
            "[CV] END .............shrinkage=auto, solver=eigen, tol=0.01; total time=   3.2s\n",
            "[CV] END ......solver=svd, store_covariance=True, tol=0.0001; total time=   4.8s\n",
            "[CV] END ......solver=svd, store_covariance=True, tol=0.0001; total time=   4.8s\n",
            "[CV] END ......solver=svd, store_covariance=True, tol=0.0001; total time=   4.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.models.exploration_phase.model_saver:Model saved: ../src/models/exploration_phase/LDA_no_sampling.joblib\n",
            "INFO:src.models.exploration_phase.model_saver:Metadata saved: ../src/models/exploration_phase/LDA_no_sampling_metadata.pkl\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model LDA saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# Check if model already exists\n",
        "classifier_name = \"LDA\"\n",
        "experiment_name = \"no_sampling\"\n",
        "\n",
        "if model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_lda = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    param_distributions = [\n",
        "        {\n",
        "            \"solver\": [\"svd\"],\n",
        "            \"store_covariance\": [False, True],\n",
        "            \"tol\": [1e-4, 1e-3, 1e-2],\n",
        "            # n_components kept implicit (None) to avoid invalid values vs. n_classes-1\n",
        "        },\n",
        "        {\n",
        "            \"solver\": [\"lsqr\", \"eigen\"],\n",
        "            \"shrinkage\": [None, \"auto\", 0.0, 0.05, 0.1, 0.15, 0.25, 0.35, 0.5, 0.65, 0.75, 0.85, 0.9],\n",
        "            \"tol\": [1e-4, 1e-3, 1e-2],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
        "\n",
        "    lda = LinearDiscriminantAnalysis()\n",
        "    rs_lda = RandomizedSearchCV(\n",
        "        estimator=lda,\n",
        "        param_distributions=param_distributions,\n",
        "        n_iter=50,\n",
        "        scoring=scoring,\n",
        "        refit=\"f1_macro\",\n",
        "        cv=cv,\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "    )\n",
        "\n",
        "\n",
        "    rs_lda.fit(X_train_s, y_train)\n",
        "    \n",
        "    # Save the trained model\n",
        "    metadata = {\n",
        "        \"best_params\": rs_lda.best_params_,\n",
        "        \"best_score\": rs_lda.best_score_,\n",
        "        \"cv_results\": rs_lda.cv_results_,\n",
        "        \"experiment\": experiment_name,\n",
        "        \"classifier\": classifier_name,\n",
        "    }\n",
        "\n",
        "    model_saver.save_model(classifier_name, rs_lda, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_lda = rs_lda.best_estimator_\n",
        "results['LDA'] = eval_model(\n",
        "    best_lda,\n",
        "    X_train_s, y_train,\n",
        "    X_test_s, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-11 {color: black;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearDiscriminantAnalysis(shrinkage=&#x27;auto&#x27;, solver=&#x27;lsqr&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>LinearDiscriminantAnalysis(shrinkage=&#x27;auto&#x27;, solver=&#x27;lsqr&#x27;)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearDiscriminantAnalysis(shrinkage='auto', solver='lsqr')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'tol': 0.0001, 'solver': 'lsqr', 'shrinkage': 'auto'}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'labels': array([0, 1, 2, 3, 4]),\n",
              " 'val': {'accuracy': 0.8941297396071265,\n",
              "  'precision_macro': 0.6346495795067106,\n",
              "  'recall_macro': 0.6599108158462592,\n",
              "  'f1_macro': 0.6339529523776317,\n",
              "  'precision_per_class': array([0.94157365, 0.56157635, 0.5323475 , 0.18181818, 0.9559322 ]),\n",
              "  'recall_per_class': array([0.94274283, 0.51351351, 0.49740933, 0.46875   , 0.87713841]),\n",
              "  'f1_per_class': array([0.94215788, 0.53647059, 0.51428571, 0.26200873, 0.91484185]),\n",
              "  'support_per_class': array([7248,  222,  579,   64,  643]),\n",
              "  'confusion_matrix': array([[6833,   69,  227,  106,   13],\n",
              "         [  94,  114,   10,    4,    0],\n",
              "         [ 234,   19,  288,   25,   13],\n",
              "         [  29,    0,    5,   30,    0],\n",
              "         [  67,    1,   11,    0,  564]])},\n",
              " 'test': {'accuracy': 0.8933857116754979,\n",
              "  'precision_macro': 0.6406346941002119,\n",
              "  'recall_macro': 0.6505553927969936,\n",
              "  'f1_macro': 0.6284933290106093,\n",
              "  'precision_per_class': array([0.93934237, 0.58119658, 0.56112377, 0.15560166, 0.96590909]),\n",
              "  'recall_per_class': array([0.94447511, 0.48920863, 0.51035912, 0.46296296, 0.84577114]),\n",
              "  'f1_per_class': array([0.94190175, 0.53125   , 0.53453888, 0.23291925, 0.90185676]),\n",
              "  'support_per_class': array([18118,   556,  1448,   162,  1608]),\n",
              "  'confusion_matrix': array([[17112,   141,   509,   332,    24],\n",
              "         [  246,   272,    32,     6,     0],\n",
              "         [  565,    53,   739,    67,    24],\n",
              "         [   81,     0,     6,    75,     0],\n",
              "         [  213,     2,    31,     2,  1360]])}}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display(best_lda)\n",
        "display(rs_lda.best_params_)\n",
        "results['LDA']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.8 Artificial Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model ANN not found. Training new model...\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "[CV] END activation=relu, alpha=0.0096476850757201, batch_size=96, beta_1=0.9273818018663584, beta_2=0.9757130651499796, hidden_layer_sizes=(64,), learning_rate_init=0.001955370866274525, validation_fraction=0.1; total time=  14.7s\n",
            "[CV] END activation=relu, alpha=0.0096476850757201, batch_size=96, beta_1=0.9273818018663584, beta_2=0.9757130651499796, hidden_layer_sizes=(64,), learning_rate_init=0.001955370866274525, validation_fraction=0.1; total time=  21.4s\n",
            "[CV] END activation=relu, alpha=0.00032927591344236165, batch_size=67, beta_1=0.928053996848047, beta_2=0.9754833330377127, hidden_layer_sizes=(128,), learning_rate_init=0.003695730787054511, validation_fraction=0.1; total time=  24.5s\n",
            "[CV] END activation=relu, alpha=0.00013066739238053285, batch_size=87, beta_1=0.9585799625653968, beta_2=0.9527641673723278, hidden_layer_sizes=(128,), learning_rate_init=0.0016305687346221474, validation_fraction=0.15; total time=  25.4s\n",
            "[CV] END activation=relu, alpha=0.00010629918194937652, batch_size=123, beta_1=0.9506959396060986, beta_2=0.9688854086244558, hidden_layer_sizes=(128,), learning_rate_init=0.0012521954287060388, validation_fraction=0.15; total time=  25.4s\n",
            "[CV] END activation=relu, alpha=0.0096476850757201, batch_size=96, beta_1=0.9273818018663584, beta_2=0.9757130651499796, hidden_layer_sizes=(64,), learning_rate_init=0.001955370866274525, validation_fraction=0.1; total time=  25.6s\n",
            "[CV] END activation=relu, alpha=0.0010677482709481358, batch_size=127, beta_1=0.9420086603923182, beta_2=0.9921370799300797, hidden_layer_sizes=(128, 64), learning_rate_init=0.0014808945119975188, validation_fraction=0.1; total time=  30.8s\n",
            "[CV] END activation=relu, alpha=0.00010629918194937652, batch_size=123, beta_1=0.9506959396060986, beta_2=0.9688854086244558, hidden_layer_sizes=(128,), learning_rate_init=0.0012521954287060388, validation_fraction=0.15; total time=  31.0s\n",
            "[CV] END activation=relu, alpha=0.0005611516415334506, batch_size=78, beta_1=0.9658794547630265, beta_2=0.9793342657256547, hidden_layer_sizes=(128, 64), learning_rate_init=0.0027914686374528727, validation_fraction=0.1; total time=  33.2s\n",
            "[CV] END activation=relu, alpha=0.0010677482709481358, batch_size=127, beta_1=0.9420086603923182, beta_2=0.9921370799300797, hidden_layer_sizes=(128, 64), learning_rate_init=0.0014808945119975188, validation_fraction=0.1; total time=  33.5s\n",
            "[CV] END activation=relu, alpha=0.00010629918194937652, batch_size=123, beta_1=0.9506959396060986, beta_2=0.9688854086244558, hidden_layer_sizes=(128,), learning_rate_init=0.0012521954287060388, validation_fraction=0.15; total time=  34.2s\n",
            "[CV] END activation=relu, alpha=0.0006305535040199287, batch_size=123, beta_1=0.9329725658964323, beta_2=0.9723474292266348, hidden_layer_sizes=(128, 64), learning_rate_init=0.004153230257664391, validation_fraction=0.15; total time=  35.2s\n",
            "[CV] END activation=relu, alpha=0.00032927591344236165, batch_size=67, beta_1=0.928053996848047, beta_2=0.9754833330377127, hidden_layer_sizes=(128,), learning_rate_init=0.003695730787054511, validation_fraction=0.1; total time=  35.4s\n",
            "[CV] END activation=relu, alpha=0.00869299151113955, batch_size=107, beta_1=0.984554904740777, beta_2=0.9938465401709548, hidden_layer_sizes=(128,), learning_rate_init=0.005336690243421028, validation_fraction=0.1; total time=  22.1s\n",
            "[CV] END activation=relu, alpha=0.00013066739238053285, batch_size=87, beta_1=0.9585799625653968, beta_2=0.9527641673723278, hidden_layer_sizes=(128,), learning_rate_init=0.0016305687346221474, validation_fraction=0.15; total time=  37.0s\n",
            "[CV] END activation=relu, alpha=0.0003034247005811288, batch_size=123, beta_1=0.9109834411360301, beta_2=0.9742636685954522, hidden_layer_sizes=(128, 64), learning_rate_init=0.0014906121385323263, validation_fraction=0.1; total time=  37.2s\n",
            "[CV] END activation=relu, alpha=0.00013066739238053285, batch_size=87, beta_1=0.9585799625653968, beta_2=0.9527641673723278, hidden_layer_sizes=(128,), learning_rate_init=0.0016305687346221474, validation_fraction=0.15; total time=  39.0s\n",
            "[CV] END activation=relu, alpha=0.0006305535040199287, batch_size=123, beta_1=0.9329725658964323, beta_2=0.9723474292266348, hidden_layer_sizes=(128, 64), learning_rate_init=0.004153230257664391, validation_fraction=0.15; total time=  41.8s\n",
            "[CV] END activation=relu, alpha=0.00032927591344236165, batch_size=67, beta_1=0.928053996848047, beta_2=0.9754833330377127, hidden_layer_sizes=(128,), learning_rate_init=0.003695730787054511, validation_fraction=0.1; total time=  42.1s\n",
            "[CV] END activation=relu, alpha=0.0005611516415334506, batch_size=78, beta_1=0.9658794547630265, beta_2=0.9793342657256547, hidden_layer_sizes=(128, 64), learning_rate_init=0.0027914686374528727, validation_fraction=0.1; total time=  44.6s\n",
            "[CV] END activation=relu, alpha=0.0003034247005811288, batch_size=123, beta_1=0.9109834411360301, beta_2=0.9742636685954522, hidden_layer_sizes=(128, 64), learning_rate_init=0.0014906121385323263, validation_fraction=0.1; total time=  47.5s\n",
            "[CV] END activation=relu, alpha=0.0003034247005811288, batch_size=123, beta_1=0.9109834411360301, beta_2=0.9742636685954522, hidden_layer_sizes=(128, 64), learning_rate_init=0.0014906121385323263, validation_fraction=0.1; total time=  49.6s\n",
            "[CV] END activation=relu, alpha=0.0005611516415334506, batch_size=78, beta_1=0.9658794547630265, beta_2=0.9793342657256547, hidden_layer_sizes=(128, 64), learning_rate_init=0.0027914686374528727, validation_fraction=0.1; total time=  56.5s\n",
            "[CV] END activation=relu, alpha=0.00869299151113955, batch_size=107, beta_1=0.984554904740777, beta_2=0.9938465401709548, hidden_layer_sizes=(128,), learning_rate_init=0.005336690243421028, validation_fraction=0.1; total time=  33.1s\n",
            "[CV] END activation=relu, alpha=0.0006305535040199287, batch_size=123, beta_1=0.9329725658964323, beta_2=0.9723474292266348, hidden_layer_sizes=(128, 64), learning_rate_init=0.004153230257664391, validation_fraction=0.15; total time= 1.0min\n",
            "[CV] END activation=relu, alpha=0.009413993046829943, batch_size=64, beta_1=0.9178844113380755, beta_2=0.9502705837390565, hidden_layer_sizes=(128, 64), learning_rate_init=0.0015806743432481031, validation_fraction=0.15; total time=  29.9s\n",
            "[CV] END activation=relu, alpha=0.00015030900645056822, batch_size=125, beta_1=0.9292797297686938, beta_2=0.9690451871947846, hidden_layer_sizes=(128,), learning_rate_init=0.003464911387927851, validation_fraction=0.15; total time=  39.3s\n",
            "[CV] END activation=relu, alpha=0.00869299151113955, batch_size=107, beta_1=0.984554904740777, beta_2=0.9938465401709548, hidden_layer_sizes=(128,), learning_rate_init=0.005336690243421028, validation_fraction=0.1; total time=  43.7s\n",
            "[CV] END activation=relu, alpha=0.0028708753481954683, batch_size=96, beta_1=0.9545363977302911, beta_2=0.9953887430471541, hidden_layer_sizes=(128,), learning_rate_init=0.0013057771348997226, validation_fraction=0.1; total time=  30.2s\n",
            "[CV] END activation=relu, alpha=0.0010677482709481358, batch_size=127, beta_1=0.9420086603923182, beta_2=0.9921370799300797, hidden_layer_sizes=(128, 64), learning_rate_init=0.0014808945119975188, validation_fraction=0.1; total time= 1.1min\n",
            "[CV] END activation=relu, alpha=0.002878805718308925, batch_size=98, beta_1=0.9424993432645754, beta_2=0.9558601180509767, hidden_layer_sizes=(128,), learning_rate_init=0.007061774157217603, validation_fraction=0.1; total time=  24.4s\n",
            "[CV] END activation=relu, alpha=0.0028708753481954683, batch_size=96, beta_1=0.9545363977302911, beta_2=0.9953887430471541, hidden_layer_sizes=(128,), learning_rate_init=0.0013057771348997226, validation_fraction=0.1; total time=  32.6s\n",
            "[CV] END activation=relu, alpha=0.005012762811014231, batch_size=75, beta_1=0.9297808222367384, beta_2=0.9531143591640151, hidden_layer_sizes=(128, 64), learning_rate_init=0.004664888338326056, validation_fraction=0.1; total time=  29.1s\n",
            "[CV] END activation=relu, alpha=0.0028708753481954683, batch_size=96, beta_1=0.9545363977302911, beta_2=0.9953887430471541, hidden_layer_sizes=(128,), learning_rate_init=0.0013057771348997226, validation_fraction=0.1; total time=  36.5s\n",
            "[CV] END activation=relu, alpha=0.00015030900645056822, batch_size=125, beta_1=0.9292797297686938, beta_2=0.9690451871947846, hidden_layer_sizes=(128,), learning_rate_init=0.003464911387927851, validation_fraction=0.15; total time=  52.6s\n",
            "[CV] END activation=relu, alpha=0.00016435497475111326, batch_size=126, beta_1=0.9806187236106167, beta_2=0.9732931409359235, hidden_layer_sizes=(64,), learning_rate_init=0.0032253042671791196, validation_fraction=0.15; total time=  18.2s\n",
            "[CV] END activation=relu, alpha=0.0005170191786366995, batch_size=123, beta_1=0.9488426474842424, beta_2=0.9569052870237633, hidden_layer_sizes=(128, 64), learning_rate_init=0.0014630761777791696, validation_fraction=0.1; total time=  49.6s\n",
            "[CV] END activation=relu, alpha=0.00016435497475111326, batch_size=126, beta_1=0.9806187236106167, beta_2=0.9732931409359235, hidden_layer_sizes=(64,), learning_rate_init=0.0032253042671791196, validation_fraction=0.15; total time=  18.3s\n",
            "[CV] END activation=relu, alpha=0.002878805718308925, batch_size=98, beta_1=0.9424993432645754, beta_2=0.9558601180509767, hidden_layer_sizes=(128,), learning_rate_init=0.007061774157217603, validation_fraction=0.1; total time=  35.2s\n",
            "[CV] END activation=relu, alpha=0.002878805718308925, batch_size=98, beta_1=0.9424993432645754, beta_2=0.9558601180509767, hidden_layer_sizes=(128,), learning_rate_init=0.007061774157217603, validation_fraction=0.1; total time=  41.4s\n",
            "[CV] END activation=relu, alpha=0.00016435497475111326, batch_size=126, beta_1=0.9806187236106167, beta_2=0.9732931409359235, hidden_layer_sizes=(64,), learning_rate_init=0.0032253042671791196, validation_fraction=0.15; total time=  18.9s\n",
            "[CV] END activation=relu, alpha=0.0005170191786366995, batch_size=123, beta_1=0.9488426474842424, beta_2=0.9569052870237633, hidden_layer_sizes=(128, 64), learning_rate_init=0.0014630761777791696, validation_fraction=0.1; total time=  51.5s\n",
            "[CV] END activation=relu, alpha=0.005012762811014231, batch_size=75, beta_1=0.9297808222367384, beta_2=0.9531143591640151, hidden_layer_sizes=(128, 64), learning_rate_init=0.004664888338326056, validation_fraction=0.1; total time=  47.0s\n",
            "[CV] END activation=relu, alpha=0.0005170191786366995, batch_size=123, beta_1=0.9488426474842424, beta_2=0.9569052870237633, hidden_layer_sizes=(128, 64), learning_rate_init=0.0014630761777791696, validation_fraction=0.1; total time=  55.9s\n",
            "[CV] END activation=relu, alpha=0.005766810466779011, batch_size=105, beta_1=0.9727308341607975, beta_2=0.9810367840690107, hidden_layer_sizes=(128,), learning_rate_init=0.0028611101001573156, validation_fraction=0.15; total time=  18.3s\n",
            "[CV] END activation=relu, alpha=0.00015030900645056822, batch_size=125, beta_1=0.9292797297686938, beta_2=0.9690451871947846, hidden_layer_sizes=(128,), learning_rate_init=0.003464911387927851, validation_fraction=0.15; total time= 1.0min\n",
            "[CV] END activation=relu, alpha=0.005766810466779011, batch_size=105, beta_1=0.9727308341607975, beta_2=0.9810367840690107, hidden_layer_sizes=(128,), learning_rate_init=0.0028611101001573156, validation_fraction=0.15; total time=  20.5s\n",
            "[CV] END activation=relu, alpha=0.005012762811014231, batch_size=75, beta_1=0.9297808222367384, beta_2=0.9531143591640151, hidden_layer_sizes=(128, 64), learning_rate_init=0.004664888338326056, validation_fraction=0.1; total time=  52.8s\n",
            "[CV] END activation=relu, alpha=0.00018996032713181662, batch_size=78, beta_1=0.9679996024688744, beta_2=0.9612111101090894, hidden_layer_sizes=(128, 64), learning_rate_init=0.0019487290213568475, validation_fraction=0.15; total time=  24.7s\n",
            "[CV] END activation=relu, alpha=0.005766810466779011, batch_size=105, beta_1=0.9727308341607975, beta_2=0.9810367840690107, hidden_layer_sizes=(128,), learning_rate_init=0.0028611101001573156, validation_fraction=0.15; total time=  19.7s\n",
            "[CV] END activation=relu, alpha=0.009413993046829943, batch_size=64, beta_1=0.9178844113380755, beta_2=0.9502705837390565, hidden_layer_sizes=(128, 64), learning_rate_init=0.0015806743432481031, validation_fraction=0.15; total time=  58.5s\n",
            "[CV] END activation=relu, alpha=0.00018996032713181662, batch_size=78, beta_1=0.9679996024688744, beta_2=0.9612111101090894, hidden_layer_sizes=(128, 64), learning_rate_init=0.0019487290213568475, validation_fraction=0.15; total time=  28.8s\n",
            "[CV] END activation=relu, alpha=0.0013260331922696556, batch_size=77, beta_1=0.9444416036727952, beta_2=0.9756139086397176, hidden_layer_sizes=(128, 64), learning_rate_init=0.0012908947091547854, validation_fraction=0.15; total time=  45.3s\n",
            "[CV] END activation=relu, alpha=0.00023612399244412623, batch_size=93, beta_1=0.9794952233026981, beta_2=0.9658929060292584, hidden_layer_sizes=(128, 64), learning_rate_init=0.00787211264452507, validation_fraction=0.1; total time=  17.2s\n",
            "[CV] END activation=relu, alpha=0.009413993046829943, batch_size=64, beta_1=0.9178844113380755, beta_2=0.9502705837390565, hidden_layer_sizes=(128, 64), learning_rate_init=0.0015806743432481031, validation_fraction=0.15; total time= 1.0min\n",
            "[CV] END activation=relu, alpha=0.00023612399244412623, batch_size=93, beta_1=0.9794952233026981, beta_2=0.9658929060292584, hidden_layer_sizes=(128, 64), learning_rate_init=0.00787211264452507, validation_fraction=0.1; total time=  16.3s\n",
            "[CV] END activation=relu, alpha=0.0013260331922696556, batch_size=77, beta_1=0.9444416036727952, beta_2=0.9756139086397176, hidden_layer_sizes=(128, 64), learning_rate_init=0.0012908947091547854, validation_fraction=0.15; total time=  41.9s\n",
            "[CV] END activation=relu, alpha=0.0013260331922696556, batch_size=77, beta_1=0.9444416036727952, beta_2=0.9756139086397176, hidden_layer_sizes=(128, 64), learning_rate_init=0.0012908947091547854, validation_fraction=0.15; total time=  44.4s\n",
            "[CV] END activation=relu, alpha=0.00023612399244412623, batch_size=93, beta_1=0.9794952233026981, beta_2=0.9658929060292584, hidden_layer_sizes=(128, 64), learning_rate_init=0.00787211264452507, validation_fraction=0.1; total time=  29.4s\n",
            "[CV] END activation=relu, alpha=0.00018996032713181662, batch_size=78, beta_1=0.9679996024688744, beta_2=0.9612111101090894, hidden_layer_sizes=(128, 64), learning_rate_init=0.0019487290213568475, validation_fraction=0.15; total time=  36.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.models.exploration_phase.model_saver:Model saved: ../src/models/exploration_phase/ANN_no_sampling.joblib\n",
            "INFO:src.models.exploration_phase.model_saver:Metadata saved: ../src/models/exploration_phase/ANN_no_sampling_metadata.pkl\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model ANN saved successfully!\n"
          ]
        }
      ],
      "source": [
        "classifier_name = \"ANN\"\n",
        "experiment_name = \"no_sampling\"\n",
        "\n",
        "if model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_lda = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    ann = MLPClassifier(\n",
        "        max_iter=300,\n",
        "        early_stopping=True,\n",
        "        random_state=random_state,\n",
        "        n_iter_no_change=10,\n",
        "        solver=\"adam\",\n",
        "    )\n",
        "\n",
        "\n",
        "    param_distributions = {\n",
        "        \"hidden_layer_sizes\": [\n",
        "            (64,),\n",
        "            (128,),\n",
        "            (128, 64),\n",
        "        ],\n",
        "        \"activation\": [\"relu\"],  # focused, fast\n",
        "        \"alpha\": loguniform(1e-4, 1e-2),  # L2\n",
        "        \"learning_rate_init\": loguniform(1e-3, 1e-2),\n",
        "        \"batch_size\": randint(64, 129),\n",
        "        \"beta_1\": uniform(0.9, 0.09),   # ~0.90-0.99\n",
        "        \"beta_2\": uniform(0.95, 0.049), # ~0.95-0.999\n",
        "        \"validation_fraction\": [0.1, 0.15],\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
        "\n",
        "    rs_ann = RandomizedSearchCV(\n",
        "        estimator=ann,\n",
        "        param_distributions=param_distributions,\n",
        "        n_iter=20,\n",
        "        scoring=scoring,\n",
        "        refit=\"f1_macro\",\n",
        "        cv=cv,\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "    )\n",
        "\n",
        "    rs_ann.fit(X_train_s, y_train)\n",
        "    \n",
        "    metadata = {\n",
        "        \"best_params\": rs_ann.best_params_,\n",
        "        \"best_score\": rs_ann.best_score_,\n",
        "        \"cv_results\": rs_ann.cv_results_,\n",
        "        \"experiment\": experiment_name,\n",
        "        \"classifier\": classifier_name,\n",
        "    }\n",
        "    model_saver.save_model(classifier_name, rs_ann, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_ann = rs_ann.best_estimator_\n",
        "results['ANN'] = eval_model(\n",
        "    best_ann,\n",
        "    X_train_s, y_train,\n",
        "    X_test_s, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-15 {color: black;}#sk-container-id-15 pre{padding: 0;}#sk-container-id-15 div.sk-toggleable {background-color: white;}#sk-container-id-15 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-15 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-15 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-15 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-15 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-15 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-15 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-15 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-15 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-15 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-15 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-15 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-15 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-15 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-15 div.sk-item {position: relative;z-index: 1;}#sk-container-id-15 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-15 div.sk-item::before, #sk-container-id-15 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-15 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-15 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-15 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-15 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-15 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-15 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-15 div.sk-label-container {text-align: center;}#sk-container-id-15 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-15 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-15\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.0005170191786366995, batch_size=123,\n",
              "              beta_1=0.9488426474842424, beta_2=0.9569052870237633,\n",
              "              early_stopping=True, hidden_layer_sizes=(128, 64),\n",
              "              learning_rate_init=0.0014630761777791696, max_iter=300,\n",
              "              random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.0005170191786366995, batch_size=123,\n",
              "              beta_1=0.9488426474842424, beta_2=0.9569052870237633,\n",
              "              early_stopping=True, hidden_layer_sizes=(128, 64),\n",
              "              learning_rate_init=0.0014630761777791696, max_iter=300,\n",
              "              random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier(alpha=0.0005170191786366995, batch_size=123,\n",
              "              beta_1=0.9488426474842424, beta_2=0.9569052870237633,\n",
              "              early_stopping=True, hidden_layer_sizes=(128, 64),\n",
              "              learning_rate_init=0.0014630761777791696, max_iter=300,\n",
              "              random_state=42)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0.8895580263203478"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'labels': array([0, 1, 2, 3, 4]),\n",
              " 'val': {'accuracy': 0.9809273640931933,\n",
              "  'precision_macro': 0.9202085425774307,\n",
              "  'recall_macro': 0.8750374326674202,\n",
              "  'f1_macro': 0.8959547701775328,\n",
              "  'precision_per_class': array([0.98765432, 0.88421053, 0.94415358, 0.80357143, 0.98145286]),\n",
              "  'recall_per_class': array([0.99337748, 0.75675676, 0.9343696 , 0.703125  , 0.98755832]),\n",
              "  'f1_per_class': array([0.99050764, 0.81553398, 0.93923611, 0.75      , 0.98449612]),\n",
              "  'support_per_class': array([7248,  222,  579,   64,  643]),\n",
              "  'confusion_matrix': array([[7200,   18,   17,    6,    7],\n",
              "         [  50,  168,    2,    0,    2],\n",
              "         [  26,    4,  541,    5,    3],\n",
              "         [   8,    0,   11,   45,    0],\n",
              "         [   6,    0,    2,    0,  635]])},\n",
              " 'test': {'accuracy': 0.9778457884158597,\n",
              "  'precision_macro': 0.9155768607194622,\n",
              "  'recall_macro': 0.8594817664280281,\n",
              "  'f1_macro': 0.8849524970380223,\n",
              "  'precision_per_class': array([0.98451521, 0.86067416, 0.93907563, 0.80985915, 0.98376015]),\n",
              "  'recall_per_class': array([0.99310078, 0.68884892, 0.92610497, 0.70987654, 0.97947761]),\n",
              "  'f1_per_class': array([0.98878936, 0.76523477, 0.9325452 , 0.75657895, 0.98161421]),\n",
              "  'support_per_class': array([18118,   556,  1448,   162,  1608]),\n",
              "  'confusion_matrix': array([[17993,    50,    48,    10,    17],\n",
              "         [  156,   383,    12,     0,     5],\n",
              "         [   77,     9,  1341,    17,     4],\n",
              "         [   26,     0,    21,   115,     0],\n",
              "         [   24,     3,     6,     0,  1575]])}}"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display(best_ann)\n",
        "display(rs_ann.best_score_)\n",
        "results['ANN']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.9 Results Summary and Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "MODEL COMPARISON WITH BEST PARAMETERS FROM RANDOMIZEDSEARCHCV\n",
            "====================================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>val_accuracy</th>\n",
              "      <th>val_f1_macro</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>test_f1_macro</th>\n",
              "      <th>best_cv_score</th>\n",
              "      <th>best_parameters</th>\n",
              "      <th>val_f1_cls_0</th>\n",
              "      <th>val_f1_cls_1</th>\n",
              "      <th>val_f1_cls_2</th>\n",
              "      <th>val_f1_cls_3</th>\n",
              "      <th>val_f1_cls_4</th>\n",
              "      <th>test_f1_cls_0</th>\n",
              "      <th>test_f1_cls_1</th>\n",
              "      <th>test_f1_cls_2</th>\n",
              "      <th>test_f1_cls_3</th>\n",
              "      <th>test_f1_cls_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.90</td>\n",
              "      <td>{'subsample': 0.8, 'reg_lambda': 0, 'reg_alpha...</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.88</td>\n",
              "      <td>{'metric': 'manhattan', 'n_neighbors': 4, 'p':...</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.88</td>\n",
              "      <td>{'n_estimators': 200, 'min_samples_split': 20,...</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.87</td>\n",
              "      <td>{'kernel': 'poly', 'gamma': 0.01, 'C': 10}</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DecisionTree</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.83</td>\n",
              "      <td>{'splitter': 'best', 'min_samples_split': 20, ...</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.66</td>\n",
              "      <td>{'C': 4.0428727350273315, 'penalty': 'l2', 'so...</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                model  val_accuracy  val_f1_macro  test_accuracy  \\\n",
              "0             XGBoost          0.98          0.90           0.98   \n",
              "1                 KNN          0.98          0.89           0.98   \n",
              "2        RandomForest          0.98          0.89           0.97   \n",
              "3                 SVM          0.97          0.88           0.97   \n",
              "4        DecisionTree          0.96          0.82           0.96   \n",
              "5  LogisticRegression          0.92          0.68           0.92   \n",
              "\n",
              "   test_f1_macro  best_cv_score  \\\n",
              "0           0.90           0.90   \n",
              "1           0.88           0.88   \n",
              "2           0.88           0.88   \n",
              "3           0.87           0.87   \n",
              "4           0.81           0.83   \n",
              "5           0.66           0.66   \n",
              "\n",
              "                                     best_parameters  val_f1_cls_0  \\\n",
              "0  {'subsample': 0.8, 'reg_lambda': 0, 'reg_alpha...          0.99   \n",
              "1  {'metric': 'manhattan', 'n_neighbors': 4, 'p':...          0.99   \n",
              "2  {'n_estimators': 200, 'min_samples_split': 20,...          0.99   \n",
              "3         {'kernel': 'poly', 'gamma': 0.01, 'C': 10}          0.99   \n",
              "4  {'splitter': 'best', 'min_samples_split': 20, ...          0.98   \n",
              "5  {'C': 4.0428727350273315, 'penalty': 'l2', 'so...          0.95   \n",
              "\n",
              "   val_f1_cls_1  val_f1_cls_2  val_f1_cls_3  val_f1_cls_4  test_f1_cls_0  \\\n",
              "0          0.80          0.94          0.79          0.99           0.99   \n",
              "1          0.79          0.94          0.76          0.99           0.99   \n",
              "2          0.80          0.93          0.73          0.98           0.99   \n",
              "3          0.76          0.91          0.77          0.99           0.99   \n",
              "4          0.68          0.88          0.60          0.95           0.98   \n",
              "5          0.61          0.46          0.44          0.93           0.95   \n",
              "\n",
              "   test_f1_cls_1  test_f1_cls_2  test_f1_cls_3  test_f1_cls_4  \n",
              "0           0.81           0.95           0.79           0.98  \n",
              "1           0.77           0.93           0.75           0.98  \n",
              "2           0.78           0.93           0.76           0.97  \n",
              "3           0.74           0.91           0.72           0.98  \n",
              "4           0.67           0.88           0.59           0.95  \n",
              "5           0.56           0.44           0.45           0.92  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def _safe_col(label):\n",
        "    # Make safe column names like \"val_f1_cls_0\" or \"val_f1_cls_N\"\n",
        "    return re.sub(r'[^0-9a-zA-Z_]+', '_', str(label)).strip('_')\n",
        "\n",
        "# Mapping of model names to their RandomizedSearchCV objects\n",
        "models_and_searchers = {\n",
        "    \"LogisticRegression\": rs_logreg,\n",
        "    \"KNN\": rs_knn, \n",
        "    \"RandomForest\": rs_rf,\n",
        "    \"SVM\": rs_svm,\n",
        "    \"DecisionTree\": rs_dt,\n",
        "    \"XGBoost\": rs_xgb,\n",
        "    \"LDA\": rs_lda,\n",
        "    \"ANN\": rs_ann,\n",
        "}\n",
        "\n",
        "rows = []\n",
        "for name, res in results.items():\n",
        "    row = {\n",
        "        'model': name,\n",
        "        'test_accuracy': round(res['test']['accuracy'], 2),\n",
        "        'test_f1_macro': round(res['test']['f1_macro'], 2),\n",
        "    }\n",
        "\n",
        "    # Add best parameters from RandomizedSearchCV\n",
        "    if name in models_and_searchers:\n",
        "        searcher = models_and_searchers[name]\n",
        "        best_params = searcher.best_params_\n",
        "        best_cv_score = searcher.best_score_\n",
        "        row['best_cv_score'] = round(best_cv_score, 2)\n",
        "        row['best_parameters'] = str(best_params)\n",
        "    else:\n",
        "        row['best_cv_score'] = None\n",
        "        row['best_parameters'] = None\n",
        "\n",
        "    labels = res['labels']\n",
        "    f1_t = res['test']['f1_per_class']\n",
        "\n",
        "    # Add per-class F1 columns for test set only\n",
        "    for lbl, f1 in zip(labels, f1_t):\n",
        "        row[f'test_f1_cls_{_safe_col(lbl)}'] = round(f1, 2)\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "comparison_df = (\n",
        "    pd.DataFrame(rows)\n",
        "      .sort_values(by=['test_f1_macro'], ascending=False)\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "best_model_name = comparison_df.iloc[0]['model']\n",
        "best_model_results = results[best_model_name]\n",
        "\n",
        "comparison_df_display = comparison_df.copy()\n",
        "comparison_df_display['best_parameters'] = comparison_df_display['best_parameters'].apply(\n",
        "    lambda x: json.dumps(x, indent=2) if isinstance(x, dict) else x\n",
        ")\n",
        "import os \n",
        "\n",
        "comparison_df_display.to_csv(\"../src/data/03_model_testing_results/model_comparison_without_resampling.csv\", index=False)\n",
        "\n",
        "# Display the comparison table with best parameters\n",
        "print(\"=\" * 100)\n",
        "print(\"MODEL COMPARISON WITH BEST PARAMETERS FROM RANDOMIZEDSEARCHCV\")\n",
        "print(\"=\" * 100)\n",
        "display(comparison_df_display)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SAVED MODELS INFORMATION\n",
            "================================================================================\n",
            "\n",
            "Model: DecisionTree_no_sampling\n",
            "  Exists: True\n",
            "  Path: ../src/models/exploration_phase/DecisionTree_no_sampling.joblib\n",
            "  Size: 232415 bytes\n",
            "  Modified: 1760415820.9521165\n",
            "\n",
            "Model: SVM_no_sampling\n",
            "  Exists: True\n",
            "  Path: ../src/models/exploration_phase/SVM_no_sampling.joblib\n",
            "  Size: 12504219 bytes\n",
            "  Modified: 1760415063.7689745\n",
            "\n",
            "Model: KNN_no_sampling\n",
            "  Exists: True\n",
            "  Path: ../src/models/exploration_phase/KNN_no_sampling.joblib\n",
            "  Size: 118528799 bytes\n",
            "  Modified: 1760396388.739042\n",
            "\n",
            "Model: RandomForest_no_sampling\n",
            "  Exists: True\n",
            "  Path: ../src/models/exploration_phase/RandomForest_no_sampling.joblib\n",
            "  Size: 48327727 bytes\n",
            "  Modified: 1760399598.0772789\n",
            "\n",
            "Model: LogisticRegression_no_sampling\n",
            "  Exists: True\n",
            "  Path: ../src/models/exploration_phase/LogisticRegression_no_sampling.joblib\n",
            "  Size: 26175 bytes\n",
            "  Modified: 1760395829.1684663\n",
            "\n",
            "Model: XGBoost_no_sampling\n",
            "  Exists: True\n",
            "  Path: ../src/models/exploration_phase/XGBoost_no_sampling.joblib\n",
            "  Size: 2872222 bytes\n",
            "  Modified: 1760417090.5740137\n"
          ]
        }
      ],
      "source": [
        "# Check saved models\n",
        "print(\"=\" * 80)\n",
        "print(\"SAVED MODELS INFORMATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "saved_models = model_saver.list_saved_models()\n",
        "if saved_models:\n",
        "    for model_key, info in saved_models.items():\n",
        "        print(f\"\\nModel: {model_key}\")\n",
        "        print(f\"  Exists: {info['exists']}\")\n",
        "        print(f\"  Path: {info['model_path']}\")\n",
        "        if info['exists']:\n",
        "            print(f\"  Size: {info['size_bytes']} bytes\")\n",
        "            print(f\"  Modified: {info['modified_time']}\")\n",
        "        \n",
        "        # Load and display metadata if available\n",
        "        if info['metadata_exists']:\n",
        "            try:\n",
        "                metadata = model_saver.load_metadata(model_key.split('_')[0], model_key.split('_')[1] if '_' in model_key else 'default')\n",
        "                if metadata:\n",
        "                    print(f\"  Best Score: {metadata.get('best_score', 'N/A')}\")\n",
        "                    print(f\"  Best Params: {metadata.get('best_params', 'N/A')}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error loading metadata: {e}\")\n",
        "else:\n",
        "    print(\"No saved models found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. With Sampling Methods\n",
        "\n",
        "but without\n",
        "- Feature Engineering ( RR-Interval! )\n",
        "- baseline wandering removal\n",
        "- denoising\n",
        "- Leak-Free Scaling\n",
        "- RepeatedStratifiedKFold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### 3.2.1 Quick run - Using the best models from above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'best_knn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      1\u001b[39m sampling_methods = {\n\u001b[32m      2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mNo_Sampling\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mRandomOverSampler\u001b[39m\u001b[33m'\u001b[39m: RandomOverSampler(random_state=random_state),\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mSMOTEENN\u001b[39m\u001b[33m'\u001b[39m: SMOTEENN(random_state=random_state, smote=SMOTE(random_state=random_state, k_neighbors=\u001b[32m5\u001b[39m)),\n\u001b[32m      8\u001b[39m }\n\u001b[32m     10\u001b[39m sampling_results = {}\n\u001b[32m     12\u001b[39m best_models = {\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mKNN\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mbest_knn\u001b[49m,\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mRandomForest\u001b[39m\u001b[33m'\u001b[39m: best_rf,\n\u001b[32m     15\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mXGBoost\u001b[39m\u001b[33m'\u001b[39m: best_xgb,\n\u001b[32m     16\u001b[39m }\n\u001b[32m     18\u001b[39m scale_sensitive = [\u001b[33m'\u001b[39m\u001b[33mLogisticRegression\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSVM\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mKNN\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting sampling methods on best models...\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'best_knn' is not defined"
          ]
        }
      ],
      "source": [
        "sampling_methods = {\n",
        "    'No_Sampling': None,\n",
        "    'RandomOverSampler': RandomOverSampler(random_state=random_state),\n",
        "    'SMOTE': SMOTE(random_state=random_state, k_neighbors=5),\n",
        "    'ADASYN': ADASYN(random_state=random_state, n_neighbors=5),\n",
        "    'SMOTETomek': SMOTETomek(random_state=random_state, smote=SMOTE(random_state=random_state, k_neighbors=5)),\n",
        "    'SMOTEENN': SMOTEENN(random_state=random_state, smote=SMOTE(random_state=random_state, k_neighbors=5)),\n",
        "}\n",
        "\n",
        "sampling_results = {}\n",
        "\n",
        "best_models = {\n",
        "    'KNN': best_knn,\n",
        "    'RandomForest': best_rf,\n",
        "    'XGBoost': best_xgb,\n",
        "}\n",
        "\n",
        "scale_sensitive = ['LogisticRegression', 'SVM', 'KNN']\n",
        "\n",
        "print(\"Testing sampling methods on best models...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for sampling_name, sampler in sampling_methods.items():\n",
        "    print(f\"\\nTesting {sampling_name}...\")\n",
        "    sampling_results[sampling_name] = {}\n",
        "    \n",
        "    for model_name, model in best_models.items():\n",
        "        print(f\"  - {model_name}\")\n",
        "        \n",
        "        try:\n",
        "            if sampler is None:\n",
        "                # No sampling - use original, only scaled data\n",
        "                result = eval_model(model,\n",
        "                                    X_train_s if model_name in scale_sensitive else X_train , y_train,\n",
        "                                    X_test_s if model_name in scale_sensitive else X_test, y_test)\n",
        "            else:\n",
        "                # Apply sampling on unscaled data\n",
        "                X_train_sampled, y_train_sampled = sampler.fit_resample(X_train, y_train)\n",
        "                \n",
        "                # Re-scale if needed for models that require scaling\n",
        "                if model_name in scale_sensitive:\n",
        "                    scaler_sampling = StandardScaler()\n",
        "                    X_train_sampled = scaler_sampling.fit_transform(X_train_sampled)\n",
        "                    X_test_sampled = scaler_sampling.transform(X_test)\n",
        "                else: # e.g. RF, XGBoost\n",
        "                    X_test_sampled = X_test\n",
        "            \n",
        "                result = eval_model(\n",
        "                    model,\n",
        "                    X_train_sampled, y_train_sampled,\n",
        "                    X_test_sampled, y_test,\n",
        "                )\n",
        "            \n",
        "            sampling_results[sampling_name][model_name] = result\n",
        "            \n",
        "            # Printing statistics\n",
        "            if sampler is not None:\n",
        "                unique, counts = np.unique(y_train_sampled, return_counts=True)\n",
        "                print(f\"    Class distribution after {sampling_name}:\")\n",
        "                for cls, count in zip(unique, counts):\n",
        "                    print(f\"      Class {cls}: {count:,} samples\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"    ERROR with {sampling_name} + {model_name}: {str(e)}\")\n",
        "            sampling_results[sampling_name][model_name] = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Create comprehensive comparison table\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"SAMPLING METHODS COMPARISON\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Prepare comparison data\n",
        "comparison_rows = []\n",
        "\n",
        "for sampling_name, models_results in sampling_results.items():\n",
        "    for model_name, result in models_results.items():\n",
        "        if result is not None:\n",
        "            row = {\n",
        "                'sampling_method': sampling_name,\n",
        "                'model': model_name,\n",
        "                'test_accuracy': round(result['test']['accuracy'],2),\n",
        "                'test_f1_macro': round(result['test']['f1_macro'],2)\n",
        "            }\n",
        "            \n",
        "            # Add per-class F1 scores for test set only\n",
        "            labels = result['labels']\n",
        "            f1_t = result['test']['f1_per_class']\n",
        "            \n",
        "            for lbl, f1 in zip(labels, f1_t):\n",
        "                row[f'test_f1_cls_{_safe_col(lbl)}'] = round(f1,2)\n",
        "            \n",
        "            comparison_rows.append(row)\n",
        "\n",
        "# Create and display comparison DataFrame\n",
        "sampling_comparison_df = (\n",
        "    pd.DataFrame(comparison_rows)\n",
        "    .sort_values(by=['test_f1_macro'], ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BEST COMBINATION:\n",
            "Sampling Method: RandomOverSampler\n",
            "Model: XGBoost\n",
            "Test F1-Macro: 0.9200\n",
            "Validation F1-Macro: 0.9200\n",
            "\n",
            "SUMMARY STATISTICS:\n",
            "Total combinations tested: 18\n",
            "Best test F1-macro: 0.9200\n",
            "Best validation F1-macro: 0.9200\n",
            "\n",
            "TOP 5 COMBINATIONS:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sampling_method</th>\n",
              "      <th>model</th>\n",
              "      <th>test_f1_macro</th>\n",
              "      <th>val_f1_macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RandomOverSampler</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SMOTE</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SMOTETomek</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>No_Sampling</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RandomOverSampler</td>\n",
              "      <td>RandomForest</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sampling_method         model  test_f1_macro  val_f1_macro\n",
              "0  RandomOverSampler       XGBoost           0.92          0.92\n",
              "1              SMOTE       XGBoost           0.91          0.91\n",
              "2         SMOTETomek       XGBoost           0.91          0.91\n",
              "3        No_Sampling       XGBoost           0.90          0.90\n",
              "4  RandomOverSampler  RandomForest           0.90          0.90"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sampling_comparison_df.to_csv(\"../reports/03_model_testing_results/model_comparison_with_sampling_on_best_models.csv\", index=False)\n",
        "\n",
        "# Find best combination\n",
        "best_sampling_model = sampling_comparison_df.iloc[0]\n",
        "print(f\"\\nBEST COMBINATION:\")\n",
        "print(f\"Sampling Method: {best_sampling_model['sampling_method']}\")\n",
        "print(f\"Model: {best_sampling_model['model']}\")\n",
        "print(f\"Test F1-Macro: {best_sampling_model['test_f1_macro']:.4f}\")\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\nSUMMARY STATISTICS:\")\n",
        "print(f\"Total combinations tested: {len(comparison_rows)}\")\n",
        "print(f\"Best test F1-macro: {sampling_comparison_df['test_f1_macro'].max():.4f}\")\n",
        "\n",
        "# Show top 5 combinations\n",
        "print(f\"\\nTOP 5 COMBINATIONS:\")\n",
        "top_5 = sampling_comparison_df.head(5)[['sampling_method', 'model', 'test_f1_macro']]\n",
        "display(top_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.2.2 Extended Run: Sampling + RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Model Training Phase\n",
        "# This cell focuses only on training models with RandomizedSearchCV\n",
        "# Evaluation is separated to prevent interruption of cross-validation runs\n",
        "\n",
        "sampling_methods = {\n",
        "    'RandomOverSampler': RandomOverSampler(random_state=random_state),\n",
        "    'SMOTE': SMOTE(random_state=random_state, k_neighbors=5),\n",
        "    'ADASYN': ADASYN(random_state=random_state, n_neighbors=5),\n",
        "    'SMOTETomek': SMOTETomek(random_state=random_state, smote=SMOTE(random_state=random_state, k_neighbors=5)),\n",
        "    'SMOTEENN': SMOTEENN(random_state=random_state, smote=SMOTE(random_state=random_state, k_neighbors=5)),\n",
        "}\n",
        "\n",
        "# Which models need scaling (no pipeline used; fit scaler once on the resampled training set)\n",
        "scale_sensitive = [\"LogisticRegression\", \"KNN\", \"SVM\", \"LDA\", \"ANN\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting 3.2.2 TRAINING PHASE: Full RandomizedSearchCV for each model, per sampling method\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for sampling_name, sampler in sampling_methods.items():\n",
        "    print(f\"\\n=== Training with Sampling: {sampling_name} ===\")\n",
        "\n",
        "    # Apply sampling on original training set (before CV)\n",
        "    try:\n",
        "        if sampler is None:\n",
        "            X_train_res, y_train_res = X_train, y_train\n",
        "        else:\n",
        "            X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
        "    except Exception as e:\n",
        "        print(f\"  Skipping sampling '{sampling_name}' due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Show distribution if sampling applied\n",
        "    if sampler is not None:\n",
        "        unique, counts = np.unique(y_train_res, return_counts=True)\n",
        "        print(\"  Class distribution after sampling:\")\n",
        "        for cls, cnt in zip(unique, counts):\n",
        "            print(f\"    Class {cls}: {cnt:,} samples\")\n",
        "\n",
        "    # For each model, run RS-CV on the resampled dataset (TRAINING ONLY)\n",
        "    for model_name, spec in param_spaces.items():\n",
        "        experiment_name = f\"with_sampling_{sampling_name}\"\n",
        "        classifier_name = model_name\n",
        "\n",
        "        # Prepare data (leak-prone scaling by design here; no pipelines)\n",
        "        if model_name in scale_sensitive:\n",
        "            scaler = StandardScaler()\n",
        "            X_tr_fit = scaler.fit_transform(X_train_res)\n",
        "        else:\n",
        "            X_tr_fit = X_train_res\n",
        "\n",
        "        # Train or load if already saved\n",
        "        try:\n",
        "            # Check if an RS-CV object already exists for this sampler+model\n",
        "            if model_saver.model_exists(classifier_name, experiment_name):\n",
        "                print(f\"  [{model_name}] Exists for {experiment_name}. LoaLoading next one...\")\n",
        "            else:\n",
        "                print(f\"  [{model_name}] Training RS-CV for {experiment_name}...\")\n",
        "                rs = RandomizedSearchCV(\n",
        "                    estimator=spec[\"estimator\"],\n",
        "                    param_distributions=spec[\"params\"],\n",
        "                    n_iter=spec[\"n_iter\"],\n",
        "                    scoring=scoring,\n",
        "                    refit=\"f1_macro\",\n",
        "                    cv=spec[\"cv\"],\n",
        "                    random_state=random_state,\n",
        "                    n_jobs=-1,\n",
        "                    verbose=2,\n",
        "                )\n",
        "                rs.fit(X_tr_fit, y_train_res)\n",
        "\n",
        "                metadata = {\n",
        "                    \"best_params\": rs.best_params_,\n",
        "                    \"best_score\": rs.best_score_,\n",
        "                    \"cv_results\": rs.cv_results_,\n",
        "                    \"experiment\": experiment_name,\n",
        "                    \"classifier\": classifier_name,\n",
        "                    \"sampling_method\": sampling_name,\n",
        "                }\n",
        "                model_saver.save_model(classifier_name, rs, experiment_name, metadata)\n",
        "                print(f\"  [{model_name}] Saved for {experiment_name}.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  [{model_name}] ERROR for {experiment_name}: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TRAINING PHASE COMPLETED\")\n",
        "print(\"=\" * 80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting 3.2.2 EVALUATION PHASE: Evaluating trained models\n",
            "================================================================================\n",
            "No existing results found. Will create ../reports/03_model_testing_results/model_comparison_with_sampling_randomized_search.csv\n",
            "Starting 3.2.2 EVALUATION PHASE: Evaluating trained models\n",
            "================================================================================\n",
            "\n",
            "=== Evaluating with Sampling: RandomOverSampler ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/LogisticRegression_with_sampling_RandomOverSampler.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [LogisticRegression] Loading trained model for evaluation...\n",
            "  Evaluating model [LogisticRegression]\n",
            "  ✅ Result saved for [LogisticRegression] (RandomOverSampler)\n",
            "  [KNN] Loading trained model for evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/KNN_with_sampling_RandomOverSampler.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Evaluating model [KNN]\n",
            "  ✅ Result saved for [KNN] (RandomOverSampler)\n",
            "  [RandomForest] Loading trained model for evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/RandomForest_with_sampling_RandomOverSampler.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Evaluating model [RandomForest]\n",
            "  ✅ Result saved for [RandomForest] (RandomOverSampler)\n",
            "  [SVM] Loading trained model for evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/SVM_with_sampling_RandomOverSampler.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Evaluating model [SVM]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/DecisionTree_with_sampling_RandomOverSampler.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ Result saved for [SVM] (RandomOverSampler)\n",
            "  [DecisionTree] Loading trained model for evaluation...\n",
            "  Evaluating model [DecisionTree]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/XGBoost_with_sampling_RandomOverSampler.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ Result saved for [DecisionTree] (RandomOverSampler)\n",
            "  [XGBoost] Loading trained model for evaluation...\n",
            "  Evaluating model [XGBoost]\n",
            "  ✅ Result saved for [XGBoost] (RandomOverSampler)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/LDA_with_sampling_RandomOverSampler.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [LDA] Loading trained model for evaluation...\n",
            "  Evaluating model [LDA]\n",
            "  ✅ Result saved for [LDA] (RandomOverSampler)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/ANN_with_sampling_RandomOverSampler.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [ANN] Loading trained model for evaluation...\n",
            "  Evaluating model [ANN]\n",
            "  ✅ Result saved for [ANN] (RandomOverSampler)\n",
            "\n",
            "=== Evaluating with Sampling: SMOTE ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/LogisticRegression_with_sampling_SMOTE.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [LogisticRegression] Loading trained model for evaluation...\n",
            "  Evaluating model [LogisticRegression]\n",
            "  ✅ Result saved for [LogisticRegression] (SMOTE)\n",
            "  [KNN] Loading trained model for evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/KNN_with_sampling_SMOTE.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Evaluating model [KNN]\n",
            "  ✅ Result saved for [KNN] (SMOTE)\n",
            "  [RandomForest] Loading trained model for evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/RandomForest_with_sampling_SMOTE.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Evaluating model [RandomForest]\n",
            "  ✅ Result saved for [RandomForest] (SMOTE)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/SVM_with_sampling_SMOTE.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [SVM] Loading trained model for evaluation...\n",
            "  Evaluating model [SVM]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/DecisionTree_with_sampling_SMOTE.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ Result saved for [SVM] (SMOTE)\n",
            "  [DecisionTree] Loading trained model for evaluation...\n",
            "  Evaluating model [DecisionTree]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/XGBoost_with_sampling_SMOTE.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ Result saved for [DecisionTree] (SMOTE)\n",
            "  [XGBoost] Loading trained model for evaluation...\n",
            "  Evaluating model [XGBoost]\n",
            "  ✅ Result saved for [XGBoost] (SMOTE)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/LDA_with_sampling_SMOTE.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [LDA] Loading trained model for evaluation...\n",
            "  Evaluating model [LDA]\n",
            "  ✅ Result saved for [LDA] (SMOTE)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/ANN_with_sampling_SMOTE.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [ANN] Loading trained model for evaluation...\n",
            "  Evaluating model [ANN]\n",
            "  ✅ Result saved for [ANN] (SMOTE)\n",
            "\n",
            "=== Evaluating with Sampling: ADASYN ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/LogisticRegression_with_sampling_ADASYN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [LogisticRegression] Loading trained model for evaluation...\n",
            "  Evaluating model [LogisticRegression]\n",
            "  ✅ Result saved for [LogisticRegression] (ADASYN)\n",
            "  [KNN] Loading trained model for evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/KNN_with_sampling_ADASYN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Evaluating model [KNN]\n",
            "  ✅ Result saved for [KNN] (ADASYN)\n",
            "  [RandomForest] Loading trained model for evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/RandomForest_with_sampling_ADASYN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Evaluating model [RandomForest]\n",
            "  ✅ Result saved for [RandomForest] (ADASYN)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/SVM_with_sampling_ADASYN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [SVM] Loading trained model for evaluation...\n",
            "  Evaluating model [SVM]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/DecisionTree_with_sampling_ADASYN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ Result saved for [SVM] (ADASYN)\n",
            "  [DecisionTree] Loading trained model for evaluation...\n",
            "  Evaluating model [DecisionTree]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/XGBoost_with_sampling_ADASYN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ Result saved for [DecisionTree] (ADASYN)\n",
            "  [XGBoost] Loading trained model for evaluation...\n",
            "  Evaluating model [XGBoost]\n",
            "  ✅ Result saved for [XGBoost] (ADASYN)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/LDA_with_sampling_ADASYN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [LDA] Loading trained model for evaluation...\n",
            "  Evaluating model [LDA]\n",
            "  ✅ Result saved for [LDA] (ADASYN)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/ANN_with_sampling_ADASYN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [ANN] Loading trained model for evaluation...\n",
            "  Evaluating model [ANN]\n",
            "  ✅ Result saved for [ANN] (ADASYN)\n",
            "\n",
            "=== Evaluating with Sampling: SMOTETomek ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/LogisticRegression_with_sampling_SMOTETomek.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [LogisticRegression] Loading trained model for evaluation...\n",
            "  Evaluating model [LogisticRegression]\n",
            "  ✅ Result saved for [LogisticRegression] (SMOTETomek)\n",
            "  [KNN] Loading trained model for evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/KNN_with_sampling_SMOTETomek.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Evaluating model [KNN]\n",
            "  ✅ Result saved for [KNN] (SMOTETomek)\n",
            "  [RandomForest] Loading trained model for evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/RandomForest_with_sampling_SMOTETomek.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Evaluating model [RandomForest]\n",
            "  ✅ Result saved for [RandomForest] (SMOTETomek)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/SVM_with_sampling_SMOTETomek.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [SVM] Loading trained model for evaluation...\n",
            "  Evaluating model [SVM]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/DecisionTree_with_sampling_SMOTETomek.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ Result saved for [SVM] (SMOTETomek)\n",
            "  [DecisionTree] Loading trained model for evaluation...\n",
            "  Evaluating model [DecisionTree]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/XGBoost_with_sampling_SMOTETomek.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ Result saved for [DecisionTree] (SMOTETomek)\n",
            "  [XGBoost] Loading trained model for evaluation...\n",
            "  Evaluating model [XGBoost]\n",
            "  ✅ Result saved for [XGBoost] (SMOTETomek)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/LDA_with_sampling_SMOTETomek.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [LDA] Loading trained model for evaluation...\n",
            "  Evaluating model [LDA]\n",
            "  ✅ Result saved for [LDA] (SMOTETomek)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/ANN_with_sampling_SMOTETomek.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [ANN] Loading trained model for evaluation...\n",
            "  Evaluating model [ANN]\n",
            "  ✅ Result saved for [ANN] (SMOTETomek)\n",
            "\n",
            "=== Evaluating with Sampling: SMOTEENN ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/LogisticRegression_with_sampling_SMOTEENN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [LogisticRegression] Loading trained model for evaluation...\n",
            "  Evaluating model [LogisticRegression]\n",
            "  ✅ Result saved for [LogisticRegression] (SMOTEENN)\n",
            "  [KNN] Loading trained model for evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/KNN_with_sampling_SMOTEENN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Evaluating model [KNN]\n",
            "  ✅ Result saved for [KNN] (SMOTEENN)\n",
            "  [RandomForest] Loading trained model for evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/RandomForest_with_sampling_SMOTEENN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Evaluating model [RandomForest]\n",
            "  ✅ Result saved for [RandomForest] (SMOTEENN)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/SVM_with_sampling_SMOTEENN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [SVM] Loading trained model for evaluation...\n",
            "  Evaluating model [SVM]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/DecisionTree_with_sampling_SMOTEENN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ Result saved for [SVM] (SMOTEENN)\n",
            "  [DecisionTree] Loading trained model for evaluation...\n",
            "  Evaluating model [DecisionTree]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/XGBoost_with_sampling_SMOTEENN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ Result saved for [DecisionTree] (SMOTEENN)\n",
            "  [XGBoost] Loading trained model for evaluation...\n",
            "  Evaluating model [XGBoost]\n",
            "  ✅ Result saved for [XGBoost] (SMOTEENN)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/LDA_with_sampling_SMOTEENN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [LDA] Loading trained model for evaluation...\n",
            "  Evaluating model [LDA]\n",
            "  ✅ Result saved for [LDA] (SMOTEENN)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.utils.model_saver:Model loaded: ../src/models/exploration_phase/ANN_with_sampling_SMOTEENN.joblib\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [ANN] Loading trained model for evaluation...\n",
            "  Evaluating model [ANN]\n",
            "  ✅ Result saved for [ANN] (SMOTEENN)\n",
            "\n",
            "====================================================================================================\n",
            "CURRENT MODEL EVALUATION SUMMARY\n",
            "====================================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sampling_method</th>\n",
              "      <th>model</th>\n",
              "      <th>test_f1_macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>RandomOverSampler</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>SMOTE</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>SMOTETomek</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>ADASYN</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>ADASYN</td>\n",
              "      <td>ANN</td>\n",
              "      <td>0.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>SMOTEENN</td>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>RandomOverSampler</td>\n",
              "      <td>ANN</td>\n",
              "      <td>0.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SMOTE</td>\n",
              "      <td>ANN</td>\n",
              "      <td>0.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>SMOTETomek</td>\n",
              "      <td>ANN</td>\n",
              "      <td>0.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>SMOTETomek</td>\n",
              "      <td>SVM</td>\n",
              "      <td>0.86</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      sampling_method    model  test_f1_macro\n",
              "5   RandomOverSampler  XGBoost           0.92\n",
              "13              SMOTE  XGBoost           0.91\n",
              "29         SMOTETomek  XGBoost           0.91\n",
              "21             ADASYN  XGBoost           0.90\n",
              "23             ADASYN      ANN           0.89\n",
              "37           SMOTEENN  XGBoost           0.89\n",
              "7   RandomOverSampler      ANN           0.88\n",
              "15              SMOTE      ANN           0.88\n",
              "31         SMOTETomek      ANN           0.88\n",
              "27         SMOTETomek      SVM           0.86"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "EVALUATION PHASE COMPLETED\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Model Evaluation Phase\n",
        "# This cell focuses only on evaluating the trained models\n",
        "# This separation prevents interruption of cross-validation runs from affecting evaluation\n",
        "\n",
        "print(\"Starting 3.2.2 EVALUATION PHASE: Evaluating trained models\")\n",
        "print(\"=\" * 80)\n",
        "# Path to results file\n",
        "out_path = \"../reports/03_model_testing_results/model_comparison_with_sampling_randomized_search.csv\"\n",
        "\n",
        "# Load existing results if CSV already exists\n",
        "if os.path.exists(out_path):\n",
        "    existing_df = pd.read_csv(out_path)\n",
        "    print(f\"Loaded existing results with {len(existing_df)} rows from {out_path}\")\n",
        "else:\n",
        "    existing_df = pd.DataFrame()\n",
        "    print(f\"No existing results found. Will create {out_path}\")\n",
        "\n",
        "print(\"Starting 3.2.2 EVALUATION PHASE: Evaluating trained models\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Evaluation and saving\n",
        "sampling_results = {}\n",
        "\n",
        "for sampling_name, sampler in sampling_methods.items():\n",
        "    print(f\"\\n=== Evaluating with Sampling: {sampling_name} ===\")\n",
        "    sampling_results[sampling_name] = {}\n",
        "\n",
        "    # Apply sampling on original training set (before CV)\n",
        "    try:\n",
        "        if sampler is None:\n",
        "            X_train_res, y_train_res = X_train, y_train\n",
        "        else:\n",
        "            X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
        "    except Exception as e:\n",
        "        print(f\"  Skipping sampling '{sampling_name}' due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "    for model_name, spec in param_spaces.items():\n",
        "        experiment_name = f\"with_sampling_{sampling_name}\"\n",
        "        classifier_name = model_name\n",
        "\n",
        "        # Skip if already evaluated\n",
        "        if not existing_df.empty and (\n",
        "            (existing_df[\"sampling_method\"] == sampling_name)\n",
        "            & (existing_df[\"model\"] == model_name)\n",
        "        ).any():\n",
        "            print(f\"  [{model_name}] Skipping (already in CSV)\")\n",
        "            continue\n",
        "\n",
        "        # Prepare scaled data if necessary\n",
        "        if model_name in scale_sensitive:\n",
        "            scaler = StandardScaler()\n",
        "            X_tr_fit = scaler.fit_transform(X_train_res)\n",
        "            X_te_fit = scaler.transform(X_test)\n",
        "        else:\n",
        "            X_tr_fit, X_te_fit = X_train_res, X_test\n",
        "\n",
        "        try:\n",
        "            # Load the trained model\n",
        "            if model_saver.model_exists(classifier_name, experiment_name):\n",
        "                print(f\"  [{model_name}] Loading trained model for evaluation...\")\n",
        "                rs = model_saver.load_model(classifier_name, experiment_name)\n",
        "\n",
        "                # Evaluate best estimator on test set\n",
        "                print(f\"  Evaluating model [{model_name}]\")\n",
        "                best_est = rs.best_estimator_\n",
        "                res = eval_model(\n",
        "                    best_est,\n",
        "                    X_tr_fit, y_train_res,\n",
        "                    X_te_fit, y_test,\n",
        "                )\n",
        "\n",
        "                sampling_results[sampling_name][model_name] = {\"rs\": rs, \"eval\": res}\n",
        "\n",
        "                # Prepare single row result\n",
        "                row = {\n",
        "                    \"sampling_method\": sampling_name,\n",
        "                    \"model\": model_name,\n",
        "                    \"test_accuracy\": round(res[\"test\"][\"accuracy\"], 2),\n",
        "                    \"test_f1_macro\": round(res[\"test\"][\"f1_macro\"], 2),\n",
        "                    \"best_cv_score\": round(rs.best_score_, 2),\n",
        "                    \"best_parameters\": json.dumps(rs.best_params_),\n",
        "                }\n",
        "\n",
        "                # Add per-class F1 scores\n",
        "                labels = res[\"labels\"]\n",
        "                for lbl, f1 in zip(labels, res[\"test\"][\"f1_per_class\"]):\n",
        "                    row[f\"test_f1_cls_{lbl}\"] = round(float(f1), 2)\n",
        "\n",
        "                # Convert to DataFrame and append immediately\n",
        "                new_df = pd.DataFrame([row])\n",
        "                header = not os.path.exists(out_path)\n",
        "                new_df.to_csv(out_path, mode=\"a\", index=False, header=header)\n",
        "                print(f\"  ✅ Result saved for [{model_name}] ({sampling_name})\")\n",
        "\n",
        "                # Update in-memory record too\n",
        "                existing_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
        "\n",
        "            else:\n",
        "                print(f\"  [{model_name}] No trained model found for {experiment_name}\")\n",
        "                sampling_results[sampling_name][model_name] = None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  [{model_name}] ERROR for {experiment_name}: {e}\")\n",
        "            sampling_results[sampling_name][model_name] = None\n",
        "\n",
        "\n",
        "# Final check summary\n",
        "if not existing_df.empty:\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(\"CURRENT MODEL EVALUATION SUMMARY\")\n",
        "    print(\"=\" * 100)\n",
        "    display(\n",
        "        existing_df[[\"sampling_method\", \"model\", \"test_f1_macro\"]]\n",
        "        .sort_values(by=[\"test_f1_macro\"], ascending=False)\n",
        "        .head(10)\n",
        "    )\n",
        "else:\n",
        "    print(\"No results to display.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EVALUATION PHASE COMPLETED\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. GridSearch - Final run on 3 best models\n",
        "\n",
        "- test with an without outlier removal\n",
        "- GridSearchCV on same parameter spaces\n",
        "- using common train test dataset\n",
        "- RepeatedStratifiedKFold:\n",
        "    - single CV split can be \"lucky\" or \"unlucky\" --> dependency how data is shuffled\n",
        "    - repeating stratified k-fold with different shuffles averages out randomness\n",
        "    - more stable, less noisy estimates of performance\n",
        "- Implement Leak-free scaling\n",
        "    - current: without pipeline: scale once on the full training set, then do CV on the already sclaed data --> scaler \"saw\" all CV folds, including each folds validation part --> data leakage\n",
        "    - makes CV too optimisic?\n",
        "    - pipeline fits the scaler only on each training fold, then applieos it to that folds validation split. \n",
        "    - solution: Pipeline(StandardScaler(), model) so scaling is fit per CV fold\n",
        "- Try to optimize the signal - run best models with the optimized signal and new features\n",
        "    - add RR-Interval as feature\n",
        "    - add new Target \"not_normal\" in MIT to compare to PTB\n",
        "    - baseline wandering removal\n",
        "    - denoising\n",
        "- target models. XGBoost, ANN, SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
