{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5694e-e5a1-4b6e-90a9-8f566d9122b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For manipulating arrays and DataFrames\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For visualizing performance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# For instantiating a Dense layer and sequential model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Conv1D, MaxPooling1D, Flatten, Add, ReLU, LSTM, Reshape, Concatenate\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d1421-cfb0-47d4-a052-5e5fe4930f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import PTB data\n",
    "X_ptb_train = pd.read_csv('data/processed/PTB/X_ptb_train.csv')\n",
    "y_ptb_train = pd.read_csv('data/processed/PTB/y_ptb_train.csv')\n",
    "\n",
    "X_ptb_train_sm = pd.read_csv('data/processed/PTB/X_ptb_train_sm.csv')\n",
    "y_ptb_train_sm = pd.read_csv('data/processed/PTB/y_ptb_train_sm.csv')\n",
    "\n",
    "X_ptb_val = pd.read_csv('data/processed/PTB/X_ptb_val.csv')\n",
    "y_ptb_val = pd.read_csv('data/processed/PTB/y_ptb_val.csv')\n",
    "\n",
    "X_ptb_test = pd.read_csv('data/processed/PTB/X_ptb_test.csv')\n",
    "y_ptb_test = pd.read_csv('data/processed/PTB/y_ptb_test.csv')\n",
    "\n",
    "\n",
    "display(X_ptb_train.shape)\n",
    "display(y_ptb_train.shape)\n",
    "\n",
    "display(X_ptb_train_sm.shape)\n",
    "display(y_ptb_train_sm.shape)\n",
    "\n",
    "display(X_ptb_val.shape)\n",
    "display(y_ptb_val.shape)\n",
    "\n",
    "display(X_ptb_test.shape)\n",
    "display(y_ptb_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Reshape the data for 1D CNN\n",
    "X_ptb_train_cnn = np.expand_dims(X_ptb_train, axis=2)\n",
    "X_ptb_train_sm_cnn = np.expand_dims(X_ptb_train_sm, axis=2)\n",
    "X_ptb_val_cnn = np.expand_dims(X_ptb_val, axis=2)\n",
    "X_ptb_test_cnn = np.expand_dims(X_ptb_test, axis=2)\n",
    "\n",
    "display(X_ptb_train_cnn.shape)\n",
    "display(y_ptb_train.shape)\n",
    "\n",
    "display(X_ptb_train_sm_cnn.shape)\n",
    "display(y_ptb_train_sm.shape)\n",
    "\n",
    "display(X_ptb_val_cnn.shape)\n",
    "display(y_ptb_val.shape)\n",
    "\n",
    "display(X_ptb_test_cnn.shape)\n",
    "display(y_ptb_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0661c546-26cd-4937-9ecc-c879a8c981bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79f0c94-f018-403d-8482-d44575cb6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "model_trained = load_model('best_dl_model/cnn6_sm_lr_bs128_epoch_06_valloss_0.0677.keras')\n",
    "\n",
    "model_trained.summary()\n",
    "\n",
    "\n",
    "# Extract features from the last convolutional block (pool_5)\n",
    "feature_extractor = Model(inputs=model_trained.input, outputs=model_trained.get_layer('max_pooling1d_4').output)\n",
    "\n",
    "\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ffff9b-52a3-4505-920c-cc8629198bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze all convolutional layers in feature_extractor\n",
    "for layer in feature_extractor.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759499f4-4efb-4eb4-8ca8-f9db011f5eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build new classifier for 2 class problem\n",
    "# Input same shape as original\n",
    "input_layer = Input(shape=(187, 1))\n",
    "x = feature_extractor(input_layer, training=False)  # Freeze convolutional base\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "output_layer = Dense(2, activation='softmax')(x)\n",
    "\n",
    "transfer_model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f52285-af6f-4a3b-ad8e-2026c7e486da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate with exponential decay\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.75,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Adam optimizer with specified hyperparameters\n",
    "optimizer = Adam(\n",
    "    learning_rate=lr_schedule,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999\n",
    ")\n",
    "\n",
    "\n",
    "#Compile model\n",
    "transfer_model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# Define where and how to save the best model\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='transfer/cnn6_transfer_lr_bs128_epoch_{epoch:02d}_valloss_{val_loss:.4f}.keras',   # file path (can be .keras or .h5)\n",
    "    monitor='val_loss',        # metric to monitor\n",
    "    mode='min',                    # because higher accuracy is better\n",
    "    save_best_only=True,           # only save when val_accuracy improves\n",
    "    verbose=1                      # print message when a model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889cd13e-2128-4f00-ad0d-692cbab4ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = transfer_model.fit(\n",
    "    X_ptb_train_sm_cnn, \n",
    "    y_ptb_train_sm,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_ptb_val_cnn, y_ptb_val),  \n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc61fa-6e11-4850-868d-f67031000958",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transfer/cnn6_transfer_lr_bs128_epoch_33_valloss_0.0730.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "\n",
    "best_model = load_model('transfer/cnn6_transfer_lr_bs128_epoch_33_valloss_0.0730.keras')\n",
    "\n",
    "test_pred = best_model.predict(X_ptb_test_cnn)\n",
    "y_test_class = y_ptb_test\n",
    "y_pred_class = np.argmax(test_pred, axis=1)\n",
    "\n",
    "\n",
    "print(classification_report(y_test_class, y_pred_class, digits=4))\n",
    "\n",
    "print(pd.crosstab(y_test_class, y_pred_class, colnames=['Predictions']))\n",
    "\n",
    "\n",
    "#save results of metrics\n",
    "with open(\"transfer/cnn6_transfer_lr_bs128_epoch_33_valloss_0.0730.txt\", \"w\") as file:\n",
    "    file.write(\"\\nModel: CNN6 transfer MIT \\n\")\n",
    "    file.write(\"\\nData augmentation: MIT Smote, PTB None\\n\")\n",
    "    file.write(\"\\nConfusion Matrix on test set:\\n\")\n",
    "    file.write(str(pd.crosstab(y_test_class, y_pred_class, colnames=['Predictions'])))\n",
    "    file.write(\"\\n\\nClassification Report on test set:\\n\")\n",
    "    file.write(classification_report(y_test_class, y_pred_class, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66541ef1-a334-4d2a-b72a-7d19af4ec3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history, save_dir=\"transfer\", prefix=\"cnn6_transfer_lr_bs128_epoch_33_valloss_0.0730\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascientest_project",
   "language": "python",
   "name": "datascientest_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
