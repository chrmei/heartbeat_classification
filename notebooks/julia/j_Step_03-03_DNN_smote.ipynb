{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d328f-d220-4722-a362-25e4bd96fb17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run on Google Colab to use GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c93d0b-2da6-494a-be5c-c0b962c399b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Conv1D, MaxPooling1D, Flatten, Add, ReLU, LSTM, Reshape, Concatenate, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3585c9f-5063-4232-9b19-8de0daafe70d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import MIT data\n",
    "df_mitbih_test = pd.read_csv('data/original/mitbih_test.csv', header = None)\n",
    "\n",
    "X_train = pd.read_csv('data/processed/X_train.csv')\n",
    "y_train = pd.read_csv('data/processed/y_train.csv')\n",
    "y_train = y_train['187']\n",
    "\n",
    "X_train_sm = pd.read_csv('data/processed/X_train_sm.csv')\n",
    "y_train_sm = pd.read_csv('data/processed/y_train_sm.csv')\n",
    "y_train_sm = y_train_sm['187']\n",
    "\n",
    "X_val = pd.read_csv('data/processed/X_val.csv')\n",
    "y_val = pd.read_csv('data/processed/y_val.csv')\n",
    "y_val = y_val['187']\n",
    "\n",
    "X_test = df_mitbih_test.drop(187, axis = 1)\n",
    "y_test = df_mitbih_test[187]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59285ef6-c735-471f-a682-ef14b89432c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot and save loss and accuracy over epochs from training history\n",
    "def plot_training_history(history, save_dir, prefix): \n",
    "    hist = history.history\n",
    "    metrics = [m for m in hist.keys() if not m.startswith('val_')]  \n",
    "\n",
    "    # Create the output folder if it does not exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for m in metrics:\n",
    "        plt.figure()\n",
    "        plt.plot(hist[m], label=f'Train {m}')\n",
    "        if f'val_{m}' in hist:\n",
    "            plt.plot(hist[f'val_{m}'], label=f'Val {m}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(m)\n",
    "        plt.title(f'{m} over epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Construct filename with prefix and filepath with directory and filename\n",
    "        filename = f\"{prefix}_{m}.png\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save figure\n",
    "        plt.savefig(filepath, format='png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {filepath}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0663cbd-09fa-4067-af23-875237210c2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Used DNNs\n",
    "\n",
    "#DNN1\n",
    "dnn1 = Sequential()\n",
    "dnn1.add(Dense(units=64, activation=\"relu\", input_shape=(187,)))\n",
    "dnn1.add(Dense(units=32, activation=\"relu\"))\n",
    "dnn1.add(Dense(units=16, activation=\"relu\"))\n",
    "dnn1.add(Dense(units=5, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "#DNN2\n",
    "dnn2 = Sequential([\n",
    "    Dense(64, input_shape=(187,), kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(32, kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(16, kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.1),  \n",
    "    \n",
    "    Dense(5, activation='softmax')  \n",
    "])\n",
    "\n",
    "#DNN3\n",
    "dnn3 = Sequential([\n",
    "    Dense(64, input_shape=(187,)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(32),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(16),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.1),  \n",
    "    \n",
    "    Dense(5, activation='softmax')  \n",
    "])\n",
    "\n",
    "\n",
    "#DNN4\n",
    "dnn4 = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(187,), kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.2), \n",
    "    \n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(16, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Dense(5, activation='softmax')  \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f3101-3dea-4302-9ba0-89798614a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model summary\n",
    "dnn1.summary()\n",
    "\n",
    "dnn2.summary()\n",
    "\n",
    "dnn3.summary()\n",
    "\n",
    "dnn4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529bdb97-73e2-4584-9062-34d6e754d1ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reduce learning rate when loss is on plateau\n",
    "lrredpl = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    factor=0.5,          # Factor by which the learning rate will be reduced\n",
    "    patience=10,         # Number of epochs with no improvement after which learning rate is reduced\n",
    "    min_lr=1e-7,         # Minimum learning rate\n",
    "    verbose=1,           # Print message when the learning rate is reduced\n",
    "    min_delta=0.001      # reduce if improvement < 0.001\n",
    ")\n",
    "\n",
    "\n",
    "#learning rate with exponential decay\n",
    "initial_learning_rate = 5e-4\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96)\n",
    "\n",
    "\n",
    "#Early stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',       \n",
    "    patience=20,               # how many epochs with no improvement before stopping\n",
    "    restore_best_weights=True, \n",
    "    min_delta=0.001            #only stop if improvement < 0.001\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model for lrredpl\n",
    "#dnn1.compile(\n",
    " #   optimizer=Adam(learning_rate=5e-4),\n",
    "  #  loss='sparse_categorical_crossentropy',\n",
    "   # metrics=['accuracy', F1Score(average='macro', name='f1_macro')])\n",
    "\n",
    "\n",
    "#Compile when lr exp decay\n",
    "dnn3.compile(\n",
    "    optimizer=Adam(learning_rate=lr_schedule),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Define where and how to save the best model\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='../DNN_output/dnn3_sm_expdec5e-4_earlystop20_bs512_epoch_{epoch:02d}_valloss_{val_loss:.4f}.keras',   # file path (can be .keras or .h5)\n",
    "    monitor='val_loss',        # metric to monitor\n",
    "    mode='min',                # minimize loss\n",
    "    save_best_only=False,      # save model for every epoch\n",
    "    verbose=1                  # print message when model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4442cb5-66a8-4cdf-93ee-e99b72ebef01",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "history = dnn3.fit(                  # change when using different model architecture\n",
    "    X_train_sm,\n",
    "    y_train_sm,\n",
    "    epochs=500,                      # change when needed\n",
    "    batch_size=512,                  # change when needed\n",
    "    validation_data=(X_val, y_val),  # unaltered validation set\n",
    "    callbacks=[checkpoint, early_stop]          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16bdc4b-c8d1-4e27-949b-cb06f9ad55f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot and save loss and accuracy over epochs from history\n",
    "plot_training_history(history, save_dir=\"../DNN_output\", prefix=\"dnn3_sm_expdec5e-4_earlystop20_bs512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca563f08-e666-43be-b470-e0e9e8a3eb18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Save training history\n",
    "with open(\"../DNN_output/dnn3_sm_expdec5e-4_earlystop20_bs512_epoch_valloss_.pkl\", \"wb\") as f: #change for model\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "    \n",
    "best_model = load_model('../DNN_output/dnn3_sm_expdec5e-4_earlystop20_bs512_epoch_valloss_.keras') #change for model\n",
    "\n",
    "\n",
    "#prediction of test data\n",
    "test_pred = best_model.predict(X_test)\n",
    "y_test_class = y_test\n",
    "y_pred_class = np.argmax(test_pred, axis=1)\n",
    "\n",
    "\n",
    "#classification report\n",
    "print(classification_report(y_test_class, y_pred_class, digits=4))\n",
    "\n",
    "\n",
    "#confusion matrix\n",
    "print(pd.crosstab(y_test_class, y_pred_class, colnames=['Predictions']))\n",
    "\n",
    "\n",
    "#save results of metrics\n",
    "with open(\"../DNN_output/dnn3_sm_expdec5e-4_earlystop20_bs512_epoch_valloss_.txt\", \"w\") as file: #change for model\n",
    "    \n",
    "    file.write(\"\\nModel: DNN1\\n\")#change for model\n",
    "        \n",
    "    file.write(\"\\nData augmentation: Smote\\n\")\n",
    "    \n",
    "    file.write(\"\\nConfusion Matrix on test set:\\n\")\n",
    "    file.write(str(pd.crosstab(y_test_class, y_pred_class, colnames=['Predictions'])))\n",
    "    \n",
    "    file.write(\"\\n\\nClassification Report on test set:\\n\")\n",
    "    file.write(classification_report(y_test_class, y_pred_class, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a51331-35be-439a-8a86-bc0417ef783f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascientest_project",
   "language": "python",
   "name": "datascientest_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
