{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f2b5ec",
   "metadata": {},
   "source": [
    "# CNN Transfer Learning - PTB Dataset\n",
    "\n",
    "Transfer learning from the best-performing CNN8 model trained on MIT-BIH dataset to PTB dataset for binary MI detection.\n",
    "\n",
    "**Approach:**\n",
    "- Load pretrained CNN8 model (trained on MIT-BIH)\n",
    "- Freeze convolutional layers (first 4 residual blocks)\n",
    "- Unfreeze last residual block for fine-tuning\n",
    "- Add new classifier layers adapted for binary classification\n",
    "\n",
    "Results contribute to Tables 9 and 10 in Rendering 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c93d0b-2da6-494a-be5c-c0b962c399b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Conv1D, MaxPooling1D, Flatten, Add, ReLU, LSTM, Reshape, Concatenate, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import re \n",
    "\n",
    "import pickle\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "print(tf.config.list_physical_devices('GPU'))  # should show []\n",
    "from contextlib import redirect_stdout\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "from src.visualization.visualization import plot_training_history \n",
    "from src.visualization.confusion_matrix import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3585c9f-5063-4232-9b19-8de0daafe70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_METHOD = \"SMOTE\"\n",
    "REMOVE_OUTLIERS = False\n",
    "OUTPUT_PATH = \"models/PTB_04_02_dl_models/CNN8_TRANSFER/\"\n",
    "REPORTS_PATH = \"reports/deep_learning/cnn8_transfer/\" \n",
    "results_csv = REPORTS_PATH+\"09_DL_model_optimization.csv\"\n",
    "model_names = [\"cnn8_sm\"]\n",
    "models = {k: {} for k in model_names}\n",
    "\n",
    "#import MIT data\n",
    "X_ptb_train = pd.read_csv('data/processed/ptb/X_ptb_train.csv')\n",
    "y_ptb_train = pd.read_csv('data/processed/ptb/y_ptb_train.csv')\n",
    "\n",
    "X_ptb_train_sm = pd.read_csv('data/processed/ptb/X_ptb_train_sm.csv')\n",
    "y_ptb_train_sm = pd.read_csv('data/processed/ptb/y_ptb_train_sm.csv')\n",
    "\n",
    "X_ptb_val = pd.read_csv('data/processed/ptb/X_ptb_val.csv')\n",
    "y_ptb_val = pd.read_csv('data/processed/ptb/y_ptb_val.csv')\n",
    "\n",
    "X_ptb_test = pd.read_csv('data/processed/ptb/X_ptb_test.csv')\n",
    "y_ptb_test = pd.read_csv('data/processed/ptb/y_ptb_test.csv')\n",
    "\n",
    "display(X_ptb_train.shape)\n",
    "display(y_ptb_train.shape)\n",
    "\n",
    "display(X_ptb_train_sm.shape)\n",
    "display(y_ptb_train_sm.shape)\n",
    "\n",
    "display(X_ptb_val.shape)\n",
    "display(y_ptb_val.shape)\n",
    "\n",
    "display(X_ptb_test.shape)\n",
    "display(y_ptb_test.shape)\n",
    "\n",
    "# Reshape the data for 1D CNN\n",
    "X_ptb_train_cnn = np.expand_dims(X_ptb_train, axis=2)\n",
    "X_ptb_train_sm_cnn = np.expand_dims(X_ptb_train_sm, axis=2)\n",
    "X_ptb_val_cnn = np.expand_dims(X_ptb_val, axis=2)\n",
    "X_ptb_test_cnn = np.expand_dims(X_ptb_test, axis=2)\n",
    "\n",
    "display(X_ptb_train_cnn.shape)\n",
    "display(y_ptb_train.shape)\n",
    "\n",
    "display(X_ptb_train_sm_cnn.shape)\n",
    "display(y_ptb_train_sm.shape)\n",
    "\n",
    "display(X_ptb_val_cnn.shape)\n",
    "display(y_ptb_val.shape)\n",
    "\n",
    "display(X_ptb_test_cnn.shape)\n",
    "display(y_ptb_test.shape)\n",
    "\n",
    "\n",
    "def parse_epoch_from_name(name, default_epochs=512):\n",
    "    # Expect pattern like ..._epoch_12_...; returns int if found else default\n",
    "    m = re.search(r\"epoch_(\\d+)\", name)\n",
    "    return int(m.group(1)) if m else default_epochs\n",
    "\n",
    "def parse_val_loss_from_name(name):\n",
    "    # Expect pattern like ..._valloss_0.1234.keras\n",
    "    m = re.search(r\"valloss_([0-9]+\\.[0-9]+)\", name)\n",
    "    return float(m.group(1)) if m else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f3101-3dea-4302-9ba0-89798614a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_trained = load_model('models/MIT_02_03_dl_models/CNN/cnn8_sm_BS512_best.keras')\n",
    "model_trained.summary()\n",
    "\n",
    "# find last MaxPooling1D layer dynamically\n",
    "last_pool_layer = None\n",
    "for layer in model_trained.layers[::-1]:  \n",
    "    if isinstance(layer, MaxPooling1D):\n",
    "        last_pool_layer = layer\n",
    "        break\n",
    "\n",
    "if last_pool_layer is None:\n",
    "    raise ValueError(\"Model has no MaxPooling1D layer!\")\n",
    "\n",
    "print(\"Using pooling layer:\", last_pool_layer.name)\n",
    "\n",
    "# build feature extractor\n",
    "feature_extractor = Model(\n",
    "    inputs=model_trained.input,\n",
    "    outputs=last_pool_layer.output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78759336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze all convolutional layers in feature_extractor\n",
    "for layer in feature_extractor.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = model_trained.input\n",
    "output = feature_extractor.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer 2, with added dropout\n",
    "x = Flatten()(output)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "output_layer_2 = Dense(2, activation='softmax')(x)\n",
    "transfer_model_2 = Model(inputs=input_layer, outputs=output_layer_2)\n",
    "\n",
    "#transfer 3, with changed dropout\n",
    "x = Flatten()(output)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "output_layer_3 = Dense(2, activation='softmax')(x)\n",
    "transfer_model_3 = Model(inputs=input_layer, outputs=output_layer_3)\n",
    "\n",
    "#transfer 4, with dropout and batch normalization \n",
    "x = Flatten()(output)\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "output_layer_4 = Dense(2, activation='softmax')(x)\n",
    "transfer_model_4 = Model(inputs=input_layer, outputs=output_layer_4)\n",
    "\n",
    "\n",
    "#transfer 5, with changed dropout and batch normalization \n",
    "x = Flatten()(output)\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "output_layer_5 = Dense(2, activation='softmax')(x)\n",
    "transfer_model_5 = Model(inputs=input_layer, outputs=output_layer_5)\n",
    "\n",
    "#transfer 6\n",
    "x = Flatten()(output)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output_layer_6 = Dense(2, activation='softmax')(x)\n",
    "transfer_model_6 = Model(inputs=input_layer, outputs=output_layer_6)\n",
    "\n",
    "#transfer 7 \n",
    "x = Flatten()(output)\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "output_layer_7 = Dense(2, activation='softmax')(x)\n",
    "transfer_model_7 = Model(inputs=input_layer, outputs=output_layer_7)\n",
    "\n",
    "#transfer 8\n",
    "x = Flatten()(output)\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "output_layer_8 = Dense(2, activation='softmax')(x)\n",
    "transfer_model_8 = Model(inputs=input_layer, outputs=output_layer_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c82448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build new classifier for 2 class problem\n",
    "\n",
    "# Input same shape as original\n",
    "input_layer = Input(shape=(187, 1))\n",
    "x = feature_extractor(input_layer, training=False)  # frozen convolutional base\n",
    "\n",
    "#add transfer model here: transfer 6\n",
    "x = Flatten()(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "output_layer_2class = Dense(2, activation='softmax')(x)\n",
    "\n",
    "transfer_model_2class = Model(inputs=input_layer, outputs=output_layer_2class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"transfer_model_2\": transfer_model_2,\n",
    "    \"transfer_model_3\": transfer_model_3,\n",
    "    \"transfer_model_4\": transfer_model_4,\n",
    "    \"transfer_model_5\": transfer_model_5,\n",
    "    \"transfer_model_6\": transfer_model_6,\n",
    "    \"transfer_model_7\": transfer_model_7,\n",
    "    \"transfer_model_8\": transfer_model_8,\n",
    "    \"transfer_model_2class\": transfer_model_2class\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9789e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_metrics_to_csv(metrics: dict, csv_path: str):\n",
    "    \"\"\"\n",
    "    Appends or updates a row in the CSV based on a unique key:\n",
    "    (model_name, batch_size, training_size, lr_start, lr_schedule).\n",
    "\n",
    "    If a matching row exists, it is replaced with the new row.\n",
    "    All other rows remain unchanged.\n",
    "    \"\"\"\n",
    "\n",
    "    df_new = pd.DataFrame([metrics])\n",
    "\n",
    "    # identifier columns for uniqueness\n",
    "    key_cols = [\"model_name\", \"batch_size\", \"training_size\", \"lr_start\", \"lr_schedule\"]\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        df_existing = pd.read_csv(csv_path)\n",
    "\n",
    "        # ensure column alignment\n",
    "        all_cols = sorted(set(df_existing.columns).union(df_new.columns))\n",
    "        df_existing = df_existing.reindex(columns=all_cols)\n",
    "        df_new = df_new.reindex(columns=all_cols)\n",
    "\n",
    "        # remove row(s) with matching key\n",
    "        mask_match = np.ones(len(df_existing), dtype=bool)\n",
    "        for col in key_cols:\n",
    "            mask_match &= (df_existing[col] == df_new.iloc[0][col])\n",
    "\n",
    "        df_existing = df_existing[~mask_match]  # drop matching row(s)\n",
    "\n",
    "        # append new row\n",
    "        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "\n",
    "        # save\n",
    "        df_combined.to_csv(csv_path, index=False)\n",
    "\n",
    "    else:\n",
    "        # create new CSV\n",
    "        df_new.to_csv(csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca563f08-e666-43be-b470-e0e9e8a3eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    print(\"*\"*80)\n",
    "    print(\"*\"*5,'\\t',model_name,\"*\"*5)\n",
    "    print(\"*\"*80)\n",
    "\n",
    "    BATCH_SIZE = 512\n",
    "    EPOCHS = 500\n",
    "\n",
    "    initial_learning_rate = 1e-3\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.96)\n",
    "    \n",
    "    #Early stopping\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',        # what to monitor \n",
    "        patience=20,               # how many epochs with no improvement before stopping\n",
    "        restore_best_weights=True, \n",
    "        min_delta=0.001            #only stop if improvement < 0.001\n",
    "    )\n",
    "\n",
    "    #Compile when lr exp decay\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr_schedule),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    #Define where and how to save the best model, note lr and bs\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=f'{OUTPUT_PATH}{model_name}_BS{BATCH_SIZE}_best.keras',\n",
    "        monitor='val_loss',            # metric to monitor\n",
    "        mode='min',                    # minimize loss\n",
    "        save_best_only=True,          \n",
    "        verbose=1                      # print message when a model is saved\n",
    "    )\n",
    "\n",
    "    #Training\n",
    "    history = model.fit(                      \n",
    "        X_ptb_train_sm_cnn, #PTB data\n",
    "        y_ptb_train_sm, #PTB data\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_ptb_val_cnn, y_ptb_val), #PTB data\n",
    "        callbacks=[checkpoint, early_stop] \n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1) Basic training info\n",
    "    # ----------------------------\n",
    "    hist = history.history\n",
    "    n_epochs = len(hist[\"loss\"])\n",
    "\n",
    "    # best epoch according to val_loss\n",
    "    best_epoch = int(np.argmin(hist[\"val_loss\"]))\n",
    "    last_epoch = n_epochs - 1\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2) Predictions for test\n",
    "    # ----------------------------\n",
    "\n",
    "    # !!! make sure to set restore_best_weights=True in early stopping\n",
    "    # or load best model before predicting \n",
    "    y_prob = model.predict(X_ptb_test)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3) F1-macro and per-class F1 on TEST\n",
    "    # ----------------------------\n",
    "    f1_macro = f1_score(y_ptb_test, y_pred, average=\"macro\")\n",
    "    f1_per_class = f1_score(y_ptb_test, y_pred, average=None)  # array (0–4)\n",
    "    acc = accuracy_score(y_ptb_test, y_pred)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4) Build output dictionary\n",
    "    # ----------------------------\n",
    "    # flatten F1-per-class into separate columns\n",
    "    f1_class_columns = {\n",
    "        f\"test_f1_class_{i}\": float(score) \n",
    "        for i, score in enumerate(f1_per_class)\n",
    "    }\n",
    "\n",
    "    metrics = {\n",
    "        \"model_name\": model_name,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"training_size\": X_ptb_train_sm_cnn.shape[0],\n",
    "        \"lr_start\": initial_learning_rate,\n",
    "        \"lr_schedule\": 'EXP_DECAY',\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"last_epoch\": last_epoch,\n",
    "        \n",
    "        # best epoch values (from history)\n",
    "        \"train_loss_best\": float(hist[\"loss\"][best_epoch]),\n",
    "        \"val_loss_best\": float(hist[\"val_loss\"][best_epoch]),\n",
    "        \"train_acc_best\": float(hist[\"accuracy\"][best_epoch]),\n",
    "        \"val_acc_best\": float(hist[\"val_accuracy\"][best_epoch]),\n",
    "\n",
    "        # last epoch values\n",
    "        \"train_loss_last\": float(hist[\"loss\"][last_epoch]),\n",
    "        \"val_loss_last\": float(hist[\"val_loss\"][last_epoch]),\n",
    "        \"train_acc_last\": float(hist[\"accuracy\"][last_epoch]),\n",
    "        \"val_acc_last\": float(hist[\"val_accuracy\"][last_epoch]),\n",
    "\n",
    "        # TEST metrics\n",
    "        \"test_f1_macro\": float(f1_macro),\n",
    "        \"test_accuracy\": float(acc)\n",
    "    }\n",
    "\n",
    "    # merge F1-per-class columns\n",
    "    metrics.update(f1_class_columns)\n",
    "\n",
    "\n",
    "    append_metrics_to_csv(metrics, csv_path=results_csv)\n",
    "\n",
    "\n",
    "    with open(f\"{OUTPUT_PATH}{model_name}_BS{BATCH_SIZE}_{initial_learning_rate}_full.pkl\", \"wb\") as f: #change for model\n",
    "        pickle.dump(history.history, f)\n",
    "\n",
    "    fig_cm, ax_cm = plot_confusion_matrix(\n",
    "        y_true=y_ptb_test,\n",
    "        y_pred=y_pred,\n",
    "        normalize=True,\n",
    "        class_names=[\"1\",\"2\"],\n",
    "        title=f\"Confusion Matrix — {model_name}\"\n",
    "    )\n",
    "\n",
    "\n",
    "    fig_cm.savefig(\n",
    "        f\"{REPORTS_PATH}/{model_name}_BS{BATCH_SIZE}_{initial_learning_rate}_confusion_matrix.png\",\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.close(fig_cm)\n",
    "\n",
    "    plot_training_history(\n",
    "        history=history,                     # raw history\n",
    "        save_dir=REPORTS_PATH,               # where plots go\n",
    "        prefix=f\"{model_name}_BS{BATCH_SIZE}_{initial_learning_rate}_training_history\"  # prefix for filenames\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
