{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14eab3c2",
   "metadata": {},
   "source": [
    "# 4.3 Refit best and transfer test on PTB dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64239f57",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "686a1cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/christianm/Projects/Repos/heartbeat_classification\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# Add src to path\n",
    "print(os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    ")\n",
    "from sklearn import set_config\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Sampling\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Custom utilities\n",
    "from src.utils.preprocessing import (\n",
    "    load_processed_dataset,\n",
    "    DatasetSplit,\n",
    "    build_full_suffix as pp_build_full_suffix,\n",
    "    generate_all_processed_datasets,\n",
    "    _normalize_sampling_method_name,\n",
    "    _SAMPLING_REGISTRY\n",
    ")\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from src.utils.evaluation import eval_model\n",
    "from src.utils.model_saver import create_model_saver\n",
    "\n",
    "import numpy as np\n",
    "from typing import Optional, Dict, Union, Tuple\n",
    "from scipy.signal import butter, filtfilt, medfilt\n",
    "import pywt\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "from src.utils.model_saver import create_model_saver\n",
    "from src.utils.preprocessing import (\n",
    "    load_processed_dataset,\n",
    "    build_full_suffix as pp_build_full_suffix,   # if not exported, import as in your notebook (pp_build_full_suffix alias)\n",
    "    generate_all_processed_datasets,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ArrayLike = Union[np.ndarray, list]\n",
    "\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = \"data/processed/mitbih\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3a6a8",
   "metadata": {},
   "source": [
    "## 2. Refit best Model from part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fitted MIT-trained XGBoost on the entire PTB dataset (train + val)\n",
    "from src.utils.preprocessing import prepare_ptbdb\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Evaluating MIT-trained 5-class XGBoost mapped to binary on full PTB (train+val)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load PTB split (both train and validation)\n",
    "ptb_split_full = prepare_ptbdb(data_dir=\"data/original\", random_state=42)\n",
    "X_ptb_full = np.vstack([ptb_split_full.X_train.values, ptb_split_full.X_val.values])\n",
    "y_ptb_full = np.concatenate([ptb_split_full.y_train.values, ptb_split_full.y_val.values])\n",
    "\n",
    "# Predict probabilities for 5 classes and map to binary (0 -> 0, 1/2/3/4 -> 1)\n",
    "ptb_proba_5_full = estimator.predict_proba(X_ptb_full)\n",
    "y_pred_ptb_5_full = np.argmax(ptb_proba_5_full, axis=1)\n",
    "y_pred_ptb_bin_full = (y_pred_ptb_5_full != 0).astype(int)\n",
    "\n",
    "# Metrics\n",
    "acc_full = accuracy_score(y_ptb_full, y_pred_ptb_bin_full)\n",
    "prec_macro_full, rec_macro_full, f1_macro_full, _ = precision_recall_fscore_support(\n",
    "    y_ptb_full, y_pred_ptb_bin_full, average=\"macro\", zero_division=0\n",
    ")\n",
    "cm_full = confusion_matrix(y_ptb_full, y_pred_ptb_bin_full, labels=[0, 1])\n",
    "\n",
    "# Distributions\n",
    "unique_full, counts_full = np.unique(y_pred_ptb_5_full, return_counts=True)\n",
    "true_dist = dict(zip(*np.unique(y_ptb_full, return_counts=True)))\n",
    "\n",
    "print(\"PTB (full) predicted 5-class distribution:\", dict(zip(unique_full, counts_full)))\n",
    "print(\"PTB (full) true binary distribution:\", true_dist)\n",
    "print(f\"PTB (full, mapped) Accuracy: {acc_full:.4f}\")\n",
    "print(f\"PTB (full, mapped) F1-Macro: {f1_macro_full:.4f}\")\n",
    "print(\"PTB (full, mapped) Confusion Matrix (rows=true, cols=pred):\\n\", cm_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ae363",
   "metadata": {},
   "source": [
    "### XGBoost, SMOTE, no outlier removal,\tno feature engineering\n",
    "\n",
    "Accuracy:   0.9834\n",
    "\n",
    "F1-Macro:   0.9148\n",
    "\n",
    "CV-Score:   0.9134\n",
    "\n",
    "\n",
    "F1 Class1:  0.99\n",
    "\n",
    "F1 Class2:  0.83\n",
    "\n",
    "F1 Class3:  0.96\n",
    "\n",
    "F1 Class4:  0.81\n",
    "\n",
    "F1 Class5:  0.99\n",
    "\n",
    "params: {\"classifier__colsample_bytree\": 0.9,\"classifier__gamma\": 0.0,\"classifier__learning_rate\": 0.2,\"classifier__max_depth\": 9,\"classifier__min_child_weight\": 5,\"classifier__n_estimators\": 250,\"classifier__reg_alpha\": 0.2,\"classifier__reg_lambda\": 0.05,\"classifier__subsample\": 0.7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99ec4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"classifier__colsample_bytree\": 0.9,\"classifier__gamma\": 0.0,\n",
    "               \"classifier__learning_rate\": 0.2,\"classifier__max_depth\": 9,\n",
    "               \"classifier__min_child_weight\": 5,\"classifier__n_estimators\": 250,\n",
    "               \"classifier__reg_alpha\": 0.2,\"classifier__reg_lambda\": 0.05,\n",
    "               \"classifier__subsample\": 0.7}\n",
    "sampling_method = \"SMOTE\"\n",
    "model_name = \"XGBoost\"\n",
    "remove_outliers = False\n",
    "estimator = \"XGBoost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f465817f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Refitting XGBoost with best parameters and transferring to PTB (5-class as-is)\n",
      "Sampling: SMOTE | Outlier removal: False\n",
      "================================================================================\n",
      "Loading processed X_train dataset from: data/processed/mitbih/X_train.csv\n",
      "Loading processed y_train dataset from: data/processed/mitbih/y_train.csv\n",
      "\n",
      "MIT-BIH Validation Metrics:\n",
      "Accuracy: 0.9834\n",
      "F1-Macro: 0.9148\n",
      "Precision-Macro: 0.9190\n",
      "Recall-Macro: 0.9110\n",
      "\n",
      "Per-class F1 scores:\n",
      "  Class 0.0: 0.9913\n",
      "  Class 1.0: 0.8328\n",
      "  Class 2.0: 0.9580\n",
      "  Class 3.0: 0.8062\n",
      "  Class 4.0: 0.9860\n",
      "\n",
      "MIT-BIH Confusion Matrix (rows=true, cols=pred):\n",
      "[[14385    58    25    16    10]\n",
      " [   79   361     4     0     1]\n",
      " [   37     2  1106     9     4]\n",
      " [   12     0    12   104     0]\n",
      " [   15     1     4     1  1265]]\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset_with_sampling(\n",
    "    data_dir: str = DATA_DIR,\n",
    "    sampling_method: str = \"No_Sampling\",\n",
    "    remove_outliers: bool = False\n",
    ") -> Tuple[np.ndarray, Optional[np.ndarray], np.ndarray, Optional[np.ndarray]]:\n",
    "    \"\"\"Load an existing processed dataset for the given configuration.\n",
    "\n",
    "    Datasets are assumed to be pre-generated by preprocessing utilities. This\n",
    "    function never overwrites or generates new data; it only loads.\n",
    "    \"\"\"\n",
    "    # Ensure all datasets are generated once (no-op if already done)\n",
    "    generate_all_processed_datasets(data_dir=data_dir, only_once=True)\n",
    "\n",
    "    full_suffix = pp_build_full_suffix(sampling_method, remove_outliers)\n",
    "    split = load_processed_dataset(data_dir=data_dir, sampling_suffix=full_suffix)\n",
    "\n",
    "    X_train_res = split.X_train.values\n",
    "    y_train_res = split.y_train.values\n",
    "    X_val = split.X_val.values if split.X_val is not None else None\n",
    "    y_val = split.y_val.values if split.y_val is not None else None\n",
    "\n",
    "    return X_train_res, X_val, y_train_res, y_val\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Refitting {model_name} with best parameters and transferring to PTB (5-class as-is)\")\n",
    "print(f\"Sampling: {sampling_method} | Outlier removal: {remove_outliers}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# 1) Train multi-class on MIT-BIH (as-is, 5 classes)\n",
    "X_train, X_val, y_train, y_val = prepare_dataset_with_sampling(\n",
    "    sampling_method=\"No_Sampling\",\n",
    "    remove_outliers=remove_outliers\n",
    ")\n",
    "\n",
    "best_params_clean = {k.replace(\"classifier__\", \"\"): v for k, v in params.items()}\n",
    "pipe = Pipeline([\n",
    "    (\"sampler\", SMOTE(random_state=42)),\n",
    "    (\"classifier\", xgb.XGBClassifier(\n",
    "        **best_params_clean,\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=5,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        eval_metric=\"mlogloss\",\n",
    "    )),\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "estimator = pipe  # so the rest of the code works unchanged\n",
    "\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on MIT validation set first\n",
    "y_pred_mit = estimator.predict(X_val)\n",
    "mit_accuracy = accuracy_score(y_val, y_pred_mit)\n",
    "mit_precision_macro, mit_recall_macro, mit_f1_macro, _ = precision_recall_fscore_support(\n",
    "    y_val, y_pred_mit, average='macro', zero_division=0\n",
    ")\n",
    "mit_labels = np.unique(np.concatenate([y_train, y_val]))\n",
    "mit_precision_per_class, mit_recall_per_class, mit_f1_per_class, _ = precision_recall_fscore_support(\n",
    "    y_val, y_pred_mit, average=None, labels=mit_labels, zero_division=0\n",
    ")\n",
    "mit_confusion = confusion_matrix(y_val, y_pred_mit, labels=mit_labels)\n",
    "\n",
    "print(f\"\\nMIT-BIH Validation Metrics:\")\n",
    "print(f\"Accuracy: {mit_accuracy:.4f}\")\n",
    "print(f\"F1-Macro: {mit_f1_macro:.4f}\")\n",
    "print(f\"Precision-Macro: {mit_precision_macro:.4f}\")\n",
    "print(f\"Recall-Macro: {mit_recall_macro:.4f}\")\n",
    "print(f\"\\nPer-class F1 scores:\")\n",
    "for lbl, f1 in zip(mit_labels, mit_f1_per_class):\n",
    "    print(f\"  Class {lbl}: {f1:.4f}\")\n",
    "print(f\"\\nMIT-BIH Confusion Matrix (rows=true, cols=pred):\\n{mit_confusion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dede1691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Evaluating MIT-trained 5-class XGBoost mapped to binary on full PTB (train+val)\n",
      "================================================================================\n",
      "PTB (full) predicted 5-class distribution: {np.int64(0): np.int64(13150), np.int64(1): np.int64(449), np.int64(2): np.int64(430), np.int64(3): np.int64(13), np.int64(4): np.int64(503)}\n",
      "PTB (full) true binary distribution: {np.int64(0): np.int64(4045), np.int64(1): np.int64(10500)}\n",
      "PTB (full, mapped) Accuracy: 0.3488\n",
      "PTB (full, mapped) F1-Macro: 0.3265\n",
      "PTB (full, mapped) Confusion Matrix (rows=true, cols=pred):\n",
      " [[3862  183]\n",
      " [9288 1212]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Evaluating MIT-trained 5-class XGBoost mapped to binary on full PTB (train+val)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load PTB split (both train and validation)\n",
    "ptb_split_full = prepare_ptbdb(data_dir=\"data/original\", random_state=42)\n",
    "X_ptb_full = np.vstack([ptb_split_full.X_train.values, ptb_split_full.X_val.values])\n",
    "y_ptb_full = np.concatenate([ptb_split_full.y_train.values, ptb_split_full.y_val.values])\n",
    "\n",
    "# Predict probabilities for 5 classes and map to binary (0 -> 0, 1/2/3/4 -> 1)\n",
    "ptb_proba_5_full = estimator.predict_proba(X_ptb_full)\n",
    "y_pred_ptb_5_full = np.argmax(ptb_proba_5_full, axis=1)\n",
    "y_pred_ptb_bin_full = (y_pred_ptb_5_full != 0).astype(int)\n",
    "\n",
    "# Metrics\n",
    "acc_full = accuracy_score(y_ptb_full, y_pred_ptb_bin_full)\n",
    "prec_macro_full, rec_macro_full, f1_macro_full, _ = precision_recall_fscore_support(\n",
    "    y_ptb_full, y_pred_ptb_bin_full, average=\"macro\", zero_division=0\n",
    ")\n",
    "cm_full = confusion_matrix(y_ptb_full, y_pred_ptb_bin_full, labels=[0, 1])\n",
    "\n",
    "# Distributions\n",
    "unique_full, counts_full = np.unique(y_pred_ptb_5_full, return_counts=True)\n",
    "true_dist = dict(zip(*np.unique(y_ptb_full, return_counts=True)))\n",
    "\n",
    "print(\"PTB (full) predicted 5-class distribution:\", dict(zip(unique_full, counts_full)))\n",
    "print(\"PTB (full) true binary distribution:\", true_dist)\n",
    "print(f\"PTB (full, mapped) Accuracy: {acc_full:.4f}\")\n",
    "print(f\"PTB (full, mapped) F1-Macro: {f1_macro_full:.4f}\")\n",
    "print(\"PTB (full, mapped) Confusion Matrix (rows=true, cols=pred):\\n\", cm_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0acce4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Refitting XGBoost with best parameters and evaluating on PTB (binary)\n",
      "Sampling: SMOTE | Outlier removal: False\n",
      "================================================================================\n",
      "\n",
      "PTB Validation Metrics (binary):\n",
      "Accuracy: 0.9777\n",
      "F1-Macro: 0.9722\n",
      "Precision-Macro: 0.9723\n",
      "Recall-Macro: 0.9720\n",
      "\n",
      "Per-class F1 scores:\n",
      "  Class 0: 0.9598\n",
      "  Class 1: 0.9845\n",
      "\n",
      "PTB Confusion Matrix (rows=true, cols=pred):\n",
      "[[ 776   33]\n",
      " [  32 2068]]\n"
     ]
    }
   ],
   "source": [
    "# Refit XGBoost on PTB (binary) with SMOTE using the same params, then evaluate on PTB val\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Refitting XGBoost with best parameters and evaluating on PTB (binary)\")\n",
    "print(f\"Sampling: SMOTE | Outlier removal: {False}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1) Prepare PTB split (binary)\n",
    "ptb_split = prepare_ptbdb(data_dir=\"data/original\", random_state=42)\n",
    "X_train_ptb = ptb_split.X_train.values\n",
    "y_train_ptb = ptb_split.y_train.values.astype(int)\n",
    "X_val_ptb = ptb_split.X_val.values\n",
    "y_val_ptb = ptb_split.y_val.values.astype(int)\n",
    "\n",
    "# 2) Clean params from GridSearchCV (strip 'classifier__' prefix)\n",
    "best_params_clean_ptb = {k.replace(\"classifier__\", \"\"): v for k, v in params.items()}\n",
    "\n",
    "# 3) Build pipeline: SMOTE (train only) + XGBClassifier (binary)\n",
    "pipe_ptb = Pipeline([\n",
    "    (\"sampler\", SMOTE(random_state=42)),\n",
    "    (\"classifier\", xgb.XGBClassifier(\n",
    "        **best_params_clean_ptb,\n",
    "        objective=\"binary:logistic\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "    )),\n",
    "])\n",
    "\n",
    "# 4) Fit on PTB train\n",
    "pipe_ptb.fit(X_train_ptb, y_train_ptb)\n",
    "\n",
    "# 5) Evaluate on PTB val\n",
    "y_pred_ptb = pipe_ptb.predict(X_val_ptb)\n",
    "\n",
    "acc_ptb = accuracy_score(y_val_ptb, y_pred_ptb)\n",
    "prec_macro_ptb, rec_macro_ptb, f1_macro_ptb, _ = precision_recall_fscore_support(\n",
    "    y_val_ptb, y_pred_ptb, average=\"macro\", zero_division=0\n",
    ")\n",
    "labels_ptb = np.array([0, 1])\n",
    "prec_per_class_ptb, rec_per_class_ptb, f1_per_class_ptb, _ = precision_recall_fscore_support(\n",
    "    y_val_ptb, y_pred_ptb, average=None, labels=labels_ptb, zero_division=0\n",
    ")\n",
    "cm_ptb = confusion_matrix(y_val_ptb, y_pred_ptb, labels=labels_ptb)\n",
    "\n",
    "print(\"\\nPTB Validation Metrics (binary):\")\n",
    "print(f\"Accuracy: {acc_ptb:.4f}\")\n",
    "print(f\"F1-Macro: {f1_macro_ptb:.4f}\")\n",
    "print(f\"Precision-Macro: {prec_macro_ptb:.4f}\")\n",
    "print(f\"Recall-Macro: {rec_macro_ptb:.4f}\")\n",
    "\n",
    "print(\"\\nPer-class F1 scores:\")\n",
    "for lbl, f1 in zip(labels_ptb, f1_per_class_ptb):\n",
    "    print(f\"  Class {lbl}: {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nPTB Confusion Matrix (rows=true, cols=pred):\\n{cm_ptb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a95b44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Transfer Learning: Evaluating PTB-trained binary XGBoost on MIT-BIH dataset\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluation 1: All abnormal classes (1-4) mapped to binary class 1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MIT true binary distribution (all abnormal): {np.int64(0): np.int64(14494), np.int64(1): np.int64(3017)}\n",
      "PTB model predictions distribution: {np.int64(0): np.int64(2186), np.int64(1): np.int64(15325)}\n",
      "\n",
      "Accuracy: 0.2850\n",
      "F1-Macro: 0.2834\n",
      "Precision-Macro: 0.5707\n",
      "Recall-Macro: 0.5542\n",
      "\n",
      "Per-class F1 scores:\n",
      "  Class 0 (Normal): 0.2494\n",
      "  Class 1 (Abnormal (all 1-4)): 0.3174\n",
      "\n",
      "Confusion Matrix (rows=true, cols=pred):\n",
      "[[ 2080 12414]\n",
      " [  106  2911]]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluation 2: Only class 1 mapped to binary class 1 (classes 2,3,4 dropped)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MIT samples: 17511 total, 2572 dropped (classes 2,3,4), 14939 kept\n",
      "MIT true binary distribution (class 1 only): {np.int64(0): np.int64(14494), np.int64(1): np.int64(445)}\n",
      "PTB model predictions distribution: {np.int64(0): np.int64(2167), np.int64(1): np.int64(12772)}\n",
      "\n",
      "Accuracy: 0.1632\n",
      "F1-Macro: 0.1519\n",
      "Precision-Macro: 0.4939\n",
      "Recall-Macro: 0.4740\n",
      "\n",
      "Per-class F1 scores:\n",
      "  Class 0 (Normal): 0.2497\n",
      "  Class 1 (Abnormal (class 1 only)): 0.0542\n",
      "\n",
      "Confusion Matrix (rows=true, cols=pred):\n",
      "[[ 2080 12414]\n",
      " [   87   358]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate PTB-trained binary model on MIT dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Transfer Learning: Evaluating PTB-trained binary XGBoost on MIT-BIH dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use MIT validation set (already loaded from previous cells)\n",
    "# X_val and y_val are from MIT-BIH\n",
    "\n",
    "# Get binary predictions from PTB model on MIT data\n",
    "y_pred_mit_binary = pipe_ptb.predict(X_val)\n",
    "\n",
    "# ============================================================================\n",
    "# Evaluation 1: Multi-class abnormal mapping\n",
    "# Map MIT classes: 0→0 (normal), {1,2,3,4}→1 (all abnormal)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Evaluation 1: All abnormal classes (1-4) mapped to binary class 1\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "y_true_mit_multi_binary = (y_val != 0).astype(int)  # 0 stays 0, all others become 1\n",
    "\n",
    "acc_1 = accuracy_score(y_true_mit_multi_binary, y_pred_mit_binary)\n",
    "prec_macro_1, rec_macro_1, f1_macro_1, _ = precision_recall_fscore_support(\n",
    "    y_true_mit_multi_binary, y_pred_mit_binary, average=\"macro\", zero_division=0\n",
    ")\n",
    "prec_per_class_1, rec_per_class_1, f1_per_class_1, _ = precision_recall_fscore_support(\n",
    "    y_true_mit_multi_binary, y_pred_mit_binary, average=None, labels=[0, 1], zero_division=0\n",
    ")\n",
    "cm_1 = confusion_matrix(y_true_mit_multi_binary, y_pred_mit_binary, labels=[0, 1])\n",
    "\n",
    "# Distribution info\n",
    "true_dist_multi = dict(zip(*np.unique(y_true_mit_multi_binary, return_counts=True)))\n",
    "pred_dist_multi = dict(zip(*np.unique(y_pred_mit_binary, return_counts=True)))\n",
    "\n",
    "print(f\"\\nMIT true binary distribution (all abnormal): {true_dist_multi}\")\n",
    "print(f\"PTB model predictions distribution: {pred_dist_multi}\")\n",
    "print(f\"\\nAccuracy: {acc_1:.4f}\")\n",
    "print(f\"F1-Macro: {f1_macro_1:.4f}\")\n",
    "print(f\"Precision-Macro: {prec_macro_1:.4f}\")\n",
    "print(f\"Recall-Macro: {rec_macro_1:.4f}\")\n",
    "print(f\"\\nPer-class F1 scores:\")\n",
    "for lbl, f1 in zip([0, 1], f1_per_class_1):\n",
    "    class_name = \"Normal\" if lbl == 0 else \"Abnormal (all 1-4)\"\n",
    "    print(f\"  Class {lbl} ({class_name}): {f1:.4f}\")\n",
    "print(f\"\\nConfusion Matrix (rows=true, cols=pred):\\n{cm_1}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Evaluation 2: Single-class abnormal mapping\n",
    "# Map MIT classes: 0→0 (normal), 1→1 (abnormal), drop classes {2,3,4}\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Evaluation 2: Only class 1 mapped to binary class 1 (classes 2,3,4 dropped)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create mask: keep only MIT classes 0 and 1\n",
    "keep_mask = (y_val == 0) | (y_val == 1)\n",
    "y_val_filtered = y_val[keep_mask]\n",
    "X_val_filtered = X_val[keep_mask]\n",
    "y_pred_mit_binary_filtered = pipe_ptb.predict(X_val_filtered)\n",
    "\n",
    "# Map to binary: 0→0, 1→1 (already binary since we filtered)\n",
    "y_true_mit_single_binary = y_val_filtered.astype(int)\n",
    "\n",
    "acc_2 = accuracy_score(y_true_mit_single_binary, y_pred_mit_binary_filtered)\n",
    "prec_macro_2, rec_macro_2, f1_macro_2, _ = precision_recall_fscore_support(\n",
    "    y_true_mit_single_binary, y_pred_mit_binary_filtered, average=\"macro\", zero_division=0\n",
    ")\n",
    "prec_per_class_2, rec_per_class_2, f1_per_class_2, _ = precision_recall_fscore_support(\n",
    "    y_true_mit_single_binary, y_pred_mit_binary_filtered, average=None, labels=[0, 1], zero_division=0\n",
    ")\n",
    "cm_2 = confusion_matrix(y_true_mit_single_binary, y_pred_mit_binary_filtered, labels=[0, 1])\n",
    "\n",
    "# Distribution info\n",
    "true_dist_single = dict(zip(*np.unique(y_true_mit_single_binary, return_counts=True)))\n",
    "pred_dist_single = dict(zip(*np.unique(y_pred_mit_binary_filtered, return_counts=True)))\n",
    "dropped_count = len(y_val) - len(y_val_filtered)\n",
    "\n",
    "print(f\"\\nMIT samples: {len(y_val)} total, {dropped_count} dropped (classes 2,3,4), {len(y_val_filtered)} kept\")\n",
    "print(f\"MIT true binary distribution (class 1 only): {true_dist_single}\")\n",
    "print(f\"PTB model predictions distribution: {pred_dist_single}\")\n",
    "print(f\"\\nAccuracy: {acc_2:.4f}\")\n",
    "print(f\"F1-Macro: {f1_macro_2:.4f}\")\n",
    "print(f\"Precision-Macro: {prec_macro_2:.4f}\")\n",
    "print(f\"Recall-Macro: {rec_macro_2:.4f}\")\n",
    "print(f\"\\nPer-class F1 scores:\")\n",
    "for lbl, f1 in zip([0, 1], f1_per_class_2):\n",
    "    class_name = \"Normal\" if lbl == 0 else \"Abnormal (class 1 only)\"\n",
    "    print(f\"  Class {lbl} ({class_name}): {f1:.4f}\")\n",
    "print(f\"\\nConfusion Matrix (rows=true, cols=pred):\\n{cm_2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
