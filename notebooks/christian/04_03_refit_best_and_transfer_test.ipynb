{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14eab3c2",
   "metadata": {},
   "source": [
    "# 4.3 Refit best and transfer test on PTB dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64239f57",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "686a1cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/christianm/Projects/Repos/heartbeat_classification\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix,\n",
    ")\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Show working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Ensure `src` is importable when running from the notebook\n",
    "sys.path.append(str((Path.cwd() / \"src\").resolve()))\n",
    "\n",
    "# Custom utilities\n",
    "from src.utils.preprocessing import (\n",
    "    load_processed_dataset,\n",
    "    build_full_suffix as pp_build_full_suffix,\n",
    "    generate_all_processed_datasets,\n",
    ")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3a6a8",
   "metadata": {},
   "source": [
    "## 2. Refit best Model from part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ec4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"classifier__colsample_bytree\": 0.9, \"classifier__gamma\": 0.0,\n",
    "               \"classifier__learning_rate\": 0.2, \"classifier__max_depth\": 9,\n",
    "               \"classifier__min_child_weight\": 5, \"classifier__n_estimators\": 250,\n",
    "               \"classifier__reg_alpha\": 0.2, \"classifier__reg_lambda\": 0.05,\n",
    "               \"classifier__subsample\": 0.7}\n",
    "\n",
    "# Preprocessed data sampling (loader) vs. model-time sampler\n",
    "DATA_DIR = \"data/processed/mitbih\"\n",
    "sampling_method = \"No_Sampling\"   # dataset loader suffix\n",
    "model_sampler = \"SMOTE\"           # sampler applied inside the pipeline\n",
    "\n",
    "model_name = \"XGBoost\"\n",
    "remove_outliers = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ae363",
   "metadata": {},
   "source": [
    "### XGBoost, SMOTE, no outlier removal,\tno feature engineering\n",
    "\n",
    "### Model Results\n",
    "\n",
    "- **Accuracy**: 0.9834  \n",
    "- **F1-Macro**: 0.9148  \n",
    "- **CV-Score**: 0.9134  \n",
    "\n",
    "### Per-class F1\n",
    "\n",
    "| Class | F1 |\n",
    "|---|---|\n",
    "| 1 | 0.99 |\n",
    "| 2 | 0.83 |\n",
    "| 3 | 0.96 |\n",
    "| 4 | 0.81 |\n",
    "| 5 | 0.99 |\n",
    "\n",
    "### Best Parameters\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"classifier__colsample_bytree\": 0.9,\n",
    "  \"classifier__gamma\": 0.0,\n",
    "  \"classifier__learning_rate\": 0.2,\n",
    "  \"classifier__max_depth\": 9,\n",
    "  \"classifier__min_child_weight\": 5,\n",
    "  \"classifier__n_estimators\": 250,\n",
    "  \"classifier__reg_alpha\": 0.2,\n",
    "  \"classifier__reg_lambda\": 0.05,\n",
    "  \"classifier__subsample\": 0.7\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f465817f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Refitting XGBoost with best parameters and transferring to PTB (5-class as-is)\n",
      "Data sampling (loader): No_Sampling | Model sampler: SMOTE | Outlier removal: False\n",
      "================================================================================\n",
      "Loading processed X_train dataset from: data/processed/mitbih/X_train.csv\n",
      "Loading processed y_train dataset from: data/processed/mitbih/y_train.csv\n",
      "\n",
      "MIT-BIH Validation Metrics:\n",
      "Accuracy: 0.9834\n",
      "F1-Macro: 0.9148\n",
      "Precision-Macro: 0.9190\n",
      "Recall-Macro: 0.9110\n",
      "\n",
      "Per-class F1 scores:\n",
      "  Class 0: 0.9913\n",
      "  Class 1: 0.8328\n",
      "  Class 2: 0.9580\n",
      "  Class 3: 0.8062\n",
      "  Class 4: 0.9860\n",
      "\n",
      "MIT-BIH Confusion Matrix (rows=true, cols=pred):\n",
      "[[14385    58    25    16    10]\n",
      " [   79   361     4     0     1]\n",
      " [   37     2  1106     9     4]\n",
      " [   12     0    12   104     0]\n",
      " [   15     1     4     1  1265]]\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset_with_sampling(\n",
    "    data_dir: str = DATA_DIR,\n",
    "    sampling_method: str = \"No_Sampling\",\n",
    "    remove_outliers: bool = False\n",
    ") -> Tuple[np.ndarray, Optional[np.ndarray], np.ndarray, Optional[np.ndarray]]:\n",
    "    \"\"\"Load an existing processed dataset for the given configuration.\n",
    "\n",
    "    Datasets are assumed to be pre-generated by preprocessing utilities. This\n",
    "    function never overwrites or generates new data; it only loads.\n",
    "    \"\"\"\n",
    "    # Ensure all datasets are generated once (no-op if already done)\n",
    "    generate_all_processed_datasets(data_dir=data_dir, only_once=True)\n",
    "\n",
    "    full_suffix = pp_build_full_suffix(sampling_method, remove_outliers)\n",
    "    split = load_processed_dataset(data_dir=data_dir, sampling_suffix=full_suffix)\n",
    "\n",
    "    X_train_res = split.X_train.values\n",
    "    y_train_res = split.y_train.values\n",
    "    X_val = split.X_val.values if split.X_val is not None else None\n",
    "    y_val = split.y_val.values if split.y_val is not None else None\n",
    "\n",
    "    return X_train_res, X_val, y_train_res, y_val\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Refitting {model_name} with best parameters and transferring to PTB (5-class as-is)\")\n",
    "print(f\"Data sampling (loader): {sampling_method} | Model sampler: {model_sampler} | Outlier removal: {remove_outliers}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# 1) Train multi-class on MIT-BIH (as-is, 5 classes)\n",
    "X_train, X_val, y_train, y_val = prepare_dataset_with_sampling(\n",
    "    sampling_method=sampling_method,\n",
    "    remove_outliers=remove_outliers\n",
    ")\n",
    "\n",
    "best_params_clean = {k.replace(\"classifier__\", \"\"): v for k, v in params.items()}\n",
    "estimator = Pipeline([\n",
    "    (\"sampler\", SMOTE(random_state=42)),\n",
    "    (\"classifier\", xgb.XGBClassifier(\n",
    "        **best_params_clean,\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=5,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        eval_metric=\"mlogloss\",\n",
    "    )),\n",
    "])\n",
    "\n",
    "# Fit once\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on MIT validation set first\n",
    "y_pred_mit = estimator.predict(X_val)\n",
    "mit_accuracy = accuracy_score(y_val, y_pred_mit)\n",
    "mit_precision_macro, mit_recall_macro, mit_f1_macro, _ = precision_recall_fscore_support(\n",
    "    y_val, y_pred_mit, average='macro', zero_division=0\n",
    ")\n",
    "mit_labels = np.unique(np.concatenate([y_train, y_val]))\n",
    "mit_precision_per_class, mit_recall_per_class, mit_f1_per_class, _ = precision_recall_fscore_support(\n",
    "    y_val, y_pred_mit, average=None, labels=mit_labels, zero_division=0\n",
    ")\n",
    "mit_confusion = confusion_matrix(y_val, y_pred_mit, labels=mit_labels)\n",
    "\n",
    "print(f\"\\nMIT-BIH Validation Metrics:\")\n",
    "print(f\"Accuracy: {mit_accuracy:.4f}\")\n",
    "print(f\"F1-Macro: {mit_f1_macro:.4f}\")\n",
    "print(f\"Precision-Macro: {mit_precision_macro:.4f}\")\n",
    "print(f\"Recall-Macro: {mit_recall_macro:.4f}\")\n",
    "print(f\"\\nPer-class F1 scores:\")\n",
    "for lbl, f1 in zip(mit_labels, mit_f1_per_class):\n",
    "    print(f\"  Class {int(lbl)}: {f1:.4f}\")\n",
    "print(f\"\\nMIT-BIH Confusion Matrix (rows=true, cols=pred):\\n{mit_confusion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1bb1950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Evaluating MIT-trained 5-class XGBoost mapped to binary on full PTB (train+val)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAGdCAYAAAAPLEfqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAALp1JREFUeJzt3XtYVXW+x/EPiICVG0STywmVLoPXvGAaVqZJ0uh0opwmi8qnSKsDjeacTCdFu41lqalxNKfxMmf0aM2k46iDIowyKd5ARiRlbMYJyzaMB2UnKSKs88cc1uP21g9C9t76fj3Peh73+n33Wt8f69nwce211/azLMsSAAAALsnf0w0AAAD4AkITAACAAUITAACAAUITAACAAUITAACAAUITAACAAUITAACAAUITAACAgQBPN3ClqKur05EjR9S6dWv5+fl5uh0AAGDAsix98803ioqKkr//pc8lEZqayJEjRxQdHe3pNgAAQCMcPnxYN9xwwyVrCE1NpHXr1pL+9UN3OBwe7gYAAJhwuVyKjo62/45fCqGpidS/JedwOAhNAAD4GJNLa7gQHAAAwAChCQAAwAChCQAAwAChCQAAwAChCQAAwAChCQAAwAChCQAAwAChCQAAwAChCQAAwAChCQAAwAChCQAAwAChCQAAwAChCQAAwECApxuAmdLSUh09erTZ99uuXTt16NCh2fcLAIC3ITT5gNLSUnXu3EUnT37b7Ptu1eoaHTiwn+AEALjqEZp8wNGjR3Xy5Lfq//RUOSI7Ndt+XV//QzsWvaqjR48SmgAAVz1Ckw9xRHZSWIdYT7cBAMBViQvBAQAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADBCaAAAADHg0NOXm5ur+++9XVFSU/Pz8tHr1anuspqZGL7/8snr06KFrr71WUVFRevLJJ3XkyBG3bVRUVCg5OVkOh0OhoaFKSUnRiRMn3Gr27t2ru+66S8HBwYqOjtaMGTPO6+Xjjz9W586dFRwcrB49emj9+vWXZc4AAMA3eTQ0VVVVqWfPnsrIyDhv7Ntvv1VBQYGmTJmigoICffLJJyopKdG///u/u9UlJyeruLhYWVlZWrt2rXJzczVmzBh73OVyaejQoerYsaPy8/P1zjvvaNq0aVq4cKFds23bNj366KNKSUnRnj17lJSUpKSkJO3bt+/yTR4AAPgUP8uyLE83IUl+fn5atWqVkpKSLlqza9cu9evXT1988YU6dOig/fv3q2vXrtq1a5f69u0rScrMzNSwYcP05ZdfKioqSvPnz9crr7wip9OpwMBASdLEiRO1evVqHThwQJL0yCOPqKqqSmvXrrX3dfvtt6tXr15asGCBUf8ul0shISGqrKyUw+Fo5E/hwgoKChQXF6d7X1mssA6xTbrtS6koLVHWm08pPz9fffr0abb9AgDQXBry99unrmmqrKyUn5+fQkNDJUl5eXkKDQ21A5MkJSQkyN/fXzt27LBrBg4caAcmSUpMTFRJSYmOHTtm1yQkJLjtKzExUXl5eRftpbq6Wi6Xy20BAABXLp8JTadOndLLL7+sRx991E6CTqdT7du3d6sLCAhQWFiYnE6nXRMeHu5WU//4u2rqxy9k+vTpCgkJsZfo6OjvN0EAAODVfCI01dTU6Cc/+Yksy9L8+fM93Y4kadKkSaqsrLSXw4cPe7olAABwGQV4uoHvUh+YvvjiC+Xk5Li93xgREaHy8nK3+jNnzqiiokIRERF2TVlZmVtN/ePvqqkfv5CgoCAFBQU1fmIAAMCnePWZpvrAdPDgQW3atElt27Z1G4+Pj9fx48eVn59vr8vJyVFdXZ369+9v1+Tm5qqmpsauycrKUmxsrNq0aWPXZGdnu207KytL8fHxl2tqAADAx3g0NJ04cUKFhYUqLCyUJB06dEiFhYUqLS1VTU2NfvzjH2v37t1atmyZamtr5XQ65XQ6dfr0aUlSly5ddN9992n06NHauXOntm7dqrS0NI0cOVJRUVGSpMcee0yBgYFKSUlRcXGxVq5cqTlz5mj8+PF2H2PHjlVmZqZmzpypAwcOaNq0adq9e7fS0tKa/WcCAAC8k0dD0+7du9W7d2/17t1bkjR+/Hj17t1b6enp+uqrr7RmzRp9+eWX6tWrlyIjI+1l27Zt9jaWLVumzp07a8iQIRo2bJjuvPNOt3swhYSEaOPGjTp06JDi4uL0s5/9TOnp6W73chowYICWL1+uhQsXqmfPnvrtb3+r1atXq3v37s33wwAAAF7No9c0DRo0SJe6TZTJLaTCwsK0fPnyS9bceuut+vOf/3zJmocfflgPP/zwd+4PAABcnbz6miYAAABvQWgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAwQGgCAAAw4NHQlJubq/vvv19RUVHy8/PT6tWr3cYty1J6eroiIyPVqlUrJSQk6ODBg241FRUVSk5OlsPhUGhoqFJSUnTixAm3mr179+quu+5ScHCwoqOjNWPGjPN6+fjjj9W5c2cFBwerR48eWr9+fZPPFwAA+C6Phqaqqir17NlTGRkZFxyfMWOG5s6dqwULFmjHjh269tprlZiYqFOnTtk1ycnJKi4uVlZWltauXavc3FyNGTPGHne5XBo6dKg6duyo/Px8vfPOO5o2bZoWLlxo12zbtk2PPvqoUlJStGfPHiUlJSkpKUn79u27fJMHAAA+xc+yLMvTTUiSn5+fVq1apaSkJEn/OssUFRWln/3sZ/rP//xPSVJlZaXCw8O1ZMkSjRw5Uvv371fXrl21a9cu9e3bV5KUmZmpYcOG6csvv1RUVJTmz5+vV155RU6nU4GBgZKkiRMnavXq1Tpw4IAk6ZFHHlFVVZXWrl1r93P77berV69eWrBggVH/LpdLISEhqqyslMPhaKofiySpoKBAcXFxuveVxQrrENuk276UitISZb35lPLz89WnT59m2y8AAM2lIX+/vfaapkOHDsnpdCohIcFeFxISov79+ysvL0+SlJeXp9DQUDswSVJCQoL8/f21Y8cOu2bgwIF2YJKkxMRElZSU6NixY3bN2fupr6nfDwAAQICnG7gYp9MpSQoPD3dbHx4ebo85nU61b9/ebTwgIEBhYWFuNTExMedto36sTZs2cjqdl9zPhVRXV6u6utp+7HK5GjI9AADgY7z2TJO3mz59ukJCQuwlOjra0y0BAIDLyGtDU0REhCSprKzMbX1ZWZk9FhERofLycrfxM2fOqKKiwq3mQts4ex8Xq6kfv5BJkyapsrLSXg4fPtzQKQIAAB/itaEpJiZGERERys7Otte5XC7t2LFD8fHxkqT4+HgdP35c+fn5dk1OTo7q6urUv39/uyY3N1c1NTV2TVZWlmJjY9WmTRu75uz91NfU7+dCgoKC5HA43BYAAHDl8mhoOnHihAoLC1VYWCjpXxd/FxYWqrS0VH5+fho3bpzeeOMNrVmzRkVFRXryyScVFRVlf8KuS5cuuu+++zR69Gjt3LlTW7duVVpamkaOHKmoqChJ0mOPPabAwEClpKSouLhYK1eu1Jw5czR+/Hi7j7FjxyozM1MzZ87UgQMHNG3aNO3evVtpaWnN/SMBAABeyqMXgu/evVuDBw+2H9cHmVGjRmnJkiWaMGGCqqqqNGbMGB0/flx33nmnMjMzFRwcbD9n2bJlSktL05AhQ+Tv768RI0Zo7ty59nhISIg2btyo1NRUxcXFqV27dkpPT3e7l9OAAQO0fPlyTZ48WT//+c91yy23aPXq1erevXsz/BQAAIAv8Jr7NPk67tMEAIDvuSLu0wQAAOBNCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGCE0AAAAGvDo01dbWasqUKYqJiVGrVq1000036fXXX5dlWXaNZVlKT09XZGSkWrVqpYSEBB08eNBtOxUVFUpOTpbD4VBoaKhSUlJ04sQJt5q9e/fqrrvuUnBwsKKjozVjxoxmmSMAAPANXh2a3n77bc2fP1/vv/++9u/fr7ffflszZszQvHnz7JoZM2Zo7ty5WrBggXbs2KFrr71WiYmJOnXqlF2TnJys4uJiZWVlae3atcrNzdWYMWPscZfLpaFDh6pjx47Kz8/XO++8o2nTpmnhwoXNOl8AAOC9AjzdwKVs27ZNDzzwgIYPHy5J6tSpk/7nf/5HO3fulPSvs0zvvfeeJk+erAceeECS9Otf/1rh4eFavXq1Ro4cqf379yszM1O7du1S3759JUnz5s3TsGHD9O677yoqKkrLli3T6dOntWjRIgUGBqpbt24qLCzUrFmz3MIVAAC4enn1maYBAwYoOztbf/3rXyVJf/nLX/Tpp5/qhz/8oSTp0KFDcjqdSkhIsJ8TEhKi/v37Ky8vT5KUl5en0NBQOzBJUkJCgvz9/bVjxw67ZuDAgQoMDLRrEhMTVVJSomPHjl2wt+rqarlcLrcFAABcubz6TNPEiRPlcrnUuXNntWjRQrW1tXrzzTeVnJwsSXI6nZKk8PBwt+eFh4fbY06nU+3bt3cbDwgIUFhYmFtNTEzMeduoH2vTps15vU2fPl2vvvpqE8wSAAD4Aq8+0/TRRx9p2bJlWr58uQoKCrR06VK9++67Wrp0qadb06RJk1RZWWkvhw8f9nRLAADgMvLqM00vvfSSJk6cqJEjR0qSevTooS+++ELTp0/XqFGjFBERIUkqKytTZGSk/byysjL16tVLkhQREaHy8nK37Z45c0YVFRX28yMiIlRWVuZWU/+4vuZcQUFBCgoK+v6TBAAAPsGrzzR9++238vd3b7FFixaqq6uTJMXExCgiIkLZ2dn2uMvl0o4dOxQfHy9Jio+P1/Hjx5Wfn2/X5OTkqK6uTv3797drcnNzVVNTY9dkZWUpNjb2gm/NAQCAq49Xh6b7779fb775ptatW6d//OMfWrVqlWbNmqUHH3xQkuTn56dx48bpjTfe0Jo1a1RUVKQnn3xSUVFRSkpKkiR16dJF9913n0aPHq2dO3dq69atSktL08iRIxUVFSVJeuyxxxQYGKiUlBQVFxdr5cqVmjNnjsaPH++pqQMAAC/j1W/PzZs3T1OmTNF//Md/qLy8XFFRUXr22WeVnp5u10yYMEFVVVUaM2aMjh8/rjvvvFOZmZkKDg62a5YtW6a0tDQNGTJE/v7+GjFihObOnWuPh4SEaOPGjUpNTVVcXJzatWun9PR0bjcAAABsftbZt9dGo7lcLoWEhKiyslIOh6NJt11QUKC4uDjd+8pihXWIbdJtX0pFaYmy3nxK+fn56tOnT7PtFwCA5tKQv99e/fYcAACAtyA0AQAAGCA0AQAAGCA0AQAAGGhUaLrxxhv1v//7v+etP378uG688cbv3RQAAIC3aVRo+sc//qHa2trz1ldXV+urr7763k0BAAB4mwbdp2nNmjX2vzds2KCQkBD7cW1trbKzs9WpU6cmaw4AAMBbNCg01d9l28/PT6NGjXIba9mypTp16qSZM2c2WXMAAADeokGh6ezvfNu1a5fatWt3WZoCAADwNo36GpVDhw41dR8AAABerdHfPZedna3s7GyVl5fbZ6DqLVq06Hs3BgAA4E0aFZpeffVVvfbaa+rbt68iIyPl5+fX1H0BAAB4lUaFpgULFmjJkiV64oknmrofAAAAr9So+zSdPn1aAwYMaOpeAAAAvFajQtMzzzyj5cuXN3UvAAAAXqtRb8+dOnVKCxcu1KZNm3TrrbeqZcuWbuOzZs1qkuYAAAC8RaNC0969e9WrVy9J0r59+9zGuCgcAABciRoVmv70pz81dR8AAABerVHXNAEAAFxtGnWmafDgwZd8Gy4nJ6fRDQEAAHijRoWm+uuZ6tXU1KiwsFD79u0774t8AQAArgSNCk2zZ8++4Ppp06bpxIkT36shAAAAb9Sk1zQ9/vjjfO8cAAC4IjVpaMrLy1NwcHBTbhIAAMArNOrtuYceesjtsWVZ+vrrr7V7925NmTKlSRoDAADwJo0KTSEhIW6P/f39FRsbq9dee01Dhw5tksYAAAC8SaNC0+LFi5u6DwAAAK/WqNBULz8/X/v375ckdevWTb17926SpgAAALxNo0JTeXm5Ro4cqc2bNys0NFSSdPz4cQ0ePFgrVqzQ9ddf35Q9AgAAeFyjPj33wgsv6JtvvlFxcbEqKipUUVGhffv2yeVy6ac//WlT9wgAAOBxjTrTlJmZqU2bNqlLly72uq5duyojI4MLwQEAwBWpUWea6urq1LJly/PWt2zZUnV1dd+7KQAAAG/TqNB0zz33aOzYsTpy5Ii97quvvtKLL76oIUOGNFlzAAAA3qJRoen999+Xy+VSp06ddNNNN+mmm25STEyMXC6X5s2b19Q9AgAAeFyjrmmKjo5WQUGBNm3apAMHDkiSunTpooSEhCZtDgAAwFs06ExTTk6OunbtKpfLJT8/P91777164YUX9MILL+i2225Tt27d9Oc///ly9QoAAOAxDQpN7733nkaPHi2Hw3HeWEhIiJ599lnNmjWryZoDAADwFg0KTX/5y1903333XXR86NChys/P/95NAQAAeJsGhaaysrIL3mqgXkBAgP75z39+76YAAAC8TYNC07/9279p3759Fx3fu3evIiMjv3dTAAAA3qZBoWnYsGGaMmWKTp06dd7YyZMnNXXqVP3oRz9qsuYAAAC8RYNuOTB58mR98skn+sEPfqC0tDTFxsZKkg4cOKCMjAzV1tbqlVdeuSyNAgAAeFKDzjSFh4dr27Zt6t69uyZNmqQHH3xQDz74oH7+85+re/fu+vTTTxUeHt6kDX711Vd6/PHH1bZtW7Vq1Uo9evTQ7t277XHLspSenq7IyEi1atVKCQkJOnjwoNs2KioqlJycLIfDodDQUKWkpOjEiRNuNXv37tVdd92l4OBgRUdHa8aMGU06DwAA4NsafHPLjh07av369Tp27Jg+//xzWZalW265RW3atGny5o4dO6Y77rhDgwcP1h//+Eddf/31OnjwoNu+ZsyYoblz52rp0qWKiYnRlClTlJiYqM8++0zBwcGSpOTkZH399dfKyspSTU2NnnrqKY0ZM0bLly+XJLlcLg0dOlQJCQlasGCBioqK9PTTTys0NFRjxoxp8nkBAADf06g7gktSmzZtdNtttzVlL+d5++23FR0drcWLF9vrYmJi7H9blqX33ntPkydP1gMPPCBJ+vWvf63w8HCtXr1aI0eO1P79+5WZmaldu3apb9++kqR58+Zp2LBhevfddxUVFaVly5bp9OnTWrRokQIDA9WtWzcVFhZq1qxZhCYAACCpkd8911zWrFmjvn376uGHH1b79u3Vu3dv/fKXv7THDx06JKfT6fb1LSEhIerfv7/y8vIkSXl5eQoNDbUDkyQlJCTI399fO3bssGsGDhyowMBAuyYxMVElJSU6duzY5Z4mAADwAV4dmv7+979r/vz5uuWWW7RhwwY9//zz+ulPf6qlS5dKkpxOpySddx1VeHi4PeZ0OtW+fXu38YCAAIWFhbnVXGgbZ+/jXNXV1XK5XG4LAAC4cjX67bnmUFdXp759++oXv/iFJKl3797at2+fFixYoFGjRnm0t+nTp+vVV1/1aA8AAKD5ePWZpsjISHXt2tVtXZcuXVRaWipJioiIkPSvO5WfrayszB6LiIhQeXm52/iZM2dUUVHhVnOhbZy9j3NNmjRJlZWV9nL48OHGTBEAAPgIrw5Nd9xxh0pKStzW/fWvf1XHjh0l/eui8IiICGVnZ9vjLpdLO3bsUHx8vCQpPj5ex48fd/tOvJycHNXV1al///52TW5urmpqauyarKwsxcbGXvRTgUFBQXI4HG4LAAC4cnl1aHrxxRe1fft2/eIXv9Dnn3+u5cuXa+HChUpNTZUk+fn5ady4cXrjjTe0Zs0aFRUV6cknn1RUVJSSkpIk/evM1H333afRo0dr586d2rp1q9LS0jRy5EhFRUVJkh577DEFBgYqJSVFxcXFWrlypebMmaPx48d7auoAAMDLePU1TbfddptWrVqlSZMm6bXXXlNMTIzee+89JScn2zUTJkxQVVWVxowZo+PHj+vOO+9UZmamfY8mSVq2bJnS0tI0ZMgQ+fv7a8SIEZo7d649HhISoo0bNyo1NVVxcXFq166d0tPTud0AAACw+VmWZXm6iSuBy+VSSEiIKisrm/ytuoKCAsXFxeneVxYrrENsk277UipKS5T15lPKz89Xnz59mm2/AAA0l4b8/fbqt+cAAAC8BaEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAAKEJAADAgE+Fprfeekt+fn4aN26cve7UqVNKTU1V27Ztdd1112nEiBEqKytze15paamGDx+ua665Ru3bt9dLL72kM2fOuNVs3rxZffr0UVBQkG6++WYtWbKkGWYEAAB8hc+Epl27dumDDz7Qrbfe6rb+xRdf1B/+8Ad9/PHH2rJli44cOaKHHnrIHq+trdXw4cN1+vRpbdu2TUuXLtWSJUuUnp5u1xw6dEjDhw/X4MGDVVhYqHHjxumZZ57Rhg0bmm1+AADAu/lEaDpx4oSSk5P1y1/+Um3atLHXV1ZW6le/+pVmzZqle+65R3FxcVq8eLG2bdum7du3S5I2btyozz77TL/5zW/Uq1cv/fCHP9Trr7+ujIwMnT59WpK0YMECxcTEaObMmerSpYvS0tL04x//WLNnz/bIfAEAgPfxidCUmpqq4cOHKyEhwW19fn6+ampq3NZ37txZHTp0UF5eniQpLy9PPXr0UHh4uF2TmJgol8ul4uJiu+bcbScmJtrbuJDq6mq5XC63BQAAXLkCPN3Ad1mxYoUKCgq0a9eu88acTqcCAwMVGhrqtj48PFxOp9OuOTsw1Y/Xj12qxuVy6eTJk2rVqtV5+54+fbpeffXVRs8LAAD4Fq8+03T48GGNHTtWy5YtU3BwsKfbcTNp0iRVVlbay+HDhz3dEgAAuIy8OjTl5+ervLxcffr0UUBAgAICArRlyxbNnTtXAQEBCg8P1+nTp3X8+HG355WVlSkiIkKSFBERcd6n6eoff1eNw+G44FkmSQoKCpLD4XBbAADAlcurQ9OQIUNUVFSkwsJCe+nbt6+Sk5Ptf7ds2VLZ2dn2c0pKSlRaWqr4+HhJUnx8vIqKilReXm7XZGVlyeFwqGvXrnbN2duor6nfBgAAgFdf09S6dWt1797dbd21116rtm3b2utTUlI0fvx4hYWFyeFw6IUXXlB8fLxuv/12SdLQoUPVtWtXPfHEE5oxY4acTqcmT56s1NRUBQUFSZKee+45vf/++5owYYKefvpp5eTk6KOPPtK6deuad8IAAMBreXVoMjF79mz5+/trxIgRqq6uVmJiov7rv/7LHm/RooXWrl2r559/XvHx8br22ms1atQovfbaa3ZNTEyM1q1bpxdffFFz5szRDTfcoA8//FCJiYmemBIAAPBCPheaNm/e7PY4ODhYGRkZysjIuOhzOnbsqPXr119yu4MGDdKePXuaokUAAHAF8uprmgAAALwFoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMCAV4em6dOn67bbblPr1q3Vvn17JSUlqaSkxK3m1KlTSk1NVdu2bXXddddpxIgRKisrc6spLS3V8OHDdc0116h9+/Z66aWXdObMGbeazZs3q0+fPgoKCtLNN9+sJUuWXO7pAQAAH+LVoWnLli1KTU3V9u3blZWVpZqaGg0dOlRVVVV2zYsvvqg//OEP+vjjj7VlyxYdOXJEDz30kD1eW1ur4cOH6/Tp09q2bZuWLl2qJUuWKD093a45dOiQhg8frsGDB6uwsFDjxo3TM888ow0bNjTrfAEAgPcK8HQDl5KZmen2eMmSJWrfvr3y8/M1cOBAVVZW6le/+pWWL1+ue+65R5K0ePFidenSRdu3b9ftt9+ujRs36rPPPtOmTZsUHh6uXr166fXXX9fLL7+sadOmKTAwUAsWLFBMTIxmzpwpSerSpYs+/fRTzZ49W4mJic0+bwAA4H28+kzTuSorKyVJYWFhkqT8/HzV1NQoISHBruncubM6dOigvLw8SVJeXp569Oih8PBwuyYxMVEul0vFxcV2zdnbqK+p38aFVFdXy+VyuS0AAODK5TOhqa6uTuPGjdMdd9yh7t27S5KcTqcCAwMVGhrqVhseHi6n02nXnB2Y6sfrxy5V43K5dPLkyQv2M336dIWEhNhLdHT0954jAADwXj4TmlJTU7Vv3z6tWLHC061IkiZNmqTKykp7OXz4sKdbAgAAl5FXX9NULy0tTWvXrlVubq5uuOEGe31ERIROnz6t48ePu51tKisrU0REhF2zc+dOt+3Vf7ru7JpzP3FXVlYmh8OhVq1aXbCnoKAgBQUFfe+5AQAA3+DVZ5osy1JaWppWrVqlnJwcxcTEuI3HxcWpZcuWys7OtteVlJSotLRU8fHxkqT4+HgVFRWpvLzcrsnKypLD4VDXrl3tmrO3UV9Tvw0AAACvPtOUmpqq5cuX6/e//71at25tX4MUEhKiVq1aKSQkRCkpKRo/frzCwsLkcDj0wgsvKD4+XrfffrskaejQoerataueeOIJzZgxQ06nU5MnT1Zqaqp9pui5557T+++/rwkTJujpp59WTk6OPvroI61bt85jcwcAAN7Fq880zZ8/X5WVlRo0aJAiIyPtZeXKlXbN7Nmz9aMf/UgjRozQwIEDFRERoU8++cQeb9GihdauXasWLVooPj5ejz/+uJ588km99tprdk1MTIzWrVunrKws9ezZUzNnztSHH37I7QYAAIDNq880WZb1nTXBwcHKyMhQRkbGRWs6duyo9evXX3I7gwYN0p49exrcIwAAuDp49ZkmAAAAb0FoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMEBoAgAAMBDg6QYAAIBvKS0t1dGjR5t9v+3atVOHDh2afb/1CE3AWa7WXwQAYKq0tFSdO3fRyZPfNvu+W7W6RgcO7PfY70tCE/D/ruZfBFcLQjHw/R09elQnT36r/k9PlSOyU7Pt1/X1P7Rj0as6evQooQnwtKv5F8HVgFAMNC1HZCeFdYj1dBvNitAEnONq/EVwNSAUA/i+CE0AriqEYgCNxS0HAAAADBCaAAAADBCaAAAADBCaAAAADBCazpGRkaFOnTopODhY/fv3186dOz3dEgAA8AKEprOsXLlS48eP19SpU1VQUKCePXsqMTFR5eXlnm4NAAB4GKHpLLNmzdLo0aP11FNPqWvXrlqwYIGuueYaLVq0yNOtAQAAD+M+Tf/v9OnTys/P16RJk+x1/v7+SkhIUF5e3nn11dXVqq6uth9XVlZKklwuV5P3duLECUlSxRclOlN9ssm3fzEuZ6kkKT8/3+6hufj7+6uurq5Z91lSUiKJn/OVul+OL/tlv03D06+lEydONOnf2vptWZb13cUWLMuyrK+++sqSZG3bts1t/UsvvWT169fvvPqpU6daklhYWFhYWFiugOXw4cPfmRU409RIkyZN0vjx4+3HdXV1qqioUNu2beXn59ek+3K5XIqOjtbhw4flcDiadNvegPn5vit9jlf6/KQrf47Mz/ddrjlalqVvvvlGUVFR31lLaPp/7dq1U4sWLVRWVua2vqysTBEREefVBwUFKSgoyG1daGjo5WxRDofjin0xSMzvSnClz/FKn5905c+R+fm+yzHHkJAQozouBP9/gYGBiouLU3Z2tr2urq5O2dnZio+P92BnAADAG3Cm6Szjx4/XqFGj1LdvX/Xr10/vvfeeqqqq9NRTT3m6NQAA4GGEprM88sgj+uc//6n09HQ5nU716tVLmZmZCg8P92hfQUFBmjp16nlvB14pmJ/vu9LneKXPT7ry58j8fJ83zNHPskw+YwcAAHB145omAAAAA4QmAAAAA4QmAAAAA4QmAAAAA4QmL5GRkaFOnTopODhY/fv3186dOy9Z//HHH6tz584KDg5Wjx49tH79+mbqtHEaMr8lS5bIz8/PbQkODm7GbhsmNzdX999/v6KiouTn56fVq1d/53M2b96sPn36KCgoSDfffLOWLFly2ftsrIbOb/PmzecdPz8/PzmdzuZpuIGmT5+u2267Ta1bt1b79u2VlJRkf7fWpfjSa7Axc/Sl1+H8+fN166232jc9jI+P1x//+MdLPseXjl9D5+dLx+5C3nrrLfn5+WncuHGXrPPEMSQ0eYGVK1dq/Pjxmjp1qgoKCtSzZ08lJiaqvLz8gvXbtm3To48+qpSUFO3Zs0dJSUlKSkrSvn37mrlzMw2dn/SvO75+/fXX9vLFF180Y8cNU1VVpZ49eyojI8Oo/tChQxo+fLgGDx6swsJCjRs3Ts8884w2bNhwmTttnIbOr15JSYnbMWzfvv1l6vD72bJli1JTU7V9+3ZlZWWppqZGQ4cOVVVV1UWf42uvwcbMUfKd1+ENN9ygt956S/n5+dq9e7fuuecePfDAAyouLr5gva8dv4bOT/KdY3euXbt26YMPPtCtt956yTqPHcOm+bpbfB/9+vWzUlNT7ce1tbVWVFSUNX369AvW/+QnP7GGDx/utq5///7Ws88+e1n7bKyGzm/x4sVWSEhIM3XXtCRZq1atumTNhAkTrG7durmte+SRR6zExMTL2FnTMJnfn/70J0uSdezYsWbpqamVl5dbkqwtW7ZctMbXXoPnMpmjL78OLcuy2rRpY3344YcXHPP142dZl56frx67b775xrrlllusrKws6+6777bGjh170VpPHUPONHnY6dOnlZ+fr4SEBHudv7+/EhISlJeXd8Hn5OXludVLUmJi4kXrPakx85OkEydOqGPHjoqOjv7O/1H5Gl86ft9Hr169FBkZqXvvvVdbt271dDvGKisrJUlhYWEXrfH1Y2gyR8k3X4e1tbVasWKFqqqqLvoVWL58/EzmJ/nmsUtNTdXw4cPPOzYX4qljSGjysKNHj6q2tva8u46Hh4df9BoQp9PZoHpPasz8YmNjtWjRIv3+97/Xb37zG9XV1WnAgAH68ssvm6Ply+5ix8/lcunkyZMe6qrpREZGasGCBfrd736n3/3ud4qOjtagQYNUUFDg6da+U11dncaNG6c77rhD3bt3v2idL70Gz2U6R197HRYVFem6665TUFCQnnvuOa1atUpdu3a9YK0vHr+GzM/Xjp0krVixQgUFBZo+fbpRvaeOIV+jAq8THx/v9j+oAQMGqEuXLvrggw/0+uuve7AzmIiNjVVsbKz9eMCAAfrb3/6m2bNn67//+7892Nl3S01N1b59+/Tpp596upXLxnSOvvY6jI2NVWFhoSorK/Xb3/5Wo0aN0pYtWy4aLHxNQ+bna8fu8OHDGjt2rLKysrz+gnVCk4e1a9dOLVq0UFlZmdv6srIyRUREXPA5ERERDar3pMbM71wtW7ZU79699fnnn1+OFpvdxY6fw+FQq1atPNTV5dWvXz+vDyJpaWlau3atcnNzdcMNN1yy1pdeg2dryBzP5e2vw8DAQN18882SpLi4OO3atUtz5szRBx98cF6tLx6/hszvXN5+7PLz81VeXq4+ffrY62pra5Wbm6v3339f1dXVatGihdtzPHUMeXvOwwIDAxUXF6fs7Gx7XV1dnbKzsy/6fnV8fLxbvSRlZWVd8v1tT2nM/M5VW1uroqIiRUZGXq42m5UvHb+mUlhY6LXHz7IspaWladWqVcrJyVFMTMx3PsfXjmFj5nguX3sd1tXVqbq6+oJjvnb8LuRS8zuXtx+7IUOGqKioSIWFhfbSt29fJScnq7Cw8LzAJHnwGF7Wy8xhZMWKFVZQUJC1ZMkS67PPPrPGjBljhYaGWk6n07Isy3riiSesiRMn2vVbt261AgICrHfffdfav3+/NXXqVKtly5ZWUVGRp6ZwSQ2d36uvvmpt2LDB+tvf/mbl5+dbI0eOtIKDg63i4mJPTeGSvvnmG2vPnj3Wnj17LEnWrFmzrD179lhffPGFZVmWNXHiROuJJ56w6//+979b11xzjfXSSy9Z+/fvtzIyMqwWLVpYmZmZnprCJTV0frNnz7ZWr15tHTx40CoqKrLGjh1r+fv7W5s2bfLUFC7p+eeft0JCQqzNmzdbX3/9tb18++23do2vvwYbM0dfeh1OnDjR2rJli3Xo0CFr79691sSJEy0/Pz9r48aNlmX5/vFr6Px86dhdzLmfnvOWY0ho8hLz5s2zOnToYAUGBlr9+vWztm/fbo/dfffd1qhRo9zqP/roI+sHP/iBFRgYaHXr1s1at25dM3fcMA2Z37hx4+za8PBwa9iwYVZBQYEHujZT/xH7c5f6OY0aNcq6++67z3tOr169rMDAQOvGG2+0Fi9e3Ox9m2ro/N5++23rpptusoKDg62wsDBr0KBBVk5OjmeaN3ChuUlyOya+/hpszBx96XX49NNPWx07drQCAwOt66+/3hoyZIgdKCzL949fQ+fnS8fuYs4NTd5yDP0sy7Iu77ksAAAA38c1TQAAAAYITQAAAAYITQAAAAYITQAAAAYITQAAAAYITQAAAAYITQAAAAYITQAAAAYITQAAAAYITQAAAAYITQAAAAYITQAAAAb+DxZVjh2X3KdGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTB (full) predicted 5-class distribution: {0: 13150, 1: 449, 2: 430, 3: 13, 4: 503}\n",
      "PTB (full) true binary distribution: {0: 4045, 1: 10500}\n",
      "PTB (full, mapped) Accuracy: 0.3488\n",
      "PTB (full, mapped) F1-Macro: 0.3265\n",
      "PTB (full, mapped) Confusion Matrix (rows=true, cols=pred):\n",
      " [[3862  183]\n",
      " [9288 1212]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fitted MIT-trained XGBoost on the entire PTB dataset (train + val)\n",
    "from src.utils.preprocessing import prepare_ptbdb\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Evaluating MIT-trained 5-class XGBoost mapped to binary on full PTB (train+val)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load PTB split (both train and validation)\n",
    "ptb_split_full = prepare_ptbdb(data_dir=\"data/original\", random_state=42)\n",
    "X_ptb_full = np.vstack([ptb_split_full.X_train.values, ptb_split_full.X_val.values])\n",
    "y_ptb_full = np.concatenate([ptb_split_full.y_train.values, ptb_split_full.y_val.values])\n",
    "\n",
    "# Predict probabilities for 5 classes and map to binary (0 -> 0, 1/2/3/4 -> 1)\n",
    "ptb_proba_5_full = estimator.predict_proba(X_ptb_full)\n",
    "y_pred_ptb_5_full = np.argmax(ptb_proba_5_full, axis=1)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.histplot(y_pred_ptb_5_full)\n",
    "plt.show()\n",
    "\n",
    "y_pred_ptb_bin_full = (y_pred_ptb_5_full != 0).astype(int)\n",
    "\n",
    "# Metrics\n",
    "acc_full = accuracy_score(y_ptb_full, y_pred_ptb_bin_full)\n",
    "prec_macro_full, rec_macro_full, f1_macro_full, _ = precision_recall_fscore_support(\n",
    "    y_ptb_full, y_pred_ptb_bin_full, average=\"macro\", zero_division=0\n",
    ")\n",
    "cm_full = confusion_matrix(y_ptb_full, y_pred_ptb_bin_full, labels=[0, 1])\n",
    "\n",
    "# Distributions\n",
    "unique_full, counts_full = np.unique(y_pred_ptb_5_full, return_counts=True)\n",
    "pred_dist = {int(k): int(v) for k, v in zip(unique_full.tolist(), counts_full.tolist())}\n",
    "true_dist = {int(k): int(v) for k, v in zip(*np.unique(y_ptb_full, return_counts=True))}\n",
    "\n",
    "print(\"PTB (full) predicted 5-class distribution:\", pred_dist)\n",
    "print(\"PTB (full) true binary distribution:\", true_dist)\n",
    "print(f\"PTB (full, mapped) Accuracy: {acc_full:.4f}\")\n",
    "print(f\"PTB (full, mapped) F1-Macro: {f1_macro_full:.4f}\")\n",
    "print(\"PTB (full, mapped) Confusion Matrix (rows=true, cols=pred):\\n\", cm_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dede1691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Evaluating MIT-trained 5-class XGBoost mapped to binary on full PTB (train+val)\n",
      "================================================================================\n",
      "PTB (full) predicted 5-class distribution: {0: 13150, 1: 449, 2: 430, 3: 13, 4: 503}\n",
      "PTB (full) true binary distribution: {0: 4045, 1: 10500}\n",
      "PTB (full, mapped) Accuracy: 0.3488\n",
      "PTB (full, mapped) F1-Macro: 0.3265\n",
      "PTB (full, mapped) Confusion Matrix (rows=true, cols=pred):\n",
      " [[3862  183]\n",
      " [9288 1212]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Evaluating MIT-trained 5-class XGBoost mapped to binary on full PTB (train+val)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load PTB split (both train and validation)\n",
    "ptb_split_full = prepare_ptbdb(data_dir=\"data/original\", random_state=42)\n",
    "X_ptb_full = np.vstack([ptb_split_full.X_train.values, ptb_split_full.X_val.values])\n",
    "y_ptb_full = np.concatenate([ptb_split_full.y_train.values, ptb_split_full.y_val.values])\n",
    "\n",
    "# Predict probabilities for 5 classes and map to binary (0 -> 0, 1/2/3/4 -> 1)\n",
    "ptb_proba_5_full = estimator.predict_proba(X_ptb_full)\n",
    "y_pred_ptb_5_full = np.argmax(ptb_proba_5_full, axis=1)\n",
    "y_pred_ptb_bin_full = (y_pred_ptb_5_full != 0).astype(int)\n",
    "\n",
    "# Metrics\n",
    "acc_full = accuracy_score(y_ptb_full, y_pred_ptb_bin_full)\n",
    "prec_macro_full, rec_macro_full, f1_macro_full, _ = precision_recall_fscore_support(\n",
    "    y_ptb_full, y_pred_ptb_bin_full, average=\"macro\", zero_division=0\n",
    ")\n",
    "cm_full = confusion_matrix(y_ptb_full, y_pred_ptb_bin_full, labels=[0, 1])\n",
    "\n",
    "# Distributions\n",
    "unique_full, counts_full = np.unique(y_pred_ptb_5_full, return_counts=True)\n",
    "pred_dist = {int(k): int(v) for k, v in zip(unique_full.tolist(), counts_full.tolist())}\n",
    "true_dist = {int(k): int(v) for k, v in zip(*np.unique(y_ptb_full, return_counts=True))}\n",
    "\n",
    "print(\"PTB (full) predicted 5-class distribution:\", pred_dist)\n",
    "print(\"PTB (full) true binary distribution:\", true_dist)\n",
    "print(f\"PTB (full, mapped) Accuracy: {acc_full:.4f}\")\n",
    "print(f\"PTB (full, mapped) F1-Macro: {f1_macro_full:.4f}\")\n",
    "print(\"PTB (full, mapped) Confusion Matrix (rows=true, cols=pred):\\n\", cm_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acce4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Refitting XGBoost with best parameters and evaluating on PTB (binary)\n",
      "Sampling: SMOTE | Outlier removal: False\n",
      "================================================================================\n",
      "Best Params: {'colsample_bytree': 0.9, 'gamma': 0.0, 'learning_rate': 0.2, 'max_depth': 9, 'min_child_weight': 5, 'n_estimators': 250, 'reg_alpha': 0.2, 'reg_lambda': 0.05, 'subsample': 0.7}\n",
      "PTB (full) predicted binary distribution: {0: 808, 1: 2101}\n",
      "PTB (full) true binary distribution: {0: 4045, 1: 10500}\n",
      "PTB (full) Accuracy: 0.9777\n",
      "PTB (full) F1-Macro: 0.9722\n",
      "PTB (full) Confusion Matrix (rows=true, cols=pred):\n",
      " [[ 776   33]\n",
      " [  32 2068]]\n"
     ]
    }
   ],
   "source": [
    "# Refit XGBoost on PTB (binary) with SMOTE using the same params, then evaluate on PTB full\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Refitting XGBoost with best parameters and evaluating on PTB (binary)\")\n",
    "print(f\"Sampling: SMOTE | Outlier removal: {False}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1) Prepare PTB split (binary)\n",
    "ptb_split = prepare_ptbdb(data_dir=\"data/original\", random_state=42)\n",
    "X_train_ptb = ptb_split.X_train.values\n",
    "y_train_ptb = ptb_split.y_train.values.astype(int)\n",
    "X_val_ptb = ptb_split.X_val.values\n",
    "y_val_ptb = ptb_split.y_val.values.astype(int)\n",
    "\n",
    "# Build \"full\" PTB set = train + val from the same split\n",
    "X_ptb_full = np.vstack([ptb_split.X_train.values, ptb_split.X_val.values])\n",
    "y_ptb_full = np.concatenate([ptb_split.y_train.values.astype(int), ptb_split.y_val.values.astype(int)])\n",
    "\n",
    "# 2) Clean params from GridSearchCV (strip 'classifier__' prefix)\n",
    "best_params_clean_ptb = {k.replace(\"classifier__\", \"\"): v for k, v in params.items()}\n",
    "\n",
    "print(f\"Best Params: {best_params_clean_ptb}\")\n",
    "\n",
    "# 3) Build pipeline: SMOTE (train only) + XGBClassifier (binary)\n",
    "pipe_ptb = Pipeline([\n",
    "    (\"sampler\", SMOTE(random_state=42)),\n",
    "    (\"classifier\", xgb.XGBClassifier(\n",
    "        **best_params_clean_ptb,\n",
    "        objective=\"binary:logistic\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "    )),\n",
    "])\n",
    "\n",
    "# 4) Fit on PTB train\n",
    "pipe_ptb.fit(X_train_ptb, y_train_ptb)\n",
    "\n",
    "# 5) Evaluate on PTB full (binary)\n",
    "# Option A: use direct class predictions\n",
    "y_pred_val = pipe_ptb.predict(X_val_ptb).astype(int)\n",
    "\n",
    "# Option B (equivalent): threshold positive-class probability at 0.5\n",
    "# proba_full = pipe_ptb.predict_proba(X_ptb_full)[:, 1]\n",
    "# y_pred_ptb_full = (proba_full >= 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "acc_full = accuracy_score(y_val_ptb, y_pred_val)\n",
    "prec_macro_full, rec_macro_full, f1_macro_full, _ = precision_recall_fscore_support(\n",
    "    y_val_ptb, y_pred_val, average=\"macro\", zero_division=0\n",
    ")\n",
    "cm_full = confusion_matrix(y_val_ptb, y_pred_val, labels=[0, 1])\n",
    "\n",
    "# Distributions\n",
    "pred_labels, pred_counts = np.unique(y_pred_val, return_counts=True)\n",
    "true_labels, true_counts = np.unique(y_ptb_full, return_counts=True)\n",
    "pred_dist = dict(zip(pred_labels.tolist(), pred_counts.tolist()))\n",
    "true_dist = dict(zip(true_labels.tolist(), true_counts.tolist()))\n",
    "\n",
    "print(\"PTB (full) predicted binary distribution:\", pred_dist)\n",
    "print(\"PTB (full) true binary distribution:\", true_dist)\n",
    "print(f\"PTB (full) Accuracy: {acc_full:.4f}\")\n",
    "print(f\"PTB (full) F1-Macro: {f1_macro_full:.4f}\")\n",
    "print(\"PTB (full) Confusion Matrix (rows=true, cols=pred):\\n\", cm_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a95b44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Transfer Learning: Evaluating PTB-trained binary XGBoost on MIT-BIH dataset\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluation 1: All abnormal classes (1-4) mapped to binary class 1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MIT true binary distribution (all abnormal): {0: 14494, 1: 3017}\n",
      "PTB model predictions distribution: {0: 2186, 1: 15325}\n",
      "\n",
      "Accuracy: 0.2850\n",
      "F1-Macro: 0.2834\n",
      "Precision-Macro: 0.5707\n",
      "Recall-Macro: 0.5542\n",
      "\n",
      "Per-class F1 scores:\n",
      "  Class 0 (Normal): 0.2494\n",
      "  Class 1 (Abnormal (all 1-4)): 0.3174\n",
      "\n",
      "Confusion Matrix (rows=true, cols=pred):\n",
      "[[ 2080 12414]\n",
      " [  106  2911]]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluation 2: Only class 1 mapped to binary class 1 (classes 2,3,4 dropped)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MIT samples: 17511 total, 2572 dropped (classes 2,3,4), 14939 kept\n",
      "MIT true binary distribution (class 1 only): {0: 14494, 1: 445}\n",
      "PTB model predictions distribution: {0: 2167, 1: 12772}\n",
      "\n",
      "Accuracy: 0.1632\n",
      "F1-Macro: 0.1519\n",
      "Precision-Macro: 0.4939\n",
      "Recall-Macro: 0.4740\n",
      "\n",
      "Per-class F1 scores:\n",
      "  Class 0 (Normal): 0.2497\n",
      "  Class 1 (Abnormal (class 1 only)): 0.0542\n",
      "\n",
      "Confusion Matrix (rows=true, cols=pred):\n",
      "[[ 2080 12414]\n",
      " [   87   358]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate PTB-trained binary model on MIT dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Transfer Learning: Evaluating PTB-trained binary XGBoost on MIT-BIH dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use MIT validation set (already loaded from previous cells)\n",
    "# X_val and y_val are from MIT-BIH\n",
    "\n",
    "# Get binary predictions from PTB model on MIT data\n",
    "y_pred_mit_binary = pipe_ptb.predict(X_val)\n",
    "\n",
    "# ============================================================================\n",
    "# Evaluation 1: Multi-class abnormal mapping\n",
    "# Map MIT classes: 0→0 (normal), {1,2,3,4}→1 (all abnormal)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Evaluation 1: All abnormal classes (1-4) mapped to binary class 1\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "y_true_mit_multi_binary = (y_val != 0).astype(int)  # 0 stays 0, all others become 1\n",
    "\n",
    "acc_1 = accuracy_score(y_true_mit_multi_binary, y_pred_mit_binary)\n",
    "prec_macro_1, rec_macro_1, f1_macro_1, _ = precision_recall_fscore_support(\n",
    "    y_true_mit_multi_binary, y_pred_mit_binary, average=\"macro\", zero_division=0\n",
    ")\n",
    "prec_per_class_1, rec_per_class_1, f1_per_class_1, _ = precision_recall_fscore_support(\n",
    "    y_true_mit_multi_binary, y_pred_mit_binary, average=None, labels=[0, 1], zero_division=0\n",
    ")\n",
    "cm_1 = confusion_matrix(y_true_mit_multi_binary, y_pred_mit_binary, labels=[0, 1])\n",
    "\n",
    "# Distribution info\n",
    "true_dist_multi = {int(k): int(v) for k, v in zip(*np.unique(y_true_mit_multi_binary, return_counts=True))}\n",
    "pred_dist_multi = {int(k): int(v) for k, v in zip(*np.unique(y_pred_mit_binary, return_counts=True))}\n",
    "\n",
    "print(f\"\\nMIT true binary distribution (all abnormal): {true_dist_multi}\")\n",
    "print(f\"PTB model predictions distribution: {pred_dist_multi}\")\n",
    "print(f\"\\nAccuracy: {acc_1:.4f}\")\n",
    "print(f\"F1-Macro: {f1_macro_1:.4f}\")\n",
    "print(f\"Precision-Macro: {prec_macro_1:.4f}\")\n",
    "print(f\"Recall-Macro: {rec_macro_1:.4f}\")\n",
    "print(f\"\\nPer-class F1 scores:\")\n",
    "for lbl, f1 in zip([0, 1], f1_per_class_1):\n",
    "    class_name = \"Normal\" if lbl == 0 else \"Abnormal (all 1-4)\"\n",
    "    print(f\"  Class {lbl} ({class_name}): {f1:.4f}\")\n",
    "print(f\"\\nConfusion Matrix (rows=true, cols=pred):\\n{cm_1}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Evaluation 2: Single-class abnormal mapping\n",
    "# Map MIT classes: 0→0 (normal), 1→1 (abnormal), drop classes {2,3,4}\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Evaluation 2: Only class 1 mapped to binary class 1 (classes 2,3,4 dropped)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create mask: keep only MIT classes 0 and 1\n",
    "keep_mask = (y_val == 0) | (y_val == 1)\n",
    "y_val_filtered = y_val[keep_mask]\n",
    "X_val_filtered = X_val[keep_mask]\n",
    "y_pred_mit_binary_filtered = pipe_ptb.predict(X_val_filtered)\n",
    "\n",
    "# Map to binary: 0→0, 1→1 (already binary since we filtered)\n",
    "y_true_mit_single_binary = y_val_filtered.astype(int)\n",
    "\n",
    "acc_2 = accuracy_score(y_true_mit_single_binary, y_pred_mit_binary_filtered)\n",
    "prec_macro_2, rec_macro_2, f1_macro_2, _ = precision_recall_fscore_support(\n",
    "    y_true_mit_single_binary, y_pred_mit_binary_filtered, average=\"macro\", zero_division=0\n",
    ")\n",
    "prec_per_class_2, rec_per_class_2, f1_per_class_2, _ = precision_recall_fscore_support(\n",
    "    y_true_mit_single_binary, y_pred_mit_binary_filtered, average=None, labels=[0, 1], zero_division=0\n",
    ")\n",
    "cm_2 = confusion_matrix(y_true_mit_single_binary, y_pred_mit_binary_filtered, labels=[0, 1])\n",
    "\n",
    "# Distribution info\n",
    "true_dist_single = {int(k): int(v) for k, v in zip(*np.unique(y_true_mit_single_binary, return_counts=True))}\n",
    "pred_dist_single = {int(k): int(v) for k, v in zip(*np.unique(y_pred_mit_binary_filtered, return_counts=True))}\n",
    "dropped_count = len(y_val) - len(y_val_filtered)\n",
    "\n",
    "print(f\"\\nMIT samples: {len(y_val)} total, {dropped_count} dropped (classes 2,3,4), {len(y_val_filtered)} kept\")\n",
    "print(f\"MIT true binary distribution (class 1 only): {true_dist_single}\")\n",
    "print(f\"PTB model predictions distribution: {pred_dist_single}\")\n",
    "print(f\"\\nAccuracy: {acc_2:.4f}\")\n",
    "print(f\"F1-Macro: {f1_macro_2:.4f}\")\n",
    "print(f\"Precision-Macro: {prec_macro_2:.4f}\")\n",
    "print(f\"Recall-Macro: {rec_macro_2:.4f}\")\n",
    "print(f\"\\nPer-class F1 scores:\")\n",
    "for lbl, f1 in zip([0, 1], f1_per_class_2):\n",
    "    class_name = \"Normal\" if lbl == 0 else \"Abnormal (class 1 only)\"\n",
    "    print(f\"  Class {lbl} ({class_name}): {f1:.4f}\")\n",
    "print(f\"\\nConfusion Matrix (rows=true, cols=pred):\\n{cm_2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
