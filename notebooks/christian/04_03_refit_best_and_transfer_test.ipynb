{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14eab3c2",
   "metadata": {},
   "source": [
    "# 4.3 Refit best and transfer test on PTB dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64239f57",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# Add src to path\n",
    "print(os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    ")\n",
    "from sklearn import set_config\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Sampling\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Custom utilities\n",
    "from src.utils.preprocessing import (\n",
    "    load_processed_dataset,\n",
    "    DatasetSplit,\n",
    "    build_full_suffix as pp_build_full_suffix,\n",
    "    generate_all_processed_datasets,\n",
    "    _normalize_sampling_method_name,\n",
    "    _SAMPLING_REGISTRY\n",
    ")\n",
    "from src.utils.evaluation import eval_model\n",
    "from src.utils.model_saver import create_model_saver\n",
    "\n",
    "import numpy as np\n",
    "from typing import Optional, Dict, Union, Tuple\n",
    "from scipy.signal import butter, filtfilt, medfilt\n",
    "import pywt\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "from src.utils.model_saver import create_model_saver\n",
    "from src.utils.preprocessing import (\n",
    "    load_processed_dataset,\n",
    "    build_full_suffix as pp_build_full_suffix,   # if not exported, import as in your notebook (pp_build_full_suffix alias)\n",
    "    generate_all_processed_datasets,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ArrayLike = Union[np.ndarray, list]\n",
    "\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = \"data/processed/mitbih\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3a6a8",
   "metadata": {},
   "source": [
    "## 2. Refit best Model from part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ae363",
   "metadata": {},
   "source": [
    "### XGBoost, SMOTE, no outlier removal,\tno feature engineering\n",
    "\n",
    "Accuracy:   0.9834\n",
    "\n",
    "F1-Macro:   0.9148\n",
    "\n",
    "CV-Score:   0.9134\n",
    "\n",
    "\n",
    "F1 Class1:  0.99\n",
    "\n",
    "F1 Class2:  0.83\n",
    "\n",
    "F1 Class3:  0.96\n",
    "\n",
    "F1 Class4:  0.81\n",
    "\n",
    "F1 Class5:  0.99\n",
    "\n",
    "params: {\"classifier__colsample_bytree\": 0.9,\"classifier__gamma\": 0.0,\"classifier__learning_rate\": 0.2,\"classifier__max_depth\": 9,\"classifier__min_child_weight\": 5,\"classifier__n_estimators\": 250,\"classifier__reg_alpha\": 0.2,\"classifier__reg_lambda\": 0.05,\"classifier__subsample\": 0.7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"classifier__colsample_bytree\": 0.9,\"classifier__gamma\": 0.0,\n",
    "               \"classifier__learning_rate\": 0.2,\"classifier__max_depth\": 9,\n",
    "               \"classifier__min_child_weight\": 5,\"classifier__n_estimators\": 250,\n",
    "               \"classifier__reg_alpha\": 0.2,\"classifier__reg_lambda\": 0.05,\n",
    "               \"classifier__subsample\": 0.7}\n",
    "sampling_method = \"SMOTE\"\n",
    "model_name = \"XGBoost\"\n",
    "remove_outliers = False\n",
    "estimator = \"XGBoost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty, Jupyter-native diagram (works in notebooks)\n",
    "def show_pipeline_diagram(pipe: Pipeline) -> None:\n",
    "    set_config(display=\"diagram\")\n",
    "    display(pipe)  # Jupyter display\n",
    "\n",
    "\n",
    "def create_leak_free_pipeline(\n",
    "    model_name: str,\n",
    "    estimator,\n",
    "    sampling_method: Optional[str] = \"none\",\n",
    "    sampler_kwargs: Optional[Dict] = None,\n",
    "    random_state: Optional[int] = 42,\n",
    ") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Build a leak-free pipeline:\n",
    "    - Using imblearn.Pipeline ensures fit/transform of SAMPLER happen within each CV fold on TRAIN only.\n",
    "    \"\"\"\n",
    "    sampler_kwargs = dict(sampler_kwargs or {})\n",
    "\n",
    "    # Provide a default random_state to samplers if not overridden\n",
    "    if random_state is not None and \"random_state\" not in sampler_kwargs:\n",
    "        sampler_kwargs[\"random_state\"] = random_state\n",
    "\n",
    "    internal_name = _normalize_sampling_method_name(sampling_method)\n",
    "\n",
    "    steps = []\n",
    "\n",
    "    SamplerClass = _SAMPLING_REGISTRY[internal_name]\n",
    "    steps.append((\"sampler\", SamplerClass(**sampler_kwargs)))\n",
    "\n",
    "    steps.append((\"classifier\", estimator))\n",
    "    display(steps)\n",
    "    return Pipeline(steps)\n",
    "\n",
    "\n",
    "def prepare_dataset_with_sampling(\n",
    "    data_dir: str = DATA_DIR,\n",
    "    sampling_method: str = \"No_Sampling\",\n",
    "    remove_outliers: bool = False\n",
    ") -> Tuple[np.ndarray, Optional[np.ndarray], np.ndarray, Optional[np.ndarray]]:\n",
    "    \"\"\"Load an existing processed dataset for the given configuration.\n",
    "\n",
    "    Datasets are assumed to be pre-generated by preprocessing utilities. This\n",
    "    function never overwrites or generates new data; it only loads.\n",
    "    \"\"\"\n",
    "    # Ensure all datasets are generated once (no-op if already done)\n",
    "    generate_all_processed_datasets(data_dir=data_dir, only_once=True)\n",
    "\n",
    "    full_suffix = pp_build_full_suffix(sampling_method, remove_outliers)\n",
    "    split = load_processed_dataset(data_dir=data_dir, sampling_suffix=full_suffix)\n",
    "\n",
    "    X_train_res = split.X_train.values\n",
    "    y_train_res = split.y_train.values\n",
    "    X_val = split.X_val.values if split.X_val is not None else None\n",
    "    y_val = split.y_val.values if split.y_val is not None else None\n",
    "\n",
    "    return X_train_res, X_val, y_train_res, y_val\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Running GridSearchCV for {model_name} with {sampling_method}\")\n",
    "print(f\"Outlier removal: {remove_outliers}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_val, y_train, y_val = prepare_dataset_with_sampling(\n",
    "    sampling_method=\"No_Sampling\", # using non-sampled method for training - apply sampling inside pipeline\n",
    "    remove_outliers=remove_outliers\n",
    ")\n",
    "\n",
    "# Create leak-free pipeline\n",
    "pipeline = create_leak_free_pipeline(model_name, estimator, sampling_method)\n",
    "\n",
    "# Adjust parameter names for pipeline\n",
    "pipeline_params = {}\n",
    "for param_name, param_values in params.items():\n",
    "    pipeline_params[f'classifier__{param_name}'] = param_values\n",
    "\n",
    "# Create experiment name\n",
    "experiment_name = f\"{sampling_method.lower()}_outliers_{remove_outliers}\"\n",
    "\n",
    "# ADD MODEL FIT HERE \n",
    "\n",
    "    \n",
    "# For evaluation, we need to fit the model again since pipeline might not be fitted\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    y_val, y_pred, average='macro', zero_division=0\n",
    ")\n",
    "\n",
    "# Per-class metrics\n",
    "labels = np.unique(np.concatenate([y_train, y_val]))\n",
    "precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
    "    y_val, y_pred, average=None, labels=labels, zero_division=0\n",
    ")\n",
    "\n",
    "confusion_mat = confusion_matrix(y_val, y_pred, labels=labels)\n",
    "\n",
    "results = {\n",
    "    'model_name': model_name,\n",
    "    'sampling_method': sampling_method,\n",
    "    'remove_outliers': remove_outliers,\n",
    "    'best_cv_score': grid_search.best_score_,\n",
    "    'best_params': grid_search.best_params_,\n",
    "    'validation_accuracy': accuracy,\n",
    "    'validation_f1_macro': f1_macro,\n",
    "    'validation_precision_macro': precision_macro,\n",
    "    'validation_recall_macro': recall_macro,\n",
    "    'validation_f1_per_class': f1_per_class,\n",
    "    'validation_precision_per_class': precision_per_class,\n",
    "    'validation_recall_per_class': recall_per_class,\n",
    "    'validation_support_per_class': support_per_class,\n",
    "    'confusion_matrix': confusion_mat,\n",
    "    'labels': labels,\n",
    "}\n",
    "\n",
    "print(f\"Validation F1-Macro: {f1_macro:.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Append to results CSV\n",
    "row = {\n",
    "    'sampling_method': sampling_method,\n",
    "    'outliers_removed': remove_outliers,\n",
    "    'model': model_name,\n",
    "    'test_accuracy': round(float(accuracy), 4),\n",
    "    'test_f1_macro': round(float(f1_macro), 4),\n",
    "    'best_cv_score': round(float(grid_search.best_score_), 4),\n",
    "    'best_parameters': json.dumps(grid_search.best_params_),\n",
    "}\n",
    "# Add per-class F1 columns\n",
    "for lbl, f1 in zip(labels, f1_per_class):\n",
    "    row[f'test_f1_cls_{lbl}'] = round(float(f1), 2)\n",
    "\n",
    "#os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "#header = not os.path.exists(results_csv)\n",
    "#pd.DataFrame([row]).to_csv(results_csv, mode='a', index=False, header=header)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
