{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14eab3c2",
   "metadata": {},
   "source": [
    "# 4 Find best \"simple\" model for given dataset\n",
    "\n",
    "This part provides a pipeline for heartbeat classification based on the requirements from `notebooks/03_model_testing_example_mit.ipynb`.\n",
    "\n",
    "### What the notebook does\n",
    "\n",
    "- The notebook builds a leak-free pipeline per experiment:\n",
    "  - [Optional] `StandardScaler` → [Optional] `Sampler` (e.g., SMOTE) → `Classifier`\n",
    "  - This pipeline is passed into `GridSearchCV` with `RepeatedStratifiedKFold`.\n",
    "\n",
    "- Data loading:\n",
    "  - Loads preprocessed MIT-BIH training split (`X_train`, `y_train`) and a held-out validation split (`X_val`, `y_val`) from `data/processed/mitbih/`.\n",
    "  - When `remove_outliers=True`, outlier removal is applied to the training split only; the validation split is never altered.\n",
    "\n",
    "- Inside each CV fold (leak-free):\n",
    "  1. Split the training data into `train_fold` and `val_fold` (internal to CV).\n",
    "  2. Fit `StandardScaler` on `train_fold` only (If Scaling applies)\n",
    "  3. Fit the `Sampler` (e.g., SMOTE) on `train_fold` only.\n",
    "  4. Train the classifier on the resampled `train_fold`.\n",
    "  5. Evaluate on the untouched `val_fold`.\n",
    "  6. Repeat across folds; aggregate metrics and select best hyperparameters.\n",
    "\n",
    "- After CV:\n",
    "  - Refit the best pipeline on the full training data (`X_train`, `y_train`).\n",
    "  - Evaluate on the held-out validation set (`X_val`, `y_val`) which was never sampled or transformed using training information.\n",
    "  - Append results to `reports/03_model_testing_results/04_model_comparison_best_models.csv` and save the fitted model artifact.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- Sampling (oversampling/undersampling) is performed only after each fold’s split and only on the training fold within the CV loop.\n",
    "- The validation fold in CV and the final held-out validation set are kept untouched, preventing information leakage.\n",
    "- This follows best practices consistently recommended in the literature for imbalanced learning and model evaluation.\n",
    "- Accordingly, reported performance (e.g., accuracy/F1) reflects a trustworthy estimate; high scores are plausible on MIT-BIH when methodology is correct.\n",
    "\n",
    "### Minimal data-flow\n",
    "\n",
    "1. Load processed MIT-BIH data:\n",
    "   - `X_train`, `y_train` (base or with outlier removal applied to training only)\n",
    "   - `X_val`, `y_val` (always untouched)\n",
    "2. For each model/sampling setting:\n",
    "   - Construct pipeline: `[Scaler?] -> [Sampler?] -> Classifier`\n",
    "   - Run `GridSearchCV` with `RepeatedStratifiedKFold`\n",
    "     - Per fold: fit scaler/sampler on `train_fold` only; score on `val_fold`\n",
    "3. Select best params; refit on full `X_train`, `y_train`\n",
    "4. Evaluate on `X_val`, `y_val`; log metrics and save model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64239f57",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# Add src to path\n",
    "print(os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    ")\n",
    "from sklearn import set_config\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Sampling\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Custom utilities\n",
    "from src.utils.preprocessing import (\n",
    "    load_processed_dataset,\n",
    "    DatasetSplit,\n",
    "    build_full_suffix as pp_build_full_suffix,\n",
    "    generate_all_processed_datasets,\n",
    "    _normalize_sampling_method_name,\n",
    "    _SAMPLING_REGISTRY\n",
    ")\n",
    "from src.utils.evaluation import eval_model\n",
    "from src.utils.model_saver import create_model_saver\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3a6a8",
   "metadata": {},
   "source": [
    "## 2. Constants & Param Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "SCORING = {'f1_macro': 'f1_macro', 'bal_acc': 'balanced_accuracy', 'f1_weighted': 'f1_weighted'}\n",
    "results_csv = \"reports/03_model_testing_results/04_model_comparison_best_models.csv\"\n",
    "\n",
    "PARAM_SPACES = {\n",
    "    \"XGBoost\": {\n",
    "        \"estimator\": xgb.XGBClassifier(\n",
    "            objective=\"multi:softmax\",\n",
    "            num_class=5,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=\"mlogloss\",\n",
    "        ),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [150, 200, 250],\n",
    "            \"max_depth\": [8, 9],\n",
    "            \"learning_rate\": [0.2],\n",
    "            \"subsample\": [0.7, 0.8],\n",
    "            \"colsample_bytree\": [0.9],\n",
    "            \"reg_alpha\": [0.1, 0.2],\n",
    "            \"reg_lambda\": [0.0, 0.05],\n",
    "            \"min_child_weight\": [5],\n",
    "            \"gamma\": [0.0, 0.05],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=RANDOM_STATE),\n",
    "    },\n",
    "    \"ANN\": {\n",
    "        \"estimator\": MLPClassifier(\n",
    "            max_iter=300,\n",
    "            early_stopping=True,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_iter_no_change=10,\n",
    "            solver=\"adam\",\n",
    "        ),\n",
    "        \"params\": {\n",
    "            \"hidden_layer_sizes\": [(128, 64)],\n",
    "            \"activation\": [\"relu\"],\n",
    "            \"alpha\": [3e-4],\n",
    "            \"learning_rate_init\": [0.001, 0.0015],\n",
    "            \"batch_size\": [96, 128],\n",
    "            \"beta_1\": [0.9, 0.91],\n",
    "            \"beta_2\": [0.97, 0.974],\n",
    "            \"validation_fraction\": [0.1],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
    "    },\n",
    "    # best: {'clf__kernel': 'rbf', 'clf__gamma': 0,5, 'clf__C': 10}\n",
    "    \"SVM\": {\n",
    "        \"estimator\": SVC(),\n",
    "        \"params\": {\n",
    "            \"kernel\": [\"rbf\"],\n",
    "            \"C\": [10],\n",
    "            \"gamma\": [0.4, 0.5, 0.6],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
    "    },\n",
    "}\n",
    "\n",
    "DATA_DIR = \"data/processed/mitbih\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13354270",
   "metadata": {},
   "source": [
    "## 3. Methods used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty, Jupyter-native diagram (works in notebooks)\n",
    "def show_pipeline_diagram(pipe: Pipeline) -> None:\n",
    "    set_config(display=\"diagram\")\n",
    "    display(pipe)  # Jupyter display\n",
    "\n",
    "\n",
    "def create_leak_free_pipeline(\n",
    "    model_name: str,\n",
    "    estimator,\n",
    "    sampling_method: Optional[str] = \"none\",\n",
    "    sampler_kwargs: Optional[Dict] = None,\n",
    "    random_state: Optional[int] = 42,\n",
    ") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Build a leak-free pipeline:\n",
    "    - Using imblearn.Pipeline ensures fit/transform of SAMPLER happen within each CV fold on TRAIN only.\n",
    "    \"\"\"\n",
    "    sampler_kwargs = dict(sampler_kwargs or {})\n",
    "\n",
    "    # Provide a default random_state to samplers if not overridden\n",
    "    if random_state is not None and \"random_state\" not in sampler_kwargs:\n",
    "        sampler_kwargs[\"random_state\"] = random_state\n",
    "\n",
    "    internal_name = _normalize_sampling_method_name(sampling_method)\n",
    "\n",
    "    steps = []\n",
    "\n",
    "    SamplerClass = _SAMPLING_REGISTRY[internal_name]\n",
    "    steps.append((\"sampler\", SamplerClass(**sampler_kwargs)))\n",
    "\n",
    "    steps.append((\"classifier\", estimator))\n",
    "    display(steps)\n",
    "    return Pipeline(steps)\n",
    "\n",
    "\n",
    "def prepare_dataset_with_sampling(\n",
    "    data_dir: str = DATA_DIR,\n",
    "    sampling_method: str = \"No_Sampling\",\n",
    "    remove_outliers: bool = False\n",
    ") -> Tuple[np.ndarray, Optional[np.ndarray], np.ndarray, Optional[np.ndarray]]:\n",
    "    \"\"\"Load an existing processed dataset for the given configuration.\n",
    "\n",
    "    Datasets are assumed to be pre-generated by preprocessing utilities. This\n",
    "    function never overwrites or generates new data; it only loads.\n",
    "    \"\"\"\n",
    "    # Ensure all datasets are generated once (no-op if already done)\n",
    "    generate_all_processed_datasets(data_dir=data_dir, only_once=True)\n",
    "\n",
    "    full_suffix = pp_build_full_suffix(sampling_method, remove_outliers)\n",
    "    split = load_processed_dataset(data_dir=data_dir, sampling_suffix=full_suffix)\n",
    "\n",
    "    X_train_res = split.X_train.values\n",
    "    y_train_res = split.y_train.values\n",
    "    X_val = split.X_val.values if split.X_val is not None else None\n",
    "    y_val = split.y_val.values if split.y_val is not None else None\n",
    "\n",
    "    return X_train_res, X_val, y_train_res, y_val\n",
    "\n",
    "\n",
    "def run_grid_search(\n",
    "    model_name: str,\n",
    "    sampling_method: str = \"No_Sampling\",\n",
    "    remove_outliers: bool = False,\n",
    "    model_saver=None,\n",
    "    results_dir: str = \"reports/comprehensive_model_testing\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run GridSearchCV for a specific model and sampling method.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to train\n",
    "        sampling_method: Sampling method to use\n",
    "        remove_outliers: Whether to remove outliers\n",
    "        model_saver: Model saver instance\n",
    "        results_dir: Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running GridSearchCV for {model_name} with {sampling_method}\")\n",
    "    print(f\"Outlier removal: {remove_outliers}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get model configuration\n",
    "    model_config = PARAM_SPACES[model_name]\n",
    "    estimator = model_config[\"estimator\"]\n",
    "    params = model_config[\"params\"]\n",
    "    cv = model_config[\"cv\"]\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_val, y_train, y_val = prepare_dataset_with_sampling(\n",
    "        sampling_method=\"No_Sampling\", # using non-sampled method for training - apply sampling inside pipeline\n",
    "        remove_outliers=remove_outliers\n",
    "    )\n",
    "    \n",
    "    # Create leak-free pipeline\n",
    "    pipeline = create_leak_free_pipeline(model_name, estimator, sampling_method)\n",
    "    \n",
    "    # Adjust parameter names for pipeline\n",
    "    pipeline_params = {}\n",
    "    for param_name, param_values in params.items():\n",
    "        pipeline_params[f'classifier__{param_name}'] = param_values\n",
    "    \n",
    "    # Create experiment name\n",
    "    experiment_name = f\"{sampling_method.lower()}_outliers_{remove_outliers}\"\n",
    "    \n",
    "    # Check if model already exists\n",
    "    if model_saver and model_saver.model_exists(model_name, experiment_name):\n",
    "        print(f\"Model {model_name} already exists for experiment {experiment_name}. Skipping training and CSV append.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Training new model for {model_name}...\")\n",
    "        \n",
    "        # Run GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=pipeline_params,\n",
    "            scoring=SCORING,\n",
    "            refit='f1_macro',\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=3\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Save model if saver is provided\n",
    "        if model_saver:\n",
    "            metadata = {\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'best_score': grid_search.best_score_,\n",
    "                'cv_results': grid_search.cv_results_,\n",
    "                'experiment': experiment_name,\n",
    "                'classifier': model_name,\n",
    "                'sampling_method': sampling_method,\n",
    "                'remove_outliers': remove_outliers,\n",
    "            }\n",
    "            model_saver.save_model(model_name, grid_search, experiment_name, metadata)\n",
    "            print(f\"Model {model_name} saved successfully!\")\n",
    "    \n",
    "    print(f\"Evaluating {model_name} on validation set...\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # For evaluation, we need to fit the model again since pipeline might not be fitted\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_val, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Per-class metrics\n",
    "    labels = np.unique(np.concatenate([y_train, y_val]))\n",
    "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
    "        y_val, y_pred, average=None, labels=labels, zero_division=0\n",
    "    )\n",
    "    \n",
    "    confusion_mat = confusion_matrix(y_val, y_pred, labels=labels)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'sampling_method': sampling_method,\n",
    "        'remove_outliers': remove_outliers,\n",
    "        'best_cv_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'validation_accuracy': accuracy,\n",
    "        'validation_f1_macro': f1_macro,\n",
    "        'validation_precision_macro': precision_macro,\n",
    "        'validation_recall_macro': recall_macro,\n",
    "        'validation_f1_per_class': f1_per_class,\n",
    "        'validation_precision_per_class': precision_per_class,\n",
    "        'validation_recall_per_class': recall_per_class,\n",
    "        'validation_support_per_class': support_per_class,\n",
    "        'confusion_matrix': confusion_mat,\n",
    "        'labels': labels,\n",
    "    }\n",
    "    \n",
    "    print(f\"Validation F1-Macro: {f1_macro:.4f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Append to results CSV\n",
    "    row = {\n",
    "        'sampling_method': sampling_method,\n",
    "        'outliers_removed': remove_outliers,\n",
    "        'model': model_name,\n",
    "        'test_accuracy': round(float(accuracy), 4),\n",
    "        'test_f1_macro': round(float(f1_macro), 4),\n",
    "        'best_cv_score': round(float(grid_search.best_score_), 4),\n",
    "        'best_parameters': json.dumps(grid_search.best_params_),\n",
    "    }\n",
    "    # Add per-class F1 columns\n",
    "    for lbl, f1 in zip(labels, f1_per_class):\n",
    "        row[f'test_f1_cls_{lbl}'] = round(float(f1), 2)\n",
    "\n",
    "    os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "    header = not os.path.exists(results_csv)\n",
    "    pd.DataFrame([row]).to_csv(results_csv, mode='a', index=False, header=header)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b47a3",
   "metadata": {},
   "source": [
    "## 4. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Model Testing\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# Initialize model saver\n",
    "# Change to project root directory\n",
    "\n",
    "model_saver = create_model_saver(\"src/models/best_simple_models_testing\")\n",
    "\n",
    "# Define experiments to run\n",
    "experiments = [\n",
    "    # Without outlier removal\n",
    "    \n",
    "    #(\"XGBoost\", \"No_Sampling\", False),\n",
    "    #(\"ANN\", \"No_Sampling\", False),\n",
    "    (\"SVM\", \"No_Sampling\", False),\n",
    "    \n",
    "    # With outlier removal\n",
    "    #(\"XGBoost\", \"No_Sampling\", True),\n",
    "    #(\"ANN\", \"No_Sampling\", True),\n",
    "    (\"SVM\", \"No_Sampling\", True),\n",
    "    \n",
    "    # With sampling (no outlier removal)\n",
    "    #(\"XGBoost\", \"SMOTE\", False),\n",
    "    #(\"ANN\", \"SMOTE\", False),\n",
    "    (\"SVM\", \"SMOTE\", False),\n",
    "    \n",
    "    # With sampling (with outlier removal)\n",
    "    #(\"XGBoost\", \"SMOTE\", True),\n",
    "    #(\"ANN\", \"SMOTE\", True),\n",
    "    (\"SVM\", \"SMOTE\", True),\n",
    "]\n",
    "\n",
    "# Run experiments\n",
    "all_results = []\n",
    "\n",
    "for model_name, sampling_method, remove_outliers in experiments:\n",
    "    try:\n",
    "        result = run_grid_search(\n",
    "            model_name=model_name,\n",
    "            sampling_method=sampling_method,\n",
    "            remove_outliers=remove_outliers,\n",
    "            model_saver=model_saver\n",
    "        )\n",
    "        if result is not None:  # Only append if result is not None\n",
    "            all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error running {model_name} with {sampling_method}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f9c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*100}\")\n",
    "print(\"BEST OVERALL RESULT (FROM ALL RUNS)\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# Load existing results from CSV to find truly best overall result\n",
    "existing_csv = \"reports/03_model_testing_results/04_model_comparison_best_models.csv\"\n",
    "if os.path.exists(existing_csv):\n",
    "    df_all_results = pd.read_csv(existing_csv)\n",
    "\n",
    "    if len(df_all_results) > 0:\n",
    "        best_idx = df_all_results['test_f1_macro'].idxmax()\n",
    "        best_result = df_all_results.loc[best_idx]\n",
    "        print(f\"Best overall model: {best_result['model']}\")\n",
    "        print(f\"Sampling Method: {best_result['sampling_method']}\")\n",
    "        print(f\"Validation (test_*) F1-Macro: {best_result['test_f1_macro']:.4f}\")\n",
    "        print(f\"Validation (test_*) Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "        print(f\"Best CV Score: {best_result['best_cv_score']:.4f}\")\n",
    "        print(f\"Best Parameters: {best_result['best_parameters']}\")\n",
    "    else:\n",
    "        print(\"No valid results found in existing CSV.\")\n",
    "else:\n",
    "    print(\"No existing results CSV found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f7ac70",
   "metadata": {},
   "source": [
    "### 5. Script to re-generate results based on created models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "from src.utils.model_saver import create_model_saver\n",
    "from src.utils.preprocessing import (\n",
    "    load_processed_dataset,\n",
    "    build_full_suffix as pp_build_full_suffix,   # if not exported, import as in your notebook (pp_build_full_suffix alias)\n",
    "    generate_all_processed_datasets,\n",
    ")\n",
    "\n",
    "# Paths and config\n",
    "results_csv = \"reports/03_model_testing_results/04_model_comparison_best_models.csv\"\n",
    "DATA_DIR = \"data/processed/mitbih\"\n",
    "\n",
    "# The experiments you previously created (match what you trained)\n",
    "experiments = [\n",
    "    # Without outlier removal\n",
    "    #(\"XGBoost\", \"No_Sampling\", False),\n",
    "    #(\"ANN\", \"No_Sampling\", False),\n",
    "    (\"SVM\", \"No_Sampling\", False),\n",
    "\n",
    "    # With outlier removal\n",
    "    #(\"XGBoost\", \"No_Sampling\", True),\n",
    "    #(\"ANN\", \"No_Sampling\", True),\n",
    "    (\"SVM\", \"No_Sampling\", True),\n",
    "\n",
    "    # With sampling (no outlier removal)\n",
    "    #(\"XGBoost\", \"SMOTE\", False),\n",
    "    #(\"ANN\", \"SMOTE\", False),\n",
    "    (\"SVM\", \"SMOTE\", False),\n",
    "\n",
    "    # With sampling (with outlier removal)\n",
    "    #(\"XGBoost\", \"SMOTE\", True),\n",
    "    #(\"ANN\", \"SMOTE\", True),\n",
    "    (\"SVM\", \"SMOTE\", True),\n",
    "]\n",
    "\n",
    "# Create/access model saver in the same location as before\n",
    "model_saver = create_model_saver(\"src/models/best_simple_models_testing\")\n",
    "\n",
    "def prepare_no_leak_data(remove_outliers: bool):\n",
    "    \"\"\"\n",
    "    Load base processed data (no sampling) for training and validation.\n",
    "    Sampling (if any) is inside the saved pipeline, so we use unsampled data.\n",
    "    \"\"\"\n",
    "    generate_all_processed_datasets(data_dir=DATA_DIR, only_once=True)\n",
    "    full_suffix = pp_build_full_suffix(\"No_Sampling\", remove_outliers)\n",
    "    split = load_processed_dataset(data_dir=DATA_DIR, sampling_suffix=full_suffix)\n",
    "\n",
    "    X_train = split.X_train.values\n",
    "    y_train = split.y_train.values\n",
    "    X_val = split.X_val.values if split.X_val is not None else None\n",
    "    y_val = split.y_val.values if split.y_val is not None else None\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def evaluate_and_append(model_name: str, sampling_method: str, remove_outliers: bool):\n",
    "    \"\"\"\n",
    "    Load saved GridSearchCV, re-fit best pipeline on full train, evaluate on held-out val,\n",
    "    and append a row with per-class F1 to the CSV with the standardized schema.\n",
    "    \"\"\"\n",
    "    experiment_name = f\"{sampling_method.lower()}_outliers_{remove_outliers}\"\n",
    "    if not model_saver.model_exists(model_name, experiment_name):\n",
    "        print(f\"Skipping {model_name} / {experiment_name}: model not found.\")\n",
    "        return\n",
    "\n",
    "    # Load model (GridSearchCV)\n",
    "    gs = model_saver.load_model(model_name, experiment_name)\n",
    "    best_model = gs.best_estimator_\n",
    "\n",
    "    # Prepare data\n",
    "    X_train, y_train, X_val, y_val = prepare_no_leak_data(remove_outliers)\n",
    "    if X_val is None or y_val is None:\n",
    "        print(f\"No validation split found for {model_name} / {experiment_name}; writing row without test_* metrics.\")\n",
    "        row = {\n",
    "            \"sampling_method\": sampling_method,\n",
    "            \"model\": model_name,\n",
    "            \"test_accuracy\": None,\n",
    "            \"test_f1_macro\": None,\n",
    "            \"best_cv_score\": round(float(gs.best_score_), 4),\n",
    "            \"best_parameters\": json.dumps(gs.best_params_),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "        header = not os.path.exists(results_csv)\n",
    "        pd.DataFrame([row]).to_csv(results_csv, mode=\"a\", index=False, header=header)\n",
    "        return\n",
    "\n",
    "    # Re-fit on full training set and evaluate on held-out validation\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_val)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_val, y_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    labels = np.unique(np.concatenate([y_train, y_val]))\n",
    "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
    "        y_val, y_pred, average=None, labels=labels, zero_division=0\n",
    "    )\n",
    "    _ = confusion_matrix(y_val, y_pred, labels=labels)  # kept if you want to save later\n",
    "\n",
    "    # Append row in standardized schema (matches model_comparison_with_sampling_randomized_search.csv)\n",
    "    row = {\n",
    "        \"sampling_method\": sampling_method,\n",
    "        \"model\": model_name,\n",
    "        \"test_accuracy\": round(float(accuracy), 4),\n",
    "        \"test_f1_macro\": round(float(f1_macro), 4),\n",
    "        \"best_cv_score\": round(float(gs.best_score_), 4),\n",
    "        \"best_parameters\": json.dumps(gs.best_params_),\n",
    "    }\n",
    "    for lbl, f1 in zip(labels, f1_per_class):\n",
    "        row[f\"test_f1_cls_{lbl}\"] = round(float(f1), 2)\n",
    "\n",
    "    os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "    header = not os.path.exists(results_csv)\n",
    "    pd.DataFrame([row]).to_csv(results_csv, mode=\"a\", index=False, header=header)\n",
    "    print(f\"Wrote result for {model_name} / {experiment_name}\")\n",
    "\n",
    "# Run re-evaluation for all saved experiments\n",
    "for model_name, sampling_method, remove_outliers in experiments:\n",
    "    evaluate_and_append(model_name, sampling_method, remove_outliers)\n",
    "\n",
    "print(\"Done rebuilding CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f9100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e87a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
