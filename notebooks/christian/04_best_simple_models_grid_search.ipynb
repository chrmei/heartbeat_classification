{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14eab3c2",
   "metadata": {},
   "source": [
    "# 4 Find best \"simple\" model for given dataset\n",
    "\n",
    "This part provides a pipeline for heartbeat classification based on the requirements from `notebooks/03_model_testing_example_mit.ipynb`.\n",
    "\n",
    "Steps:\n",
    "1. Use train/validation datasets created beforehand and shared in group\n",
    "2. GridSearchCV with optimized parameter spaces, based on the previous Notebook\n",
    "3. Target models: XGBoost, ANN, SVM\n",
    "4. GridSearch with and without outlier removal\n",
    "5. RepeatedStratifiedKFold cross-validation\n",
    "6. Leak-free scaling using Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64239f57",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../..')\n",
    "os.chdir('../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Sampling\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Custom utilities\n",
    "from src.utils.preprocessing import (\n",
    "    load_processed_dataset,\n",
    "    DatasetSplit,\n",
    "    build_full_suffix as pp_build_full_suffix,\n",
    "    generate_all_processed_datasets,\n",
    ")\n",
    "from src.utils.evaluation import eval_model\n",
    "from src.utils.model_saver import create_model_saver\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3a6a8",
   "metadata": {},
   "source": [
    "## 2. Constants & Param Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "SCORING = {'f1_macro': 'f1_macro', 'bal_acc': 'balanced_accuracy', 'f1_weighted': 'f1_weighted'}\n",
    "results_csv = \"reports/03_model_testing_results/model_comparison_best_models.csv\"\n",
    "\n",
    "PARAM_SPACES = {\n",
    "    \"XGBoost\": {\n",
    "        \"estimator\": xgb.XGBClassifier(\n",
    "            objective=\"multi:softmax\",\n",
    "            num_class=5,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=\"mlogloss\",\n",
    "        ),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [150, 200, 250],\n",
    "            \"max_depth\": [8, 9],\n",
    "            \"learning_rate\": [0.2],\n",
    "            \"subsample\": [0.7, 0.8],\n",
    "            \"colsample_bytree\": [0.9],\n",
    "            \"reg_alpha\": [0.1, 0.2],\n",
    "            \"reg_lambda\": [0.0, 0.05],\n",
    "            \"min_child_weight\": [5],\n",
    "            \"gamma\": [0.0, 0.05],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=RANDOM_STATE),\n",
    "        \"needs_scaling\": False,\n",
    "    },\n",
    "    \"ANN\": {\n",
    "        \"estimator\": MLPClassifier(\n",
    "            max_iter=300,\n",
    "            early_stopping=True,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_iter_no_change=10,\n",
    "            solver=\"adam\",\n",
    "        ),\n",
    "        \"params\": {\n",
    "            \"hidden_layer_sizes\": [(128, 64)],\n",
    "            \"activation\": [\"relu\"],\n",
    "            \"alpha\": [3e-4],\n",
    "            \"learning_rate_init\": [0.001, 0.0015],\n",
    "            \"batch_size\": [96, 128],\n",
    "            \"beta_1\": [0.9, 0.91],\n",
    "            \"beta_2\": [0.97, 0.974],\n",
    "            \"validation_fraction\": [0.1],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
    "        \"needs_scaling\": True,\n",
    "    },\n",
    "    # best: {'clf__kernel': 'rbf', 'clf__gamma': 0,5, 'clf__C': 10}\n",
    "    \"SVM\": {\n",
    "        \"estimator\": SVC(),\n",
    "        \"params\": {\n",
    "            \"kernel\": [\"rbf\"],\n",
    "            \"C\": [10],\n",
    "            \"gamma\": [0.4, 0.5, 0.6],\n",
    "        },\n",
    "        \"cv\": RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_STATE),\n",
    "        \"needs_scaling\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "DATA_DIR = \"data/processed/mitbih\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13354270",
   "metadata": {},
   "source": [
    "## 3. Methods used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leak_free_pipeline(model_name: str, estimator, needs_scaling: bool = True) -> Pipeline:\n",
    "    \"\"\"Create a leak-free pipeline with scaling if needed.\"\"\"\n",
    "    if needs_scaling:\n",
    "        return Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', estimator)\n",
    "        ])\n",
    "    else:\n",
    "        return Pipeline([\n",
    "            ('classifier', estimator)\n",
    "        ])\n",
    "\n",
    "\n",
    "def prepare_dataset_with_sampling(\n",
    "    data_dir: str = DATA_DIR,\n",
    "    sampling_method: str = \"No_Sampling\",\n",
    "    remove_outliers: bool = False\n",
    ") -> Tuple[np.ndarray, Optional[np.ndarray], np.ndarray, Optional[np.ndarray]]:\n",
    "    \"\"\"Load an existing processed dataset for the given configuration.\n",
    "\n",
    "    Datasets are assumed to be pre-generated by preprocessing utilities. This\n",
    "    function never overwrites or generates new data; it only loads.\n",
    "    \"\"\"\n",
    "    # Ensure all datasets are generated once (no-op if already done)\n",
    "    generate_all_processed_datasets(data_dir=data_dir, only_once=True)\n",
    "\n",
    "    full_suffix = pp_build_full_suffix(sampling_method, remove_outliers)\n",
    "    split = load_processed_dataset(data_dir=data_dir, sampling_suffix=full_suffix)\n",
    "\n",
    "    X_train_res = split.X_train.values\n",
    "    y_train_res = split.y_train.values\n",
    "    X_val = split.X_val.values if split.X_val is not None else None\n",
    "    y_val = split.y_val.values if split.y_val is not None else None\n",
    "\n",
    "    return X_train_res, X_val, y_train_res, y_val\n",
    "\n",
    "\n",
    "def run_grid_search(\n",
    "    model_name: str,\n",
    "    sampling_method: str = \"No_Sampling\",\n",
    "    remove_outliers: bool = False,\n",
    "    model_saver=None,\n",
    "    results_dir: str = \"reports/comprehensive_model_testing\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run GridSearchCV for a specific model and sampling method.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to train\n",
    "        sampling_method: Sampling method to use\n",
    "        remove_outliers: Whether to remove outliers\n",
    "        model_saver: Model saver instance\n",
    "        results_dir: Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running GridSearchCV for {model_name} with {sampling_method}\")\n",
    "    print(f\"Outlier removal: {remove_outliers}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get model configuration\n",
    "    model_config = PARAM_SPACES[model_name]\n",
    "    estimator = model_config[\"estimator\"]\n",
    "    params = model_config[\"params\"]\n",
    "    cv = model_config[\"cv\"]\n",
    "    needs_scaling = model_config[\"needs_scaling\"]\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_val, y_train, y_val = prepare_dataset_with_sampling(\n",
    "        sampling_method=sampling_method,\n",
    "        remove_outliers=remove_outliers\n",
    "    )\n",
    "    \n",
    "    # Create leak-free pipeline\n",
    "    pipeline = create_leak_free_pipeline(model_name, estimator, needs_scaling)\n",
    "    \n",
    "    # Adjust parameter names for pipeline\n",
    "    pipeline_params = {}\n",
    "    for param_name, param_values in params.items():\n",
    "        pipeline_params[f'classifier__{param_name}'] = param_values\n",
    "    \n",
    "    # Create experiment name\n",
    "    experiment_name = f\"{sampling_method.lower()}_outliers_{remove_outliers}\"\n",
    "    \n",
    "    # Check if model already exists\n",
    "    if model_saver and model_saver.model_exists(model_name, experiment_name):\n",
    "        print(f\"Model {model_name} already exists for experiment {experiment_name}. Skipping training and CSV append.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Training new model for {model_name}...\")\n",
    "        \n",
    "        # Run GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=pipeline_params,\n",
    "            scoring=SCORING,\n",
    "            refit='f1_macro',\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            verbose=3\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Save model if saver is provided\n",
    "        if model_saver:\n",
    "            metadata = {\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'best_score': grid_search.best_score_,\n",
    "                'cv_results': grid_search.cv_results_,\n",
    "                'experiment': experiment_name,\n",
    "                'classifier': model_name,\n",
    "                'sampling_method': sampling_method,\n",
    "                'remove_outliers': remove_outliers,\n",
    "            }\n",
    "            model_saver.save_model(model_name, grid_search, experiment_name, metadata)\n",
    "            print(f\"Model {model_name} saved successfully!\")\n",
    "    \n",
    "    # Evaluate on validation set if available\n",
    "    if X_val is not None and y_val is not None:\n",
    "        print(f\"Evaluating {model_name} on validation set...\")\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # For evaluation, we need to fit the model again since pipeline might not be fitted\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = best_model.predict(X_val)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "            y_val, y_pred, average='macro', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Per-class metrics\n",
    "        labels = np.unique(np.concatenate([y_train, y_val]))\n",
    "        precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
    "            y_val, y_pred, average=None, labels=labels, zero_division=0\n",
    "        )\n",
    "        \n",
    "        confusion_mat = confusion_matrix(y_val, y_pred, labels=labels)\n",
    "        \n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'sampling_method': sampling_method,\n",
    "            'remove_outliers': remove_outliers,\n",
    "            'best_cv_score': grid_search.best_score_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'validation_accuracy': accuracy,\n",
    "            'validation_f1_macro': f1_macro,\n",
    "            'validation_precision_macro': precision_macro,\n",
    "            'validation_recall_macro': recall_macro,\n",
    "            'validation_f1_per_class': f1_per_class,\n",
    "            'validation_precision_per_class': precision_per_class,\n",
    "            'validation_recall_per_class': recall_per_class,\n",
    "            'validation_support_per_class': support_per_class,\n",
    "            'confusion_matrix': confusion_mat,\n",
    "            'labels': labels,\n",
    "        }\n",
    "        \n",
    "        print(f\"Validation F1-Macro: {f1_macro:.4f}\")\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Append to canonical results CSV (only for newly trained models)\n",
    "        row = {\n",
    "            'model': model_name,\n",
    "            'sampling_method': sampling_method,\n",
    "            'remove_outliers': remove_outliers,\n",
    "            'val_accuracy': round(float(accuracy), 4),\n",
    "            'val_f1_macro': round(float(f1_macro), 4),\n",
    "            'best_cv_score': round(float(grid_search.best_score_), 4),\n",
    "            'best_parameters': json.dumps(grid_search.best_params_),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "        header = not os.path.exists(results_csv)\n",
    "        pd.DataFrame([row]).to_csv(results_csv, mode='a', index=False, header=header)\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        print(\"No validation set available for evaluation\")\n",
    "        # Append limited info to CSV\n",
    "        \n",
    "        row = {\n",
    "            'model': model_name,\n",
    "            'sampling_method': sampling_method,\n",
    "            'remove_outliers': remove_outliers,\n",
    "            'val_accuracy': None,\n",
    "            'val_f1_macro': None,\n",
    "            'best_cv_score': round(float(grid_search.best_score_), 4),\n",
    "            'best_parameters': json.dumps(grid_search.best_params_),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "        header = not os.path.exists(results_csv)\n",
    "        pd.DataFrame([row]).to_csv(results_csv, mode='a', index=False, header=header)\n",
    "\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'sampling_method': sampling_method,\n",
    "            'remove_outliers': remove_outliers,\n",
    "            'best_cv_score': grid_search.best_score_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b47a3",
   "metadata": {},
   "source": [
    "## 4. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e765e084",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, sampling_method, remove_outliers \u001b[38;5;129;01min\u001b[39;00m experiments:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         result = \u001b[43mrun_grid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m            \u001b[49m\u001b[43msampling_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m            \u001b[49m\u001b[43mremove_outliers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_outliers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_saver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_saver\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Only append if result is not None\u001b[39;00m\n\u001b[32m     45\u001b[39m             all_results.append(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mrun_grid_search\u001b[39m\u001b[34m(model_name, sampling_method, remove_outliers, model_saver, results_dir)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Run GridSearchCV\u001b[39;00m\n\u001b[32m     95\u001b[39m grid_search = GridSearchCV(\n\u001b[32m     96\u001b[39m     estimator=pipeline,\n\u001b[32m     97\u001b[39m     param_grid=pipeline_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m     verbose=\u001b[32m3\u001b[39m\n\u001b[32m    103\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Save model if saver is provided\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_saver:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1145\u001b[39m     estimator._validate_params()\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1148\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1149\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1150\u001b[39m     )\n\u001b[32m   1151\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:898\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, groups, **fit_params)\u001b[39m\n\u001b[32m    892\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m    893\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m    894\u001b[39m     )\n\u001b[32m    896\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m--> \u001b[39m\u001b[32m898\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m    901\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m    902\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1422\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1421\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:845\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    838\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    839\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    840\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    841\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    842\u001b[39m         )\n\u001b[32m    843\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m845\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    863\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    864\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    865\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    866\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    867\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     60\u001b[39m config = get_config()\n\u001b[32m     61\u001b[39m iterable_with_config = (\n\u001b[32m     62\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     64\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1946\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   1947\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   1948\u001b[39m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[32m   1949\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   1950\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1952\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1592\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1594\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1595\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1597\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1598\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1599\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1600\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1601\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Repos/heartbeat_classification/.venv/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1702\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1703\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1705\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1706\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1707\u001b[39m     time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1708\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1710\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1711\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1712\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting Model Testing\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize model saver\n",
    "# Change to project root directory\n",
    "\n",
    "model_saver = create_model_saver(\"src/models/best_simple_models_testing\")\n",
    "\n",
    "# Define experiments to run\n",
    "experiments = [\n",
    "    # Without outlier removal\n",
    "    \n",
    "    (\"XGBoost\", \"No_Sampling\", False),\n",
    "    (\"ANN\", \"No_Sampling\", False),\n",
    "    #(\"SVM\", \"No_Sampling\", False),\n",
    "    \n",
    "    # With outlier removal\n",
    "    (\"XGBoost\", \"No_Sampling\", True),\n",
    "    (\"ANN\", \"No_Sampling\", True),\n",
    "    #(\"SVM\", \"No_Sampling\", True),\n",
    "    \n",
    "    # With sampling (no outlier removal)\n",
    "    (\"XGBoost\", \"SMOTE\", False),\n",
    "    (\"ANN\", \"SMOTE\", False),\n",
    "    #(\"SVM\", \"SMOTE\", False),\n",
    "    \n",
    "    # With sampling (with outlier removal)\n",
    "    (\"XGBoost\", \"SMOTE\", True),\n",
    "    (\"ANN\", \"SMOTE\", True),\n",
    "    #(\"SVM\", \"SMOTE\", True),\n",
    "]\n",
    "\n",
    "# Run experiments\n",
    "all_results = []\n",
    "\n",
    "for model_name, sampling_method, remove_outliers in experiments:\n",
    "    try:\n",
    "        result = run_grid_search(\n",
    "            model_name=model_name,\n",
    "            sampling_method=sampling_method,\n",
    "            remove_outliers=remove_outliers,\n",
    "            model_saver=model_saver\n",
    "        )\n",
    "        if result is not None:  # Only append if result is not None\n",
    "            all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error running {model_name} with {sampling_method}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f9c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*100}\")\n",
    "print(\"BEST OVERALL RESULT (FROM ALL RUNS)\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# Load existing results from CSV to find truly best overall result\n",
    "existing_csv = \"reports/03_model_testing_results/model_comparison_best_models.csv\"\n",
    "if os.path.exists(existing_csv):\n",
    "    df_all_results = pd.read_csv(existing_csv)\n",
    "    # Remove rows with missing validation scores\n",
    "\n",
    "    if len(df_all_results) > 0:\n",
    "        best_idx = df_all_results['val_f1_macro'].idxmax()\n",
    "        best_result = df_all_results.loc[best_idx]\n",
    "        print(f\"Best overall model: {best_result['model']}\")\n",
    "        print(f\"Sampling Method: {best_result['sampling_method']}\")\n",
    "        print(f\"Outlier Removal: {best_result['remove_outliers']}\")\n",
    "        print(f\"Validation F1-Macro: {best_result['val_f1_macro']:.4f}\")\n",
    "        print(f\"Validation Accuracy: {best_result['val_accuracy']:.4f}\")\n",
    "        print(f\"Best CV Score: {best_result['best_cv_score']:.4f}\")\n",
    "        print(f\"Best Parameters: {best_result['best_parameters']}\")\n",
    "    else:\n",
    "        print(\"No valid results found in existing CSV.\")\n",
    "else:\n",
    "    print(\"No existing results CSV found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f432d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f9100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
