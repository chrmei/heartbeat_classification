{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92072ab0",
   "metadata": {},
   "source": [
    "# CNN Model - 2018 Paper (Kachuee, Fazeli, Sarrafzadeh), original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b4f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 12:59:59.308019: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-30 12:59:59.353115: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-30 13:00:00.765012: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 13:00:01.580839: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Conv1D, MaxPooling1D, Flatten, Add, ReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "print(tf.config.list_physical_devices('GPU'))  # should show []\n",
    "from contextlib import redirect_stdout\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d207cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289885, 187, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(17511, 187, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(21892, 187, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SAMPLING_METHOD = \"SMOTE\"\n",
    "REMOVE_OUTLIERS = False\n",
    "model_name = \"cnn6_sm_lr_bs\"\n",
    "OUTPUT_PATH = \"src/models/CNN/\"\n",
    "results_csv = \"reports/03_model_testing_results/05_CNN_model_comparison.csv\"\n",
    "EPOCHS = 50\n",
    "\n",
    "#import MIT data\n",
    "df_mitbih_test = pd.read_csv('data/original/mitbih_test.csv', header = None)\n",
    "\n",
    "X_train = pd.read_csv('data/processed/mitbih/X_train.csv')\n",
    "y_train = pd.read_csv('data/processed/mitbih/y_train.csv')\n",
    "y_train = y_train['187']\n",
    "\n",
    "X_train_sm = pd.read_csv('data/processed/mitbih/X_train_sm.csv')\n",
    "y_train_sm = pd.read_csv('data/processed/mitbih/y_train_sm.csv')\n",
    "y_train_sm = y_train_sm['187']\n",
    "\n",
    "X_val = pd.read_csv('data/processed/mitbih/X_val.csv')\n",
    "y_val = pd.read_csv('data/processed/mitbih/y_val.csv')\n",
    "y_val = y_val['187']\n",
    "\n",
    "X_test = df_mitbih_test.drop(187, axis = 1)\n",
    "y_test = df_mitbih_test[187]\n",
    "\n",
    "\n",
    "# Reshape the data for 1D CNN\n",
    "X_train_sm_cnn = np.expand_dims(X_train_sm, axis=2)\n",
    "X_val_cnn = np.expand_dims(X_val, axis=2)\n",
    "X_test_cnn = np.expand_dims(X_test, axis=2) \n",
    "\n",
    "display(X_train_sm_cnn.shape)\n",
    "display(X_val_cnn.shape)\n",
    "display(X_test_cnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a02984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot and save validation accuracy and validation loss over epochs from history\n",
    "def plot_training_history(history, save_dir, prefix): \n",
    "    hist = history.history\n",
    "    metrics = [m for m in hist.keys() if not m.startswith('val_')]  \n",
    "\n",
    "    # Create the output folder if it does not exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for m in metrics:\n",
    "        plt.figure()\n",
    "        plt.plot(hist[m], label=f'Train {m}')\n",
    "        if f'val_{m}' in hist:\n",
    "            plt.plot(hist[f'val_{m}'], label=f'Val {m}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(m)\n",
    "        plt.title(f'{m} over epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Construct filename with prefix and filepath with directory and filename\n",
    "        filename = f\"{prefix}_{m}.png\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save the figure\n",
    "        plt.savefig(filepath, format='png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {filepath}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac73b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN6, Paper 2018\n",
    "# Input layer\n",
    "input_layer = Input(shape=(187, 1))\n",
    "\n",
    "conv_0 = Conv1D(filters=32, kernel_size=5, padding='same')(input_layer)\n",
    "\n",
    "# First Residual Block\n",
    "conv1_1 = Conv1D(filters=32, kernel_size=5, padding='same')(conv_0)\n",
    "relu1_1 = ReLU()(conv1_1)\n",
    "conv2_1 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_1)\n",
    "skip_connection_1 = Add()([conv_0, conv2_1])\n",
    "relu2_1 = ReLU()(skip_connection_1)\n",
    "pool_1 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_1)\n",
    "\n",
    "# Second Residual Block\n",
    "conv1_2 = Conv1D(filters=32, kernel_size=5, padding='same')(pool_1)\n",
    "relu1_2 = ReLU()(conv1_2)\n",
    "conv2_2 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_2)\n",
    "skip_connection_2 = Add()([pool_1, conv2_2])\n",
    "relu2_2 = ReLU()(skip_connection_2)\n",
    "pool_2 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_2)\n",
    "\n",
    "# Third Residual Block\n",
    "conv1_3 = Conv1D(filters=32, kernel_size=5, padding='same')(pool_2)\n",
    "relu1_3 = ReLU()(conv1_3)\n",
    "conv2_3 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_3)\n",
    "skip_connection_3 = Add()([pool_2, conv2_3])\n",
    "relu2_3 = ReLU()(skip_connection_3)\n",
    "pool_3 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_3)\n",
    "\n",
    "# Fourth Residual Block\n",
    "conv1_4 = Conv1D(filters=32, kernel_size=5, padding='same')(pool_3)\n",
    "relu1_4 = ReLU()(conv1_4)\n",
    "conv2_4 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_4)\n",
    "skip_connection_4 = Add()([pool_3, conv2_4])\n",
    "relu2_4 = ReLU()(skip_connection_4)\n",
    "pool_4 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_4)\n",
    "\n",
    "# Fifth Residual Block\n",
    "conv1_5 = Conv1D(filters=32, kernel_size=5, padding='same')(pool_4)\n",
    "relu1_5 = ReLU()(conv1_5)\n",
    "conv2_5 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_5)\n",
    "skip_connection_5 = Add()([pool_4, conv2_5])\n",
    "relu2_5 = ReLU()(skip_connection_5)\n",
    "pool_5 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_5)\n",
    "\n",
    "# Fully connected layers\n",
    "flatten = Flatten()(pool_5)\n",
    "fc1 = Dense(32, activation='relu')(flatten)\n",
    "output_layer = Dense(5, activation='softmax')(fc1)\n",
    "\n",
    "\n",
    "# Model\n",
    "cnn6 = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe54c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate with exponential decay\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.75,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Adam optimizer with specified hyperparameters\n",
    "optimizer = Adam(\n",
    "    learning_rate=lr_schedule,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "cnn6.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# Define where and how to save the best model\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=OUTPUT_PATH+model_name+'_bs_epoch_{epoch:02d}_valloss_{val_loss:.4f}.keras',   # file path (can be .keras or .h5)\n",
    "    monitor='val_loss',        # metric to monitor\n",
    "    mode='min',                    # because higher accuracy is better\n",
    "    save_best_only=True,           # only save when val_accuracy improves\n",
    "    verbose=1                      # print message when a model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn6.fit(\n",
    "    X_train_sm_cnn,\n",
    "    y_train_sm,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_cnn, y_val),  \n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce47d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save training history\n",
    "import json \n",
    "SAMPLING_METHOD = \"SMOTE\"\n",
    "REMOVE_OUTLIERS = False\n",
    "model_name = \"cnn6_sm_lr_bs\"\n",
    "OUTPUT_PATH = \"src/models/CNN/\"\n",
    "results_csv = \"reports/03_model_testing_results/05_CNN_model_comparison.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a7747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4.]\n",
      "src/models/CNN/cnn6_sm_lr_bs.keras\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9925    0.9940    0.9933     18118\n",
      "         1.0     0.8706    0.8471    0.8587       556\n",
      "         2.0     0.9674    0.9627    0.9650      1448\n",
      "         3.0     0.7826    0.7778    0.7802       162\n",
      "         4.0     0.9956    0.9925    0.9941      1608\n",
      "\n",
      "    accuracy                         0.9865     21892\n",
      "   macro avg     0.9217    0.9148    0.9183     21892\n",
      "weighted avg     0.9864    0.9865    0.9865     21892\n",
      "\n",
      "Predictions      0    1     2    3     4\n",
      "187                                     \n",
      "0.0          18010   64    25   14     5\n",
      "1.0             77  471     5    2     1\n",
      "2.0             29    5  1394   19     1\n",
      "3.0             19    1    16  126     0\n",
      "4.0             11    0     1    0  1596\n",
      "src/models/CNN/cnn6_sm_lr_bs_bs_epoch_01_valloss_0.2328.keras\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9948    0.9104    0.9507     18118\n",
      "         1.0     0.3512    0.8849    0.5028       556\n",
      "         2.0     0.8445    0.9565    0.8970      1448\n",
      "         3.0     0.2214    0.9198    0.3569       162\n",
      "         4.0     0.9787    0.9720    0.9754      1608\n",
      "\n",
      "    accuracy                         0.9174     21892\n",
      "   macro avg     0.6781    0.9287    0.7366     21892\n",
      "weighted avg     0.9616    0.9174    0.9332     21892\n",
      "\n",
      "Predictions      0    1     2    3     4\n",
      "187                                     \n",
      "0.0          16495  884   227  481    31\n",
      "1.0             46  492     9    9     0\n",
      "2.0             14   17  1385   30     2\n",
      "3.0              2    0    10  149     1\n",
      "4.0             24    8     9    4  1563\n",
      "src/models/CNN/cnn6_sm_lr_bs_bs_epoch_02_valloss_0.1489.keras\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9950    0.9461    0.9699     18118\n",
      "         1.0     0.4198    0.8993    0.5724       556\n",
      "         2.0     0.9194    0.9537    0.9363      1448\n",
      "         3.0     0.4205    0.9136    0.5759       162\n",
      "         4.0     0.9784    0.9851    0.9817      1608\n",
      "\n",
      "    accuracy                         0.9480     21892\n",
      "   macro avg     0.7466    0.9395    0.8072     21892\n",
      "weighted avg     0.9699    0.9480    0.9555     21892\n",
      "\n",
      "Predictions      0    1     2    3     4\n",
      "187                                     \n",
      "0.0          17141  672    97  175    33\n",
      "1.0             43  500    10    1     2\n",
      "2.0             27   12  1381   28     0\n",
      "3.0              5    3     6  148     0\n",
      "4.0             12    4     8    0  1584\n",
      "src/models/CNN/cnn6_sm_lr_bs_bs_epoch_03_valloss_0.1099.keras\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9948    0.9672    0.9808     18118\n",
      "         1.0     0.5796    0.8777    0.6981       556\n",
      "         2.0     0.9193    0.9599    0.9392      1448\n",
      "         3.0     0.4565    0.9074    0.6074       162\n",
      "         4.0     0.9894    0.9851    0.9872      1608\n",
      "\n",
      "    accuracy                         0.9653     21892\n",
      "   macro avg     0.7879    0.9395    0.8426     21892\n",
      "weighted avg     0.9749    0.9653    0.9686     21892\n",
      "\n",
      "Predictions      0    1     2    3     4\n",
      "187                                     \n",
      "0.0          17524  345    95  141    13\n",
      "1.0             55  488     9    3     1\n",
      "2.0             23    3  1390   29     3\n",
      "3.0              4    2     9  147     0\n",
      "4.0              9    4     9    2  1584\n",
      "src/models/CNN/cnn6_sm_lr_bs_bs_epoch_04_valloss_0.0960.keras\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9953    0.9699    0.9824     18118\n",
      "         1.0     0.5617    0.8759    0.6845       556\n",
      "         2.0     0.9169    0.9675    0.9415      1448\n",
      "         3.0     0.7136    0.8765    0.7867       162\n",
      "         4.0     0.9720    0.9925    0.9822      1608\n",
      "\n",
      "    accuracy                         0.9683     21892\n",
      "   macro avg     0.8319    0.9365    0.8755     21892\n",
      "weighted avg     0.9753    0.9683    0.9707     21892\n",
      "\n",
      "Predictions      0    1     2    3     4\n",
      "187                                     \n",
      "0.0          17573  366   103   33    43\n",
      "1.0             54  487    11    3     1\n",
      "2.0             16    9  1401   21     1\n",
      "3.0              8    3     8  142     1\n",
      "4.0              5    2     5    0  1596\n",
      "src/models/CNN/cnn6_sm_lr_bs_bs_epoch_05_valloss_0.0701.keras\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9931    0.9842    0.9886     18118\n",
      "         1.0     0.6999    0.8597    0.7716       556\n",
      "         2.0     0.9488    0.9606    0.9547      1448\n",
      "         3.0     0.7586    0.8148    0.7857       162\n",
      "         4.0     0.9845    0.9888    0.9867      1608\n",
      "\n",
      "    accuracy                         0.9785     21892\n",
      "   macro avg     0.8770    0.9216    0.8975     21892\n",
      "weighted avg     0.9804    0.9785    0.9792     21892\n",
      "\n",
      "Predictions      0    1     2    3     4\n",
      "187                                     \n",
      "0.0          17831  195    49   22    21\n",
      "1.0             71  478     6    0     1\n",
      "2.0             28    6  1391   20     3\n",
      "3.0             16    1    13  132     0\n",
      "4.0              8    3     7    0  1590\n",
      "src/models/CNN/cnn6_sm_lr_bs_bs_epoch_07_valloss_0.0649.keras\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9932    0.9898    0.9915     18118\n",
      "         1.0     0.7774    0.8669    0.8197       556\n",
      "         2.0     0.9713    0.9572    0.9642      1448\n",
      "         3.0     0.7487    0.8827    0.8102       162\n",
      "         4.0     0.9944    0.9876    0.9910      1608\n",
      "\n",
      "    accuracy                         0.9836     21892\n",
      "   macro avg     0.8970    0.9368    0.9153     21892\n",
      "weighted avg     0.9845    0.9836    0.9840     21892\n",
      "\n",
      "Predictions      0    1     2    3     4\n",
      "187                                     \n",
      "0.0          17934  129    24   23     8\n",
      "1.0             70  482     2    2     0\n",
      "2.0             32    6  1386   23     1\n",
      "3.0             10    1     8  143     0\n",
      "4.0             11    2     7    0  1588\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re \n",
    "def parse_epoch_from_name(name, default_epochs=EPOCHS):\n",
    "    # Expect pattern like ..._epoch_12_...; returns int if found else default\n",
    "    m = re.search(r\"epoch_(\\d+)\", name)\n",
    "    return int(m.group(1)) if m else default_epochs\n",
    "\n",
    "def parse_val_loss_from_name(name):\n",
    "    # Expect pattern like ..._valloss_0.1234.keras\n",
    "    m = re.search(r\"valloss_([0-9]+\\.[0-9]+)\", name)\n",
    "    return float(m.group(1)) if m else np.nan\n",
    "\n",
    "# Safer file filtering\n",
    "model_dir = Path(OUTPUT_PATH)\n",
    "model_paths = sorted([p for p in model_dir.glob(\"*.keras\")])\n",
    "\n",
    "all_labels = np.unique(y_test)  # ground-truth labels present in test set\n",
    "rows = []\n",
    "\n",
    "print(all_labels)\n",
    "\n",
    "for p in model_paths:\n",
    "    print(p)\n",
    "    model_ = load_model(str(p))\n",
    "\n",
    "    y_pred = model_.predict(X_test_cnn)\n",
    "    y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Force consistent label space for metrics\n",
    "    print(classification_report(y_test, y_pred_class, digits=4))\n",
    "    report = classification_report(\n",
    "        y_test, y_pred_class, labels=all_labels, output_dict=True, zero_division=0\n",
    "    )\n",
    "\n",
    "    print(pd.crosstab(y_test, y_pred_class, colnames=['Predictions']))\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_class)\n",
    "    epoch_num = parse_epoch_from_name(p.name)\n",
    "    val_loss = parse_val_loss_from_name(p.name)\n",
    "\n",
    "    row = {\n",
    "        \"sampling_method\": SAMPLING_METHOD,\n",
    "        \"outliers_removed\": REMOVE_OUTLIERS,\n",
    "        \"epochs\": epoch_num,\n",
    "        \"model\": p.name,\n",
    "        \"val_loss\": round(float(val_loss), 4) if not np.isnan(val_loss) else np.nan,\n",
    "        \"test_accuracy\": round(float(accuracy), 4),\n",
    "        \"test_f1_macro\": round(float(report[\"macro avg\"][\"f1-score\"]), 4),\n",
    "        \"test_precision_macro\": round(float(report[\"macro avg\"][\"precision\"]), 4),\n",
    "        \"test_recall_macro\": round(float(report[\"macro avg\"][\"recall\"]), 4),\n",
    "        \"test_f1_weighted\": round(float(report[\"weighted avg\"][\"f1-score\"]), 4),\n",
    "        \"test_precision_weighted\": round(float(report[\"weighted avg\"][\"precision\"]), 4),\n",
    "        \"test_recall_weighted\": round(float(report[\"weighted avg\"][\"recall\"]), 4),\n",
    "    }\n",
    "    for lbl in all_labels:\n",
    "        row[f\"test_f1_cls_{int(lbl)}\"] = round(float(report[str(lbl)][\"f1-score\"]), 4)\n",
    "        row[f\"test_precision_cls_{int(lbl)}\"] = round(float(report[str(lbl)][\"precision\"]), 4)\n",
    "        row[f\"test_recall_cls_{int(lbl)}\"] = round(float(report[str(lbl)][\"recall\"]), 4)\n",
    "        row[f\"test_support_cls_{int(lbl)}\"] = int(report[str(lbl)][\"support\"])\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "df.to_csv(results_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history, \"reports/figures/training_history/\", model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
