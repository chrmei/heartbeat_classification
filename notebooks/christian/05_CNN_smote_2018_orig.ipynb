{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92072ab0",
   "metadata": {},
   "source": [
    "# CNN Model - 2018 Paper (Kachuee, Fazeli, Sarrafzadeh), original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b4f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Conv1D, MaxPooling1D, Flatten, Add, ReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "print(tf.config.list_physical_devices('GPU'))  # should show []\n",
    "from contextlib import redirect_stdout\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d207cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_METHOD = \"SMOTE\"\n",
    "REMOVE_OUTLIERS = False\n",
    "model_name = \"cnn6_sm_lr_bs\"\n",
    "OUTPUT_PATH = \"src/models/CNN/\"\n",
    "results_csv = \"reports/03_model_testing_results/05_CNN_model_comparison.csv\"\n",
    "EPOCHS = 50\n",
    "\n",
    "#import MIT data\n",
    "df_mitbih_test = pd.read_csv('data/original/mitbih_test.csv', header = None)\n",
    "\n",
    "X_train = pd.read_csv('data/processed/mitbih/X_train.csv')\n",
    "y_train = pd.read_csv('data/processed/mitbih/y_train.csv')\n",
    "y_train = y_train['187']\n",
    "\n",
    "X_train_sm = pd.read_csv('data/processed/mitbih/X_train_sm.csv')\n",
    "y_train_sm = pd.read_csv('data/processed/mitbih/y_train_sm.csv')\n",
    "y_train_sm = y_train_sm['187']\n",
    "\n",
    "X_val = pd.read_csv('data/processed/mitbih/X_val.csv')\n",
    "y_val = pd.read_csv('data/processed/mitbih/y_val.csv')\n",
    "y_val = y_val['187']\n",
    "\n",
    "X_test = df_mitbih_test.drop(187, axis = 1)\n",
    "y_test = df_mitbih_test[187]\n",
    "\n",
    "\n",
    "# Reshape the data for 1D CNN\n",
    "X_train_sm_cnn = np.expand_dims(X_train_sm, axis=2)\n",
    "X_val_cnn = np.expand_dims(X_val, axis=2)\n",
    "X_test_cnn = np.expand_dims(X_test, axis=2) \n",
    "\n",
    "display(X_train_sm_cnn.shape)\n",
    "display(X_val_cnn.shape)\n",
    "display(X_test_cnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot and save validation accuracy and validation loss over epochs from history\n",
    "def plot_training_history(history, save_dir, prefix): \n",
    "    hist = history.history\n",
    "    metrics = [m for m in hist.keys() if not m.startswith('val_')]  \n",
    "\n",
    "    # Create the output folder if it does not exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for m in metrics:\n",
    "        plt.figure()\n",
    "        plt.plot(hist[m], label=f'Train {m}')\n",
    "        if f'val_{m}' in hist:\n",
    "            plt.plot(hist[f'val_{m}'], label=f'Val {m}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(m)\n",
    "        plt.title(f'{m} over epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Construct filename with prefix and filepath with directory and filename\n",
    "        filename = f\"{prefix}_{m}.png\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "\n",
    "        # Save the figure\n",
    "        plt.savefig(filepath, format='png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {filepath}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac73b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN6, Paper 2018\n",
    "# Input layer\n",
    "input_layer = Input(shape=(187, 1))\n",
    "\n",
    "conv_0 = Conv1D(filters=32, kernel_size=5, padding='same')(input_layer)\n",
    "\n",
    "# First Residual Block\n",
    "conv1_1 = Conv1D(filters=32, kernel_size=5, padding='same')(conv_0)\n",
    "relu1_1 = ReLU()(conv1_1)\n",
    "conv2_1 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_1)\n",
    "skip_connection_1 = Add()([conv_0, conv2_1])\n",
    "relu2_1 = ReLU()(skip_connection_1)\n",
    "pool_1 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_1)\n",
    "\n",
    "# Second Residual Block\n",
    "conv1_2 = Conv1D(filters=32, kernel_size=5, padding='same')(pool_1)\n",
    "relu1_2 = ReLU()(conv1_2)\n",
    "conv2_2 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_2)\n",
    "skip_connection_2 = Add()([pool_1, conv2_2])\n",
    "relu2_2 = ReLU()(skip_connection_2)\n",
    "pool_2 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_2)\n",
    "\n",
    "# Third Residual Block\n",
    "conv1_3 = Conv1D(filters=32, kernel_size=5, padding='same')(pool_2)\n",
    "relu1_3 = ReLU()(conv1_3)\n",
    "conv2_3 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_3)\n",
    "skip_connection_3 = Add()([pool_2, conv2_3])\n",
    "relu2_3 = ReLU()(skip_connection_3)\n",
    "pool_3 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_3)\n",
    "\n",
    "# Fourth Residual Block\n",
    "conv1_4 = Conv1D(filters=32, kernel_size=5, padding='same')(pool_3)\n",
    "relu1_4 = ReLU()(conv1_4)\n",
    "conv2_4 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_4)\n",
    "skip_connection_4 = Add()([pool_3, conv2_4])\n",
    "relu2_4 = ReLU()(skip_connection_4)\n",
    "pool_4 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_4)\n",
    "\n",
    "# Fifth Residual Block\n",
    "conv1_5 = Conv1D(filters=32, kernel_size=5, padding='same')(pool_4)\n",
    "relu1_5 = ReLU()(conv1_5)\n",
    "conv2_5 = Conv1D(filters=32, kernel_size=5, padding='same')(relu1_5)\n",
    "skip_connection_5 = Add()([pool_4, conv2_5])\n",
    "relu2_5 = ReLU()(skip_connection_5)\n",
    "pool_5 = MaxPooling1D(pool_size=5, strides=2, padding='same')(relu2_5)\n",
    "\n",
    "# Fully connected layers\n",
    "flatten = Flatten()(pool_5)\n",
    "fc1 = Dense(32, activation='relu')(flatten)\n",
    "output_layer = Dense(5, activation='softmax')(fc1)\n",
    "\n",
    "\n",
    "# Model\n",
    "cnn6 = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe54c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate with exponential decay\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.75,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Adam optimizer with specified hyperparameters\n",
    "optimizer = Adam(\n",
    "    learning_rate=lr_schedule,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "cnn6.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# Define where and how to save the best model\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=OUTPUT_PATH+model_name+'_bs_epoch_{epoch:02d}_valloss_{val_loss:.4f}.keras',   # file path (can be .keras or .h5)\n",
    "    monitor='val_loss',        # metric to monitor\n",
    "    mode='min',                    # because higher accuracy is better\n",
    "    save_best_only=True,           # only save when val_accuracy improves\n",
    "    verbose=1                      # print message when a model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn6.fit(\n",
    "    X_train_sm_cnn,\n",
    "    y_train_sm,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_cnn, y_val),  \n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save training history\n",
    "import json \n",
    "SAMPLING_METHOD = \"SMOTE\"\n",
    "REMOVE_OUTLIERS = False\n",
    "model_name = \"cnn6_sm_lr_bs\"\n",
    "OUTPUT_PATH = \"src/models/CNN/\"\n",
    "results_csv = \"reports/03_model_testing_results/05_CNN_model_comparison.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re \n",
    "def parse_epoch_from_name(name, default_epochs=EPOCHS):\n",
    "    # Expect pattern like ..._epoch_12_...; returns int if found else default\n",
    "    m = re.search(r\"epoch_(\\d+)\", name)\n",
    "    return int(m.group(1)) if m else default_epochs\n",
    "\n",
    "def parse_val_loss_from_name(name):\n",
    "    # Expect pattern like ..._valloss_0.1234.keras\n",
    "    m = re.search(r\"valloss_([0-9]+\\.[0-9]+)\", name)\n",
    "    return float(m.group(1)) if m else np.nan\n",
    "\n",
    "# Safer file filtering\n",
    "model_dir = Path(OUTPUT_PATH)\n",
    "model_paths = sorted([p for p in model_dir.glob(\"*.keras\")])\n",
    "\n",
    "all_labels = np.unique(y_test)  # ground-truth labels present in test set\n",
    "rows = []\n",
    "\n",
    "print(all_labels)\n",
    "\n",
    "for p in model_paths:\n",
    "    print(p)\n",
    "    model_ = load_model(str(p))\n",
    "\n",
    "    y_pred = model_.predict(X_test_cnn)\n",
    "    y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Force consistent label space for metrics\n",
    "    print(classification_report(y_test, y_pred_class, digits=4))\n",
    "    report = classification_report(\n",
    "        y_test, y_pred_class, labels=all_labels, output_dict=True, zero_division=0\n",
    "    )\n",
    "\n",
    "    print(pd.crosstab(y_test, y_pred_class, colnames=['Predictions']))\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_class)\n",
    "    epoch_num = parse_epoch_from_name(p.name)\n",
    "    val_loss = parse_val_loss_from_name(p.name)\n",
    "\n",
    "    row = {\n",
    "        \"sampling_method\": SAMPLING_METHOD,\n",
    "        \"outliers_removed\": REMOVE_OUTLIERS,\n",
    "        \"epochs\": epoch_num,\n",
    "        \"model\": p.name,\n",
    "        \"val_loss\": round(float(val_loss), 4) if not np.isnan(val_loss) else np.nan,\n",
    "        \"test_accuracy\": round(float(accuracy), 4),\n",
    "        \"test_f1_macro\": round(float(report[\"macro avg\"][\"f1-score\"]), 4),\n",
    "        \"test_precision_macro\": round(float(report[\"macro avg\"][\"precision\"]), 4),\n",
    "        \"test_recall_macro\": round(float(report[\"macro avg\"][\"recall\"]), 4),\n",
    "        \"test_f1_weighted\": round(float(report[\"weighted avg\"][\"f1-score\"]), 4),\n",
    "        \"test_precision_weighted\": round(float(report[\"weighted avg\"][\"precision\"]), 4),\n",
    "        \"test_recall_weighted\": round(float(report[\"weighted avg\"][\"recall\"]), 4),\n",
    "    }\n",
    "    for lbl in all_labels:\n",
    "        row[f\"test_f1_cls_{int(lbl)}\"] = round(float(report[str(lbl)][\"f1-score\"]), 4)\n",
    "        row[f\"test_precision_cls_{int(lbl)}\"] = round(float(report[str(lbl)][\"precision\"]), 4)\n",
    "        row[f\"test_recall_cls_{int(lbl)}\"] = round(float(report[str(lbl)][\"recall\"]), 4)\n",
    "        row[f\"test_support_cls_{int(lbl)}\"] = int(report[str(lbl)][\"support\"])\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "df.to_csv(results_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history, \"reports/figures/training_history/\", model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
