{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Testing\n",
        "\n",
        "Questions to be answered:\n",
        "\n",
        "- Remove outliers?\n",
        "- Which Sampling method to use?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scoring = {'f1_macro': 'f1_macro', 'bal_acc': 'balanced_accuracy', 'f1_weighted': 'f1_weighted'}\n",
        "random_state = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os \n",
        "sys.path.append('..')\n",
        "\n",
        "from src.utils.preprocessing import (\n",
        "    prepare_mitbih, \n",
        "    prepare_ptbdb,\n",
        "    resample_training\n",
        ")\n",
        "from src.utils.evaluation import eval_model\n",
        "from src.visualization import plot_confusion_matrix\n",
        "from src.utils.model_saver import create_model_saver\n",
        "\n",
        "# external \n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
        "from scipy.stats import loguniform, randint, uniform\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# Samplers\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Init model saver\n",
        "model_saver = create_model_saver(\"../src/models/exploration_phase\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare datasets\n",
        "mitbih = prepare_mitbih(remove_outliers=False)\n",
        "\n",
        "print(\"MITBIH dataset prepared:\")\n",
        "print(f\"  Training size: {mitbih.X_train.shape}\")\n",
        "print(f\"  Test size: {mitbih.X_test.shape if mitbih.X_test is not None else 'None'}\")\n",
        "print(\"Note: No validation set - using train/test split only. Cross-validation handles train/val splitting.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test = mitbih.X_train.values, mitbih.X_test.values\n",
        "y_train = mitbih.y_train.astype(int).values\n",
        "y_test = mitbih.y_test.astype(int).values\n",
        "\n",
        "# Scale features using train fit only\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Param Spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_spaces = {\n",
        "    \"LogisticRegression\": {\n",
        "        \"estimator\": LogisticRegression(max_iter=10000, multi_class='multinomial', solver='lbfgs', n_jobs=-1),\n",
        "        \"params\": {\n",
        "            \"C\": loguniform(1e-3, 1e3),      # Big C = less penalty on large weights (more freedom, risk of overfitting). \n",
        "                                             # Small C = more penalty (more discipline, less overfitting).\n",
        "                                             # loguniform = means we try values spread across tiny to big scales (e.g., 0.001 up to 100), not just small steps.\n",
        "            \"penalty\": [\"l2\"], # gently pushes weights toward zero, which keeps the model simpler and more stable.\n",
        "            \"solver\": [\"lbfgs\"],\n",
        "        },\n",
        "        \"cv\": StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 5,\n",
        "        \"create_new_model\": False,\n",
        "    },\n",
        "    \"KNN\": {\n",
        "        \"estimator\": KNeighborsClassifier(n_jobs=-1),\n",
        "        \"params\": {\n",
        "            \"n_neighbors\": randint(1, 51),\n",
        "            \"weights\": [\"uniform\", \"distance\"],\n",
        "            \"metric\": [\"minkowski\", \"manhattan\", \"euclidean\"],\n",
        "            \"p\": [1, 2],\n",
        "        },\n",
        "        \"cv\": StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 5,\n",
        "    },\n",
        "    \"RandomForest\": {\n",
        "        \"estimator\": RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
        "        \"params\": {\n",
        "            \"n_estimators\": [100, 200, 300],\n",
        "            \"max_depth\": [10, 15, 20],\n",
        "            \"min_samples_split\": [2, 5, 10, 20, 50],\n",
        "            \"min_samples_leaf\": [1, 2, 4, 8],\n",
        "            \"max_features\": [\"sqrt\", \"log2\", None],\n",
        "            \"bootstrap\": [True],\n",
        "            \"class_weight\": [\"balanced\", None],\n",
        "            \"criterion\": [\"gini\", \"entropy\"],\n",
        "        },\n",
        "        \"cv\": StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 5,\n",
        "    },\n",
        "    \"SVM\": {\n",
        "        \"estimator\": SVC(),\n",
        "        \"params\": {\n",
        "            \"kernel\": [\"rbf\", \"poly\"],\n",
        "            \"C\": [0.1, 1, 10],\n",
        "            \"gamma\": [0.001, 0.01, 0.1, 0.5, 0.9],\n",
        "        },\n",
        "        \"cv\": StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 5,\n",
        "    },\n",
        "    \"DecisionTree\": {\n",
        "        \"estimator\": DecisionTreeClassifier(random_state=random_state),\n",
        "        \"params\": {\n",
        "            \"max_depth\": [None, 5, 10, 15, 20, 25, 30],\n",
        "            \"min_samples_split\": [2, 5, 10, 20, 50],\n",
        "            \"min_samples_leaf\": [1, 2, 4, 8, 16],\n",
        "            \"max_features\": [\"sqrt\", \"log2\", None],\n",
        "            \"criterion\": [\"gini\", \"entropy\"],\n",
        "            \"class_weight\": [\"balanced\", None],\n",
        "            \"splitter\": [\"best\", \"random\"],\n",
        "        },\n",
        "        \"cv\": StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 100,\n",
        "    },\n",
        "    \"XGBoost\": {\n",
        "        \"estimator\": xgb.XGBClassifier(\n",
        "            objective=\"multi:softmax\",\n",
        "            num_class=5,\n",
        "            random_state=random_state,\n",
        "            n_jobs=-1,\n",
        "            eval_metric=\"mlogloss\",\n",
        "        ),\n",
        "        \"params\": {\n",
        "            \"n_estimators\": [100, 200, 300, 500],\n",
        "            \"max_depth\": [3, 4, 5, 6, 7, 8],\n",
        "            \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "            \"subsample\": [0.8, 0.9, 1.0],\n",
        "            \"colsample_bytree\": [0.8, 0.9, 1.0],\n",
        "            \"reg_alpha\": [0, 0.1, 0.5, 1.0],\n",
        "            \"reg_lambda\": [0, 0.1, 0.5, 1.0],\n",
        "            \"min_child_weight\": [1, 3, 5, 7],\n",
        "            \"gamma\": [0, 0.1, 0.2, 0.3],\n",
        "        },\n",
        "        \"cv\": StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 40,\n",
        "    },\n",
        "    \"LDA\": {\n",
        "        \"estimator\": LinearDiscriminantAnalysis(),\n",
        "        \"params\": [\n",
        "            {\"solver\": [\"svd\"], \"store_covariance\": [False, True], \"tol\": [1e-4, 1e-3, 1e-2]},\n",
        "            {\"solver\": [\"lsqr\", \"eigen\"], \"shrinkage\": [None, \"auto\", 0.0, 0.05, 0.1, 0.15, 0.25, 0.35, 0.5, 0.65, 0.75, 0.85, 0.9], \"tol\": [1e-4, 1e-3, 1e-2]},\n",
        "        ],\n",
        "        \"cv\": StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 50,\n",
        "    },\n",
        "    \"ANN\": {\n",
        "        \"estimator\": MLPClassifier(\n",
        "            max_iter=300,\n",
        "            early_stopping=True,\n",
        "            random_state=random_state,\n",
        "            n_iter_no_change=10,\n",
        "            solver=\"adam\",\n",
        "        ),\n",
        "        \"params\": {\n",
        "            \"hidden_layer_sizes\": [(64,), (128,), (128, 64)],\n",
        "            \"activation\": [\"relu\"],\n",
        "            \"alpha\": loguniform(1e-4, 1e-2),\n",
        "            \"learning_rate_init\": loguniform(1e-3, 1e-2),\n",
        "            \"batch_size\": randint(64, 129),\n",
        "            \"beta_1\": uniform(0.9, 0.09),\n",
        "            \"beta_2\": uniform(0.95, 0.049),\n",
        "            \"validation_fraction\": [0.1, 0.15],\n",
        "        },\n",
        "        \"cv\": StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state),\n",
        "        \"n_iter\": 100,\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test models with Randomized Search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Without outlier removal or sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if model already exists\n",
        "classifier_name = \"LogisticRegression\"\n",
        "experiment_name = \"no_sampling\"\n",
        "create_new = param_spaces[classifier_name].get('create_new_model', False)\n",
        "\n",
        "if not create_new and model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_logreg = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    logreg = param_spaces[classifier_name]['estimator']\n",
        "\n",
        "    param_dist_logreg = param_spaces[classifier_name]['params']\n",
        "\n",
        "    rs_logreg = RandomizedSearchCV(\n",
        "        estimator=logreg,\n",
        "        param_distributions=param_dist_logreg,\n",
        "        n_iter=param_spaces[classifier_name]['n_iter'],\n",
        "        scoring=scoring,\n",
        "        refit='f1_macro',\n",
        "        cv=param_spaces[classifier_name]['cv'],\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=2\n",
        "    )\n",
        "    rs_logreg.fit(X_train_s, y_train)\n",
        "    \n",
        "    # Save the trained model\n",
        "    metadata = {\n",
        "        'best_params': rs_logreg.best_params_,\n",
        "        'best_score': rs_logreg.best_score_,\n",
        "        'cv_results': rs_logreg.cv_results_,\n",
        "        'experiment': experiment_name,\n",
        "        'classifier': classifier_name\n",
        "    }\n",
        "    model_saver.save_model(classifier_name, rs_logreg, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_logreg = rs_logreg.best_estimator_\n",
        "results['LogisticRegression'] = eval_model(\n",
        "    best_logreg,\n",
        "    X_train_s, y_train,\n",
        "    X_test_s, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(best_logreg)\n",
        "results['LogisticRegression']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.2 KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if model already exists\n",
        "classifier_name = \"KNN\"\n",
        "experiment_name = \"no_sampling\"\n",
        "\n",
        "if model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_knn = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    knn = KNeighborsClassifier()\n",
        "    param_dist_knn = {\n",
        "        'n_neighbors': randint(1, 51),\n",
        "        'weights': ['uniform', 'distance'],           # helps with imbalance; 'distance' often better\n",
        "        'metric': ['minkowski', 'manhattan', 'euclidean'],\n",
        "        'p': [1,2],                           # used only for minkowski, if left out it defaults to euclidean\n",
        "    }\n",
        "\n",
        "    rs_knn = RandomizedSearchCV(\n",
        "        estimator=knn,\n",
        "        param_distributions=param_dist_knn,\n",
        "        n_iter=20,\n",
        "        scoring=scoring,\n",
        "        refit='f1_macro',\n",
        "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    rs_knn.fit(X_train_s, y_train)\n",
        "    \n",
        "    # Save the trained model\n",
        "    metadata = {\n",
        "        'best_params': rs_knn.best_params_,\n",
        "        'best_score': rs_knn.best_score_,\n",
        "        'cv_results': rs_knn.cv_results_,\n",
        "        'experiment': experiment_name,\n",
        "        'classifier': classifier_name\n",
        "    }\n",
        "    model_saver.save_model(classifier_name, rs_knn, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_knn = rs_knn.best_estimator_\n",
        "results['KNN'] = eval_model(\n",
        "    best_knn,\n",
        "    X_train_s, y_train,\n",
        "    X_test_s, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(best_knn)\n",
        "results['KNN']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.3 Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if model already exists\n",
        "classifier_name = \"RandomForest\"\n",
        "experiment_name = \"no_sampling\"\n",
        "\n",
        "if model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_rf = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    rf = RandomForestClassifier(random_state=random_state, n_jobs=-1)\n",
        "    param_dist_rf = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 15, 20], # prevent overfitting majority class\n",
        "        \n",
        "        'min_samples_split': [2, 5, 10, 20, 50],\n",
        "        'min_samples_leaf': [1, 2, 4, 8], # higher = better regularization\n",
        "        \n",
        "        'max_features': ['sqrt', 'log2', None],\n",
        "        'bootstrap': [True], # better generalization\n",
        "        \n",
        "        'class_weight': ['balanced', None], # for imbalanced data\n",
        "        \n",
        "        # Split criterion: entropy can help with imbalanced classes\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "    }\n",
        "\n",
        "    rs_rf = RandomizedSearchCV(\n",
        "        estimator=rf,\n",
        "        param_distributions=param_dist_rf,\n",
        "        n_iter=20,\n",
        "        scoring=scoring,\n",
        "        refit='f1_macro',\n",
        "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "    )\n",
        "\n",
        "    rs_rf.fit(X_train, y_train) # using unscaled data - RF is not sensitive to feature scaling\n",
        "    \n",
        "    # Save the trained model\n",
        "    metadata = {\n",
        "        'best_params': rs_rf.best_params_,\n",
        "        'best_score': rs_rf.best_score_,\n",
        "        'cv_results': rs_rf.cv_results_,\n",
        "        'experiment': experiment_name,\n",
        "        'classifier': classifier_name\n",
        "    }\n",
        "    model_saver.save_model(classifier_name, rs_rf, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_rf = rs_rf.best_estimator_\n",
        "results['RandomForest'] = eval_model( \n",
        "    best_rf, \n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(best_rf)\n",
        "results['RandomForest']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.4 SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if model already exists\n",
        "classifier_name = \"SVM\"\n",
        "experiment_name = \"no_sampling\"\n",
        "\n",
        "if model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_svm = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    svm = SVC()\n",
        "    param_dist_svm = {\n",
        "        'kernel': ['rbf', 'poly'],\n",
        "        'C': [0.1, 1, 10],\n",
        "        'gamma': [0.001, 0.01, 0.1, 0.5, 1],\n",
        "    }\n",
        "    rs_svm = RandomizedSearchCV(\n",
        "        estimator=svm,\n",
        "        param_distributions=param_dist_svm,\n",
        "        n_iter=15,\n",
        "        scoring=scoring,\n",
        "        refit='f1_macro',\n",
        "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state),\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "    )\n",
        "    rs_svm.fit(X_train_s, y_train)\n",
        "    \n",
        "    # Save the trained model\n",
        "    metadata = {\n",
        "        'best_params': rs_svm.best_params_,\n",
        "        'best_score': rs_svm.best_score_,\n",
        "        'cv_results': rs_svm.cv_results_,\n",
        "        'experiment': experiment_name,\n",
        "        'classifier': classifier_name\n",
        "    }\n",
        "    model_saver.save_model(classifier_name, rs_svm, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_svm = rs_svm.best_estimator_\n",
        "results['SVM'] = eval_model(\n",
        "    best_svm,\n",
        "    X_train_s, y_train,\n",
        "    X_test_s, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(best_svm)\n",
        "results['SVM']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.5 Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if model already exists\n",
        "classifier_name = \"DecisionTree\"\n",
        "experiment_name = \"no_sampling\"\n",
        "\n",
        "if model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_dt = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    dt = DecisionTreeClassifier(random_state=random_state)\n",
        "\n",
        "    param_dist_dt = {\n",
        "        'max_depth': [None, 5, 10, 15, 20, 25, 30],\n",
        "        'min_samples_split': [2, 5, 10, 20, 50],\n",
        "        'min_samples_leaf': [1, 2, 4, 8, 16],\n",
        "        'max_features': ['sqrt', 'log2', None],\n",
        "        'criterion': ['gini', 'entropy'],  \n",
        "        'class_weight': ['balanced', None],\n",
        "        'splitter': ['best', 'random'],  # Split strategy\n",
        "    }\n",
        "\n",
        "    rs_dt = RandomizedSearchCV(\n",
        "        estimator=dt,\n",
        "        param_distributions=param_dist_dt,\n",
        "        n_iter=100,  \n",
        "        scoring=scoring,\n",
        "        refit='f1_macro',\n",
        "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "    )\n",
        "\n",
        "    rs_dt.fit(X_train, y_train)  # Using unscaled data - DT doesn't need scaling\n",
        "    \n",
        "    # Save the trained model\n",
        "    metadata = {\n",
        "        'best_params': rs_dt.best_params_,\n",
        "        'best_score': rs_dt.best_score_,\n",
        "        'cv_results': rs_dt.cv_results_,\n",
        "        'experiment': experiment_name,\n",
        "        'classifier': classifier_name\n",
        "    }\n",
        "    model_saver.save_model(classifier_name, rs_dt, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_dt = rs_dt.best_estimator_\n",
        "results['DecisionTree'] = eval_model(\n",
        "    best_dt,\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(best_dt)\n",
        "results['DecisionTree']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.6 XGBoost / Gradien Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if model already exists\n",
        "classifier_name = \"XGBoost\"\n",
        "experiment_name = \"no_sampling\"\n",
        "\n",
        "if model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_xgb = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        objective='multi:softmax',\n",
        "        num_class=5,  # no of classes\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        eval_metric='mlogloss'\n",
        "    )\n",
        "\n",
        "    param_dist_xgb = {\n",
        "        'n_estimators': [100, 200, 300, 500],\n",
        "        'max_depth': [3, 4, 5, 6, 7, 8],\n",
        "        'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "        'subsample': [0.8, 0.9, 1.0],\n",
        "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "        'reg_alpha': [0, 0.1, 0.5, 1.0],  # L1 regularization\n",
        "        'reg_lambda': [0, 0.1, 0.5, 1.0],  # L2 regularization\n",
        "        'min_child_weight': [1, 3, 5, 7],\n",
        "        'gamma': [0, 0.1, 0.2, 0.3],  # Minimum loss reduction\n",
        "    }\n",
        "\n",
        "    rs_xgb = RandomizedSearchCV(\n",
        "        estimator=xgb_model,\n",
        "        param_distributions=param_dist_xgb,\n",
        "        n_iter=30,  \n",
        "        scoring=scoring,\n",
        "        refit='f1_macro',\n",
        "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "    )\n",
        "\n",
        "    rs_xgb.fit(X_train, y_train)  # XGBoost handles scaling internally\n",
        "    \n",
        "    # Save the trained model\n",
        "    metadata = {\n",
        "        'best_params': rs_xgb.best_params_,\n",
        "        'best_score': rs_xgb.best_score_,\n",
        "        'cv_results': rs_xgb.cv_results_,\n",
        "        'experiment': experiment_name,\n",
        "        'classifier': classifier_name\n",
        "    }\n",
        "    model_saver.save_model(classifier_name, rs_xgb, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_xgb = rs_xgb.best_estimator_\n",
        "results['XGBoost'] = eval_model(\n",
        "    best_xgb,\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(best_xgb)\n",
        "results['XGBoost']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.7 Linear Discriminant Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if model already exists\n",
        "classifier_name = \"LDA\"\n",
        "experiment_name = \"no_sampling\"\n",
        "\n",
        "if model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_lda = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    param_distributions = [\n",
        "        {\n",
        "            \"solver\": [\"svd\"],\n",
        "            \"store_covariance\": [False, True],\n",
        "            \"tol\": [1e-4, 1e-3, 1e-2],\n",
        "            # n_components kept implicit (None) to avoid invalid values vs. n_classes-1\n",
        "        },\n",
        "        {\n",
        "            \"solver\": [\"lsqr\", \"eigen\"],\n",
        "            \"shrinkage\": [None, \"auto\", 0.0, 0.05, 0.1, 0.15, 0.25, 0.35, 0.5, 0.65, 0.75, 0.85, 0.9],\n",
        "            \"tol\": [1e-4, 1e-3, 1e-2],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
        "\n",
        "    lda = LinearDiscriminantAnalysis()\n",
        "    rs_lda = RandomizedSearchCV(\n",
        "        estimator=lda,\n",
        "        param_distributions=param_distributions,\n",
        "        n_iter=50,\n",
        "        scoring=scoring,\n",
        "        refit=\"f1_macro\",\n",
        "        cv=cv,\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "    )\n",
        "\n",
        "\n",
        "    rs_lda.fit(X_train_s, y_train)\n",
        "    \n",
        "    # Save the trained model\n",
        "    metadata = {\n",
        "        \"best_params\": rs_lda.best_params_,\n",
        "        \"best_score\": rs_lda.best_score_,\n",
        "        \"cv_results\": rs_lda.cv_results_,\n",
        "        \"experiment\": experiment_name,\n",
        "        \"classifier\": classifier_name,\n",
        "    }\n",
        "\n",
        "    model_saver.save_model(classifier_name, rs_lda, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_lda = rs_lda.best_estimator_\n",
        "results['LDA'] = eval_model(\n",
        "    best_lda,\n",
        "    X_train_s, y_train,\n",
        "    X_test_s, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(best_lda)\n",
        "display(rs_lda.best_params_)\n",
        "results['LDA']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.8 Artificial Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_name = \"ANN\"\n",
        "experiment_name = \"no_sampling\"\n",
        "\n",
        "if model_saver.model_exists(classifier_name, experiment_name):\n",
        "    print(f\"Model {classifier_name} already exists for experiment {experiment_name}. Loading...\")\n",
        "    rs_lda = model_saver.load_model(classifier_name, experiment_name)\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "else:\n",
        "    print(f\"Model {classifier_name} not found. Training new model...\")\n",
        "    \n",
        "    ann = MLPClassifier(\n",
        "        max_iter=300,\n",
        "        early_stopping=True,\n",
        "        random_state=random_state,\n",
        "        n_iter_no_change=10,\n",
        "        solver=\"adam\",\n",
        "    )\n",
        "\n",
        "\n",
        "    param_distributions = {\n",
        "        \"hidden_layer_sizes\": [\n",
        "            (64,),\n",
        "            (128,),\n",
        "            (128, 64),\n",
        "        ],\n",
        "        \"activation\": [\"relu\"],  # focused, fast\n",
        "        \"alpha\": loguniform(1e-4, 1e-2),  # L2\n",
        "        \"learning_rate_init\": loguniform(1e-3, 1e-2),\n",
        "        \"batch_size\": randint(64, 129),\n",
        "        \"beta_1\": uniform(0.9, 0.09),   # ~0.90-0.99\n",
        "        \"beta_2\": uniform(0.95, 0.049), # ~0.95-0.999\n",
        "        \"validation_fraction\": [0.1, 0.15],\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
        "\n",
        "    rs_ann = RandomizedSearchCV(\n",
        "        estimator=ann,\n",
        "        param_distributions=param_distributions,\n",
        "        n_iter=20,\n",
        "        scoring=scoring,\n",
        "        refit=\"f1_macro\",\n",
        "        cv=cv,\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "    )\n",
        "\n",
        "    rs_ann.fit(X_train_s, y_train)\n",
        "    \n",
        "    metadata = {\n",
        "        \"best_params\": rs_ann.best_params_,\n",
        "        \"best_score\": rs_ann.best_score_,\n",
        "        \"cv_results\": rs_ann.cv_results_,\n",
        "        \"experiment\": experiment_name,\n",
        "        \"classifier\": classifier_name,\n",
        "    }\n",
        "    model_saver.save_model(classifier_name, rs_ann, experiment_name, metadata)\n",
        "    print(f\"Model {classifier_name} saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_ann = rs_ann.best_estimator_\n",
        "results['ANN'] = eval_model(\n",
        "    best_ann,\n",
        "    X_train_s, y_train,\n",
        "    X_test_s, y_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(best_ann)\n",
        "display(rs_ann.best_score_)\n",
        "results['ANN']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.9 Results Summary and Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _safe_col(label):\n",
        "    # Make safe column names like \"val_f1_cls_0\" or \"val_f1_cls_N\"\n",
        "    return re.sub(r'[^0-9a-zA-Z_]+', '_', str(label)).strip('_')\n",
        "\n",
        "# Mapping of model names to their RandomizedSearchCV objects\n",
        "models_and_searchers = {\n",
        "    \"LogisticRegression\": rs_logreg,\n",
        "    \"KNN\": rs_knn, \n",
        "    \"RandomForest\": rs_rf,\n",
        "    \"SVM\": rs_svm,\n",
        "    \"DecisionTree\": rs_dt,\n",
        "    \"XGBoost\": rs_xgb,\n",
        "    \"LDA\": rs_lda,\n",
        "    \"ANN\": rs_ann,\n",
        "}\n",
        "\n",
        "rows = []\n",
        "for name, res in results.items():\n",
        "    row = {\n",
        "        'model': name,\n",
        "        'test_accuracy': round(res['test']['accuracy'], 2),\n",
        "        'test_f1_macro': round(res['test']['f1_macro'], 2),\n",
        "    }\n",
        "\n",
        "    # Add best parameters from RandomizedSearchCV\n",
        "    if name in models_and_searchers:\n",
        "        searcher = models_and_searchers[name]\n",
        "        best_params = searcher.best_params_\n",
        "        best_cv_score = searcher.best_score_\n",
        "        row['best_cv_score'] = round(best_cv_score, 2)\n",
        "        row['best_parameters'] = str(best_params)\n",
        "    else:\n",
        "        row['best_cv_score'] = None\n",
        "        row['best_parameters'] = None\n",
        "\n",
        "    labels = res['labels']\n",
        "    f1_t = res['test']['f1_per_class']\n",
        "\n",
        "    # Add per-class F1 columns for test set only\n",
        "    for lbl, f1 in zip(labels, f1_t):\n",
        "        row[f'test_f1_cls_{_safe_col(lbl)}'] = round(f1, 2)\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "comparison_df = (\n",
        "    pd.DataFrame(rows)\n",
        "      .sort_values(by=['test_f1_macro'], ascending=False)\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "best_model_name = comparison_df.iloc[0]['model']\n",
        "best_model_results = results[best_model_name]\n",
        "\n",
        "comparison_df_display = comparison_df.copy()\n",
        "comparison_df_display['best_parameters'] = comparison_df_display['best_parameters'].apply(\n",
        "    lambda x: json.dumps(x, indent=2) if isinstance(x, dict) else x\n",
        ")\n",
        "import os \n",
        "\n",
        "comparison_df_display.to_csv(\"../src/data/03_model_testing_results/03_01_model_comparison_without_resampling.csv\", index=False)\n",
        "\n",
        "# Display the comparison table with best parameters\n",
        "print(\"=\" * 100)\n",
        "print(\"MODEL COMPARISON WITH BEST PARAMETERS FROM RANDOMIZEDSEARCHCV\")\n",
        "print(\"=\" * 100)\n",
        "display(comparison_df_display)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check saved models\n",
        "print(\"=\" * 80)\n",
        "print(\"SAVED MODELS INFORMATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "saved_models = model_saver.list_saved_models()\n",
        "if saved_models:\n",
        "    for model_key, info in saved_models.items():\n",
        "        print(f\"\\nModel: {model_key}\")\n",
        "        print(f\"  Exists: {info['exists']}\")\n",
        "        print(f\"  Path: {info['model_path']}\")\n",
        "        if info['exists']:\n",
        "            print(f\"  Size: {info['size_bytes']} bytes\")\n",
        "            print(f\"  Modified: {info['modified_time']}\")\n",
        "        \n",
        "        # Load and display metadata if available\n",
        "        if info['metadata_exists']:\n",
        "            try:\n",
        "                metadata = model_saver.load_metadata(model_key.split('_')[0], model_key.split('_')[1] if '_' in model_key else 'default')\n",
        "                if metadata:\n",
        "                    print(f\"  Best Score: {metadata.get('best_score', 'N/A')}\")\n",
        "                    print(f\"  Best Params: {metadata.get('best_params', 'N/A')}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error loading metadata: {e}\")\n",
        "else:\n",
        "    print(\"No saved models found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. With Sampling Methods\n",
        "\n",
        "but without\n",
        "- Feature Engineering ( RR-Interval! )\n",
        "- baseline wandering removal\n",
        "- denoising\n",
        "- Leak-Free Scaling\n",
        "- RepeatedStratifiedKFold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### 3.2.1 Quick run - Using the best models from above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampling_methods = {\n",
        "    'No_Sampling': None,\n",
        "    'RandomOverSampler': RandomOverSampler(random_state=random_state),\n",
        "    'SMOTE': SMOTE(random_state=random_state, k_neighbors=5),\n",
        "    'ADASYN': ADASYN(random_state=random_state, n_neighbors=5),\n",
        "    'SMOTETomek': SMOTETomek(random_state=random_state, smote=SMOTE(random_state=random_state, k_neighbors=5)),\n",
        "    'SMOTEENN': SMOTEENN(random_state=random_state, smote=SMOTE(random_state=random_state, k_neighbors=5)),\n",
        "}\n",
        "\n",
        "sampling_results = {}\n",
        "\n",
        "best_models = {\n",
        "    'KNN': best_knn,\n",
        "    'RandomForest': best_rf,\n",
        "    'XGBoost': best_xgb,\n",
        "}\n",
        "\n",
        "scale_sensitive = ['LogisticRegression', 'SVM', 'KNN']\n",
        "\n",
        "print(\"Testing sampling methods on best models...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for sampling_name, sampler in sampling_methods.items():\n",
        "    print(f\"\\nTesting {sampling_name}...\")\n",
        "    sampling_results[sampling_name] = {}\n",
        "    \n",
        "    for model_name, model in best_models.items():\n",
        "        print(f\"  - {model_name}\")\n",
        "        \n",
        "        try:\n",
        "            if sampler is None:\n",
        "                # No sampling - use original, only scaled data\n",
        "                result = eval_model(model,\n",
        "                                    X_train_s if model_name in scale_sensitive else X_train , y_train,\n",
        "                                    X_test_s if model_name in scale_sensitive else X_test, y_test)\n",
        "            else:\n",
        "                # Apply sampling on unscaled data\n",
        "                X_train_sampled, y_train_sampled = sampler.fit_resample(X_train, y_train)\n",
        "                \n",
        "                # Re-scale if needed for models that require scaling\n",
        "                if model_name in scale_sensitive:\n",
        "                    scaler_sampling = StandardScaler()\n",
        "                    X_train_sampled = scaler_sampling.fit_transform(X_train_sampled)\n",
        "                    X_test_sampled = scaler_sampling.transform(X_test)\n",
        "                else: # e.g. RF, XGBoost\n",
        "                    X_test_sampled = X_test\n",
        "            \n",
        "                result = eval_model(\n",
        "                    model,\n",
        "                    X_train_sampled, y_train_sampled,\n",
        "                    X_test_sampled, y_test,\n",
        "                )\n",
        "            \n",
        "            sampling_results[sampling_name][model_name] = result\n",
        "            \n",
        "            # Printing statistics\n",
        "            if sampler is not None:\n",
        "                unique, counts = np.unique(y_train_sampled, return_counts=True)\n",
        "                print(f\"    Class distribution after {sampling_name}:\")\n",
        "                for cls, count in zip(unique, counts):\n",
        "                    print(f\"      Class {cls}: {count:,} samples\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"    ERROR with {sampling_name} + {model_name}: {str(e)}\")\n",
        "            sampling_results[sampling_name][model_name] = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Create comprehensive comparison table\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"SAMPLING METHODS COMPARISON\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Prepare comparison data\n",
        "comparison_rows = []\n",
        "\n",
        "for sampling_name, models_results in sampling_results.items():\n",
        "    for model_name, result in models_results.items():\n",
        "        if result is not None:\n",
        "            row = {\n",
        "                'sampling_method': sampling_name,\n",
        "                'model': model_name,\n",
        "                'test_accuracy': round(result['test']['accuracy'],2),\n",
        "                'test_f1_macro': round(result['test']['f1_macro'],2)\n",
        "            }\n",
        "            \n",
        "            # Add per-class F1 scores for test set only\n",
        "            labels = result['labels']\n",
        "            f1_t = result['test']['f1_per_class']\n",
        "            \n",
        "            for lbl, f1 in zip(labels, f1_t):\n",
        "                row[f'test_f1_cls_{_safe_col(lbl)}'] = round(f1,2)\n",
        "            \n",
        "            comparison_rows.append(row)\n",
        "\n",
        "# Create and display comparison DataFrame\n",
        "sampling_comparison_df = (\n",
        "    pd.DataFrame(comparison_rows)\n",
        "    .sort_values(by=['test_f1_macro'], ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampling_comparison_df.to_csv(\"../reports/03_model_testing_results/03_02_model_comparison_with_sampling_on_best_models.csv\", index=False)\n",
        "\n",
        "# Find best combination\n",
        "best_sampling_model = sampling_comparison_df.iloc[0]\n",
        "print(f\"\\nBEST COMBINATION:\")\n",
        "print(f\"Sampling Method: {best_sampling_model['sampling_method']}\")\n",
        "print(f\"Model: {best_sampling_model['model']}\")\n",
        "print(f\"Test F1-Macro: {best_sampling_model['test_f1_macro']:.4f}\")\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\nSUMMARY STATISTICS:\")\n",
        "print(f\"Total combinations tested: {len(comparison_rows)}\")\n",
        "print(f\"Best test F1-macro: {sampling_comparison_df['test_f1_macro'].max():.4f}\")\n",
        "\n",
        "# Show top 5 combinations\n",
        "print(f\"\\nTOP 5 COMBINATIONS:\")\n",
        "top_5 = sampling_comparison_df.head(5)[['sampling_method', 'model', 'test_f1_macro']]\n",
        "display(top_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.2.2 Extended Run: Sampling + RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Model Training Phase\n",
        "# This cell focuses only on training models with RandomizedSearchCV\n",
        "# Evaluation is separated to prevent interruption of cross-validation runs\n",
        "\n",
        "sampling_methods = {\n",
        "    'RandomOverSampler': RandomOverSampler(random_state=random_state),\n",
        "    'SMOTE': SMOTE(random_state=random_state, k_neighbors=5),\n",
        "    'ADASYN': ADASYN(random_state=random_state, n_neighbors=5),\n",
        "    'SMOTETomek': SMOTETomek(random_state=random_state, smote=SMOTE(random_state=random_state, k_neighbors=5)),\n",
        "    'SMOTEENN': SMOTEENN(random_state=random_state, smote=SMOTE(random_state=random_state, k_neighbors=5)),\n",
        "}\n",
        "\n",
        "# Which models need scaling (no pipeline used; fit scaler once on the resampled training set)\n",
        "scale_sensitive = [\"LogisticRegression\", \"KNN\", \"SVM\", \"LDA\", \"ANN\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting 3.2.2 TRAINING PHASE: Full RandomizedSearchCV for each model, per sampling method\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for sampling_name, sampler in sampling_methods.items():\n",
        "    print(f\"\\n=== Training with Sampling: {sampling_name} ===\")\n",
        "\n",
        "    # Apply sampling on original training set (before CV)\n",
        "    try:\n",
        "        if sampler is None:\n",
        "            X_train_res, y_train_res = X_train, y_train\n",
        "        else:\n",
        "            X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
        "    except Exception as e:\n",
        "        print(f\"  Skipping sampling '{sampling_name}' due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Show distribution if sampling applied\n",
        "    if sampler is not None:\n",
        "        unique, counts = np.unique(y_train_res, return_counts=True)\n",
        "        print(\"  Class distribution after sampling:\")\n",
        "        for cls, cnt in zip(unique, counts):\n",
        "            print(f\"    Class {cls}: {cnt:,} samples\")\n",
        "\n",
        "    # For each model, run RS-CV on the resampled dataset (TRAINING ONLY)\n",
        "    for model_name, spec in param_spaces.items():\n",
        "        experiment_name = f\"with_sampling_{sampling_name}\"\n",
        "        classifier_name = model_name\n",
        "\n",
        "        # Prepare data (leak-prone sampling by design here)\n",
        "        if model_name in scale_sensitive:\n",
        "            scaler = StandardScaler()\n",
        "            X_tr_fit = scaler.fit_transform(X_train_res)\n",
        "        else:\n",
        "            X_tr_fit = X_train_res\n",
        "\n",
        "        # Train or load if already saved\n",
        "        try:\n",
        "            # Check if an RS-CV object already exists for this sampler+model\n",
        "            if model_saver.model_exists(classifier_name, experiment_name):\n",
        "                print(f\"  [{model_name}] Exists for {experiment_name}. LoaLoading next one...\")\n",
        "            else:\n",
        "                print(f\"  [{model_name}] Training RS-CV for {experiment_name}...\")\n",
        "                rs = RandomizedSearchCV(\n",
        "                    estimator=spec[\"estimator\"],\n",
        "                    param_distributions=spec[\"params\"],\n",
        "                    n_iter=spec[\"n_iter\"],\n",
        "                    scoring=scoring,\n",
        "                    refit=\"f1_macro\",\n",
        "                    cv=spec[\"cv\"],\n",
        "                    random_state=random_state,\n",
        "                    n_jobs=-1,\n",
        "                    verbose=2,\n",
        "                )\n",
        "                rs.fit(X_tr_fit, y_train_res)\n",
        "\n",
        "                metadata = {\n",
        "                    \"best_params\": rs.best_params_,\n",
        "                    \"best_score\": rs.best_score_,\n",
        "                    \"cv_results\": rs.cv_results_,\n",
        "                    \"experiment\": experiment_name,\n",
        "                    \"classifier\": classifier_name,\n",
        "                    \"sampling_method\": sampling_name,\n",
        "                }\n",
        "                model_saver.save_model(classifier_name, rs, experiment_name, metadata)\n",
        "                print(f\"  [{model_name}] Saved for {experiment_name}.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  [{model_name}] ERROR for {experiment_name}: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TRAINING PHASE COMPLETED\")\n",
        "print(\"=\" * 80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Model Evaluation Phase\n",
        "# This cell focuses only on evaluating the trained models\n",
        "# This separation prevents interruption of cross-validation runs from affecting evaluation\n",
        "\n",
        "print(\"Starting 3.2.2 EVALUATION PHASE: Evaluating trained models\")\n",
        "print(\"=\" * 80)\n",
        "# Path to results file\n",
        "out_path = \"../reports/03_model_testing_results/03_03_model_comparison_with_sampling_randomized_search.csv\"\n",
        "\n",
        "# Load existing results if CSV already exists\n",
        "if os.path.exists(out_path):\n",
        "    existing_df = pd.read_csv(out_path)\n",
        "    print(f\"Loaded existing results with {len(existing_df)} rows from {out_path}\")\n",
        "else:\n",
        "    existing_df = pd.DataFrame()\n",
        "    print(f\"No existing results found. Will create {out_path}\")\n",
        "\n",
        "print(\"Starting 3.2.2 EVALUATION PHASE: Evaluating trained models\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Evaluation and saving\n",
        "sampling_results = {}\n",
        "\n",
        "for sampling_name, sampler in sampling_methods.items():\n",
        "    print(f\"\\n=== Evaluating with Sampling: {sampling_name} ===\")\n",
        "    sampling_results[sampling_name] = {}\n",
        "\n",
        "    # Apply sampling on original training set (before CV)\n",
        "    try:\n",
        "        if sampler is None:\n",
        "            X_train_res, y_train_res = X_train, y_train\n",
        "        else:\n",
        "            X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
        "    except Exception as e:\n",
        "        print(f\"  Skipping sampling '{sampling_name}' due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "    for model_name, spec in param_spaces.items():\n",
        "        experiment_name = f\"with_sampling_{sampling_name}\"\n",
        "        classifier_name = model_name\n",
        "\n",
        "        # Skip if already evaluated\n",
        "        if not existing_df.empty and (\n",
        "            (existing_df[\"sampling_method\"] == sampling_name)\n",
        "            & (existing_df[\"model\"] == model_name)\n",
        "        ).any():\n",
        "            print(f\"  [{model_name}] Skipping (already in CSV)\")\n",
        "            continue\n",
        "\n",
        "        # Prepare scaled data if necessary\n",
        "        if model_name in scale_sensitive:\n",
        "            scaler = StandardScaler()\n",
        "            X_tr_fit = scaler.fit_transform(X_train_res)\n",
        "            X_te_fit = scaler.transform(X_test)\n",
        "        else:\n",
        "            X_tr_fit, X_te_fit = X_train_res, X_test\n",
        "\n",
        "        try:\n",
        "            # Load the trained model\n",
        "            if model_saver.model_exists(classifier_name, experiment_name):\n",
        "                print(f\"  [{model_name}] Loading trained model for evaluation...\")\n",
        "                rs = model_saver.load_model(classifier_name, experiment_name)\n",
        "\n",
        "                # Evaluate best estimator on test set\n",
        "                print(f\"  Evaluating model [{model_name}]\")\n",
        "                best_est = rs.best_estimator_\n",
        "                res = eval_model(\n",
        "                    best_est,\n",
        "                    X_tr_fit, y_train_res,\n",
        "                    X_te_fit, y_test,\n",
        "                )\n",
        "\n",
        "                sampling_results[sampling_name][model_name] = {\"rs\": rs, \"eval\": res}\n",
        "\n",
        "                # Prepare single row result\n",
        "                row = {\n",
        "                    \"sampling_method\": sampling_name,\n",
        "                    \"model\": model_name,\n",
        "                    \"test_accuracy\": round(res[\"test\"][\"accuracy\"], 2),\n",
        "                    \"test_f1_macro\": round(res[\"test\"][\"f1_macro\"], 2),\n",
        "                    \"best_cv_score\": round(rs.best_score_, 2),\n",
        "                    \"best_parameters\": json.dumps(rs.best_params_),\n",
        "                }\n",
        "\n",
        "                # Add per-class F1 scores\n",
        "                labels = res[\"labels\"]\n",
        "                for lbl, f1 in zip(labels, res[\"test\"][\"f1_per_class\"]):\n",
        "                    row[f\"test_f1_cls_{lbl}\"] = round(float(f1), 2)\n",
        "\n",
        "                # Convert to DataFrame and append immediately\n",
        "                new_df = pd.DataFrame([row])\n",
        "                header = not os.path.exists(out_path)\n",
        "                new_df.to_csv(out_path, mode=\"a\", index=False, header=header)\n",
        "                print(f\"   Result saved for [{model_name}] ({sampling_name})\")\n",
        "\n",
        "                # Update in-memory record too\n",
        "                existing_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
        "\n",
        "            else:\n",
        "                print(f\"  [{model_name}] No trained model found for {experiment_name}\")\n",
        "                sampling_results[sampling_name][model_name] = None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  [{model_name}] ERROR for {experiment_name}: {e}\")\n",
        "            sampling_results[sampling_name][model_name] = None\n",
        "\n",
        "\n",
        "# Final check summary\n",
        "if not existing_df.empty:\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(\"CURRENT MODEL EVALUATION SUMMARY\")\n",
        "    print(\"=\" * 100)\n",
        "    display(\n",
        "        existing_df[[\"sampling_method\", \"model\", \"test_f1_macro\"]]\n",
        "        .sort_values(by=[\"test_f1_macro\"], ascending=False)\n",
        "        .head(10)\n",
        "    )\n",
        "else:\n",
        "    print(\"No results to display.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EVALUATION PHASE COMPLETED\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. GridSearch - Final run on 3 best models\n",
        "\n",
        "- target models. XGBoost, ANN, SVM\n",
        "- test with an without outlier removal\n",
        "- GridSearchCV on same parameter spaces\n",
        "- using common train test dataset\n",
        "- RepeatedStratifiedKFold:\n",
        "    - single CV split can be \"lucky\" or \"unlucky\" --> dependency how data is shuffled\n",
        "    - repeating stratified k-fold with different shuffles averages out randomness\n",
        "    - more stable, less noisy estimates of performance\n",
        "- Implement Leak-free scaling\n",
        "    - current: without pipeline: scale once on the full training set, then do CV on the already sclaed data --> scaler \"saw\" all CV folds, including each folds validation part --> data leakage\n",
        "    - makes CV too optimisic?\n",
        "    - pipeline fits the scaler only on each training fold, then applieos it to that folds validation split. \n",
        "    - solution: Pipeline(StandardScaler(), model) so scaling is fit per CV fold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Last Open Point\n",
        "\n",
        "- Try to optimize the signal - run best models with the optimized signal and new features\n",
        "    - add RR-Interval as feature\n",
        "    - add new Target \"not_normal\" in MIT to compare to PTB\n",
        "    - baseline wandering removal\n",
        "    - denoising"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
